
<!--Preface-->

<div class="frontmatters">
<div class="page-frontmatter preface front set-front-matter">
<div class="title-name">
<h2 id="maintitle">Python Testing Cookbook - Second Edition</h2>
</div>

<p>Easy solutions to test your Python projects using test-driven development and Selenium</p>

<p>Greg L. Turnquist</p>
<p>Bhaskar N. Das</p>

<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000039.jpg" class="lazyload" /></p>
<p><strong><strong>BIRMINGHAM - MUMBAI</strong></strong></p>

<p>Copyright &copy; 2018 Packt Publishing</p>
<p>All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.</p>
<p>Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author(s), nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.</p>
<p>Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.</p>
<p><strong>Commissioning Editor:</strong>&nbsp;Merint Matthew<br /><strong>Acquisition Editor:</strong>&nbsp;Chaitanya Nair<br /><strong>Content Development Editor:</strong>&nbsp;Rohit Singh<br /><strong>Technical Editor:</strong>&nbsp;Romy Dias<br /><strong>Copy Editor:</strong> Safis Editing<br /><strong>Project Coordinator:</strong> Vaidehi Sawant<br /><strong>Proofreader:</strong> Safis Editing<br /><strong>Indexer:&nbsp;</strong>Pratik Shirodkar<br /><strong>Graphics:</strong> Jason Monteiro<br /><strong>Production Coordinator:</strong> Deepika Naik</p>
<p>First published: May 2011<br />Second edition: June 2018</p>
<p>Production reference: 1290618</p>
<p>Published by Packt Publishing Ltd.<br />Livery Place<br />35 Livery Street<br />Birmingham<br />B3 2PB, UK.</p>
<p>978-1-78712-252-9</p>
<p><a href="http://www.packtpub.com" target="_blank" rel="noopener">www.packtpub.com</a></p>


<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000030.jpg" class="lazyload" /></p>

<p><a href="https://mapt.io/" target="_blank" rel="noopener">mapt.io</a></p>
<p>Mapt is an online digital library that gives you full access to over 5,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>


<h2>Contributors</h2>
<h3>About the authors</h3>
<p><strong>Greg L. Turnquist</strong>&nbsp;has worked in the software industry since 1997. He is an active participant in the open source community and has contributed patches to several projects, including MythTV, Spring Security, MediaWiki, and the TestNG Eclipse plugin. As a test-obsessed script junky, he has always sought the right tool for the job. He is a firm believer in agile practices and automated testing. He has developed distributed systems and LAMP-based setups, and he has supported mission-critical systems hosted on various platforms.<br />After graduating from Auburn University with a master's in computer engineering, Greg started working with the Harris Corporation. He worked on many contracts utilizing many types of technology. In 2006, he created the Spring Python project and went on to write Spring Python 1.1 in 2010. He joined SpringSource, a division of VMware in 2010, as part of its international software development team.</p>



<p><strong>Bhaskar N. Das</strong> has 8 years' experience in various projects involving application development, maintenance, and support with IBM. He has worked in various technologies and domains including Java, Python, application servers, the cloud, and various database technologies. His domain expertise includes finance and asset management (IT and finance assets). His areas of interest include big data, business finance optimization and scaling, data science, and Machine Learning.</p>


<h3>About the reviewers</h3>
<p><strong>Maurice HT Ling</strong> is a Research Assistant Professor at the Perdana University School of Data Sciences. He obtained his BSc.(Hons.) in Molecular and Cell Biology from The University of Melbourne, Australia, in 2004, and his BSc. in Computing from the University of Portsmouth, United Kingdom, in 2007, before obtaining his Ph.D. in Bioinformatics from The University of Melbourne, Australia, in 2009. He is a bioinformatician who is currently working on looking at evolutionary biology and various aspects of life. His main techniques involve experimental evolution and simulations, such as artificial life simulation, for accessing evolutionary perspectives, and using existing published data. He has developed computational algorithms as part of this research. He has a wide range of other interests, including professional and social aspects of science and education, and studies this area using autoethnographical and autobiographical methods. He co-founded Python User Group (Singapore), and has been instrumental in inaugurating PyCon Asia-Pacific as one of the 3 major Python conferences worldwide, together with PyCon US and EuroPython. On the commercial side, he is the principal partner of Colossus Technologies LLP, Singapore. In his free time, he likes to read, enjoy a cup of coffee, writing his personal journal, or philosophizing on various aspects of life.</p>




<h3>Packt is searching for authors like you</h3>
<p>If you're interested in becoming an author for Packt, please visit <a href="http://authors.packtpub.com" target="_blank" rel="noopener">authors.packtpub.com</a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>



<h2>Preface</h2>
<p>Testing has always been part of software development. For decades, comprehensive testing was defined by complex manual test procedures backed by big budgets; but something revolutionary happened in 1998. In his Guide to Better Smalltalk, Smalltalk guru, Kent Beck, introduced an automated test framework called SUnit. This triggered an avalanche of test frameworks, including JUnit, PyUnit, and many others for different languages and various platforms, dubbed the xUnit movement. Automated testing was made a cornerstone of the agile movement when the top 17 software experts signed the Agile Manifesto in 2001.</p>
<p>Testing includes many different styles, including unit testing, integration testing, acceptance testing, smoke testing, load testing, and countless others. This book delves deeper and explores testing at all the important levels while using the nimble power of Python. It also demonstrates many tools.</p>
<p>This book is meant to expand your knowledge of testing from something you either heard<br />about, or have practiced a little, into something you can apply at any level to meet your needs in improving software quality. We hope to give you the tools to reap huge rewards in better software development and customer satisfaction.</p>
<h2>Who this book is for</h2>
<p>If you're a Python developer who wants to take testing to the next level and would like to expand your testing skills, this book is for you. It is assumed that you have some Python programming knowledge.</p>
<h2>What this book covers</h2>
<p>Chapter 1, <em>Using Unittest to Develop Basic Tests</em>, gives you a quick introduction to the most commonly used test framework in the Python community.</p>
<p>Chapter 2, <em>Running Automated Test Suites with Nose</em>, introduces the most ubiquitous Python test tool and gets busy by showing you how to write specialized plugins.</p>
<p>Chapter 3, <em>Creating Testable Documentation with doctest</em>, shows many different ways of using Python's docstrings to build runnable doctests as well as write custom test runners.</p>
<p>Chapter 4, <em>Testing Customer Stories with Behavior-Driven Development,</em> dives into writing easy-to-read testable customer stories using doctest, mocking, and Lettuce/Should DSL.</p>
<p>Chapter 5, <em>High Level Customer Scenarios with Acceptance Testing</em>, helps you get into the mindset of the customer and write tests from their perspective using Pyccuracy and the Robot Framework.</p>
<p>Chapter 6, <em>Integrating Automated Tests with Continuous Integration</em>, shows you how to add continuous integration to your development process with Jenkins and TeamCity.</p>
<p>Chapter 7, <em>Measuring your Success with Test Coverage</em>, explores how to create coverage reports and interpret them correctly. It also delves deeper in terms of seeing how to tie them in with your continuous integration system.</p>
<p>Chapter 8, <em>Smoke/Load Testing&nbsp;&ndash; Testing Major Parts,</em> focuses on how to create smoke test suites to get a pulse from the system. It also demonstrates how to put the system under load in order to ensure that it can handle the current load as well as find the next breaking point for future loads.</p>
<p>Chapter 9, <em>Good Test Habits for New and Legacy Systems</em>, takes you through many different lessons learned from the author about what works when it comes to software testing.</p>
<p><em>Chapter 10</em>, <em>Web UI Tesing Using Selenium</em>, teaches you how to write suitable test sets for their software. It will explain the various test sets and frameworks to use. This chapter is not available in the book, it is available online in the following link:&nbsp;<a href="https://www.packtpub.com/sites/default/files/downloads/Web_UI_Testing_Using_Selenium.pdf">https://www.packtpub.com/sites/default/files/downloads/Web_UI_Testing_Using_Selenium.pdf</a></p>
<h2>To get the most out of this book</h2>
<p>You will need Python installed on your machine. This book uses many other Python test tools but includes detailed steps showing how to install and use them.</p>
<h2>Download the example code files</h2>
<p>You can download the example code files for this book from your account at <a href="http://www.packtpub.com">www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support" target="_blank" rel="noopener">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packtpub.com/support" target="_blank" rel="noopener">www.packtpub.com</a>.</li>
<li>Select the SUPPORT tab.</li>
<li>Click on Code Downloads &amp; Errata.</li>
<li>Enter the name of the book in the Search box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p>The code bundle for the book is also hosted on GitHub at&nbsp;<a href="https://github.com/PacktPublishing/Python-Testing-Cookbook-Second-Edition">https://github.com/PacktPublishing/Python-Testing-Cookbook-Second-Edition</a>. We also have other code bundles from our rich catalog of books and videos available at <strong><a href="https://github.com/PacktPublishing/" target="_blank" rel="noopener">https://github.com/PacktPublishing/</a></strong>. Check them out!</p>

<h2>Conventions used</h2>
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example:&nbsp;"You can also use&nbsp;<kbd>pip install virtualenv</kbd>&nbsp;as well."</p>
<p>A block of code is set as follows:</p>
<pre><code class="lang-python">if __name__== "__main__": 
    suite = unittest.TestLoader().loadTestsFromTestCase(\
              RomanNumeralConverterTest) 
    unittest.TextTestRunner(verbosity=2).run(suite) </code></pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre><code class="lang-python">def test_bad_inputs(self): 
    r = self.cvt.convert_to_roman 
    d = self.cvt.convert_to_decimal 
    edges = [("equals", r, "", None),\ </code></pre>
<p>Any command-line input or output is written as follows:</p>
<pre><code class="lang-python">$ python recipe3.py</code></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or words that you see on screen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Pick a class to test. This is known as the&nbsp;<strong>class under test</strong>."</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Warnings or important notes appear like this.</p>
</div>
</div>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>Tips and tricks appear like this.</p>
</div>
</div>
<h2>Sections</h2>
<p>In this book, you will find several headings that appear frequently (<em>Getting ready</em>,&nbsp;<em>How to do it...</em>, <em>How it works...</em>, <em>There's more...</em>, and <em>See also</em>).</p>
<p>To give clear instructions on how to complete a recipe, use these sections as follows:</p>
<h3>Getting ready</h3>
<p>This section tells you what to expect in the recipe and describes how to set up any software or any preliminary settings required for the recipe.</p>
<h3>How to do it&hellip;</h3>
<p>This section contains the steps required to follow the recipe.</p>
<h3>How it works&hellip;</h3>
<p>This section usually consists of a detailed explanation of what happened in the previous section.</p>
<h3>There's more&hellip;</h3>
<p>This section includes of additional information about the recipe in order to make you more knowledgeable about the recipe.</p>
<h3>See also</h3>
<p>This section provides helpful links to other useful information for the recipe.</p>

<h2>Get in touch</h2>
<p>Feedback from our readers is always welcome.</p>
<p><strong>General feedback</strong>: Email <kbd>feedback@packtpub.com</kbd>&nbsp;and mention the book title in the subject of your message. If you have questions about any aspect of this book, please email us at <kbd>questions@packtpub.com</kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit&nbsp;<a href="http://www.packtpub.com/submit-errata" target="_blank" rel="noopener">www.packtpub.com/submit-errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packtpub.com</kbd> with a link to the material.</p>
<p><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank" rel="noopener">authors.packtpub.com</a>.</p>
<h3>Reviews</h3>
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="https://www.packtpub.com/" target="_blank" rel="noopener">packtpub.com</a>.</p>


</div>
</div>


<!--Chapter 1-->

<div class="chapter" data-chapter-number="1">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 1 </span></div>
<h1 class="chaptertitle">Using Unittest to Develop Basic Tests</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>

<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Asserting the basics</li>
<li>Setting up and tearing down a test harness</li>
<li>Running test cases from the command line</li>
<li>Running a subset of test case methods</li>
<li>Chaining together a suite of tests</li>
<li>Defining test suites inside the test module</li>
<li>Retooling old test code to run inside unittest</li>
<li>Breaking down obscure tests into simple ones</li>
<li>Testing the edges</li>
<li>Testing corner cases by iteration</li>
</ul>
<h2>Introduction</h2>
<p>Testing has always been a part of software development. However, the world was introduced to a new concept called <strong>automated testing</strong> when Kent Beck and Erich Gamma introduced JUnit for Java development (<a href="http://junit.org">http://junit.org</a>). It was based on Kent's earlier work with Smalltalk and automated testing<a href="http://www.xprogramming.com/testfram.htm)">.</a> Currently, automated testing has become a well-accepted concept in the software industry.</p>
<p>A Python version, originally dubbed <strong>PyUnit</strong>, was created in 1999 and added to Python's standard set of libraries later in 2001 in Python 2.1. Currently, the PyUnit library is available for both versions of Python, that is, 2.7 (<a href="https://docs.python.org/2.7/library/unittest.html">https://docs.python.org/2.7/library/unittest.html</a>) and 3.x (<a href="https://docs.python.org/3.6/library/unittest.html">https://docs.python.org/3.6/library/unittest.html</a>). Since then, the Python community has referred to it as <strong>unittest</strong>, the name of the library imported into the test code.</p>
<p>Unittest is the foundation of automated testing in the Python world. In this chapter, we will explore the basics of testing and asserting code functionality, building suites of tests, test situations to avoid, and finally testing edges and corner cases.</p>
<p>For all the recipes in this chapter, we will use <kbd>virtualenv</kbd> (<a href="https://pypi.python.org/pypi/virtualenv">https://pypi.python.org/pypi/virtualenv</a>) to create a controlled Python runtime environment. Unittest is part of the standard library, which requires no extra installation steps. But in later chapters, using <kbd>virtualenv</kbd> will allow us to conveniently install other test tools, without cluttering up our default Python installation. The steps to install <kbd>virtualenv</kbd> are as follows:</p>
<ol>
<li>To install <kbd>virtualenv</kbd>, either download it from the site mentioned previously or if you have Easy Install, just type: <kbd>easy_install virtualenv</kbd>. You can also use <kbd>pip install virtualenv</kbd> as well.</li>
</ol>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>For some systems, you may need to install it either as <kbd>root</kbd> or by using&nbsp;<kbd>sudo</kbd>.</p>
</div>
</div>
<ol start="2">
<li>After installing&nbsp;<kbd>virtualenv</kbd>, use it to create a clean environment named&nbsp;<kbd>ptc</kbd>&nbsp;(an abbreviation used for <em>Python Testing Cookbook</em>) by using <kbd>--no-site-packages</kbd>.</li>
<li>Activate the virtual Python environment. This can vary, depending on which shell you are using. Take a look at this screenshot:
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000020.png" class="lazyload" /></p>
</li>
<li>For the Windows platform, you can either select the folder where you want to create the&nbsp;<kbd>ptc</kbd> folder or you can directly get it created in your desired drive. Look at this screenshot:
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000010.png" class="lazyload" /></p>
</li>
<li>Finally, verify that the environment is active by checking the path of <kbd>pip</kbd>.</li>
</ol>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>For more information on the usage and benefits of <kbd>virtualenv</kbd>, please read&nbsp;<a href="http://iamzed.com/2009/05/07/a-primer-on-virtualenv">http://iamzed.com/2009/05/07/a-primer-on-virtualenv</a>.</p>
</div>
</div>
<h2>Asserting the basics</h2>
<p>The basic concept of an automated unittest test case is to instantiate part of our code, subject it to operations, and verify certain results using assertions:</p>
<ul>
<li>If the results are as expected, unittest counts it as a test success</li>
<li>If the results don't match, an exception is thrown, and unittest counts it as a test failure</li>
</ul>
<h3>Getting ready</h3>
<p>Unittest was added to Python's standard batteries included library suite and doesn't require any extra installation.</p>
<h3>How to do it...</h3>
<p>With these steps, we will code a simple program and then write some automated tests using unittest:</p>
<ol>
<li>Create a new file called <kbd>recipe1.py</kbd> for this recipe's code. Pick a class to test. This is known as the <strong>class under test</strong>. For this recipe, we'll pick a class that uses a simplistic Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object):
    def __init__ (self, roman_numeral): 
        self.roman_numeral = roman_numeral 
        self.digit_map = {"M":1000, "D":500,"C":100,\
                         "L":50, "X":10, "V":5, "I":1} 
     def convert_to_decimal(self): 
        val = 0 
        for char in self.roman_numeral: 
            val += self.digit_map[char] 
        return val </code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This Roman numeral converter applies the simple rules of addition, but it doesn't have the special subtraction patterns such as <kbd>XL</kbd> mapping to <kbd>40</kbd>. The purpose is not to have the best Roman numeral converter, but to observe the various test assertions.</p>
</div>
</div>
<ol start="2">
<li>Write a new class and give it the same name with <kbd>Test</kbd> appended to the end, subclassing <kbd>unittest.TestCase</kbd>. Appending a test class with <kbd>Test</kbd> is a common convention, but not a requirement. Extending <kbd>unittest.TestCase</kbd> is a requirement needed to hook into unittest's standard test runner:</li>
</ol>
<pre><code class="lang-python">import unittest 
class RomanNumeralConverterTest(unittest.TestCase): </code></pre>
<ol start="3">
<li>Create several methods with names starting with <kbd>test</kbd>, so they are automatically picked up by the test number of unittest:</li>
</ol>
<pre><code class="lang-python">     def test_parsing_millenia(self):
        value =RomanNumeralConverter("M") 
        self.assertEqual(1000, value.convert_to_decimal()) 
     def test_parsing_century(self): 
        value =RomanNumeralConverter("C") 
        self.assertEqual(100, value.convert_to_decimal()) 
     def test_parsing_half_century(self): 
        value =RomanNumeralConverter("L") 
        self.assertEqual(50, value.convert_to_decimal()) 
     def test_parsing_decade(self): 
        value =RomanNumeralConverter("X") 
        self.assertEqual(10, value.convert_to_decimal()) 
     def test_parsing_half_decade(self): 
        value =RomanNumeralConverter("V") 
        self.assertEqual(5, value.convert_to_decimal()) 
     def test_parsing_one(self): 
        value = RomanNumeralConverter("I") 
        self.assertEqual(1, value.convert_to_decimal()) 
     def test_empty_roman_numeral(self): 
        value =RomanNumeralConverter("") 
        self.assertTrue(value.convert_to_decimal() == 0) 
        self.assertFalse(value.convert_to_decimal() &gt; 0) 
     def test_no_roman_numeral(self): 
        value =RomanNumeralConverter(None) 
        self.assertRaises(TypeError, value.convert_to_decimal) </code></pre>
<ol start="4">
<li>Make the entire script runnable and then use unittest's test runner:</li>
</ol>
<pre><code class="lang-python">if __name__=="__main__": 
    unittest.main()</code></pre>
<ol start="5">
<li>Run the file from the command line, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000156.jpg" class="lazyload" /></p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p><kbd>self.assertEquals()</kbd> has been deprecated in Python 3.</p>
</div>
</div>
<h3>How it works...</h3>
<p>In the first step, we picked a class to test. Next, we created a separate class to test. By naming the test class as <kbd>[class under test]Test</kbd>, it is easy to tell which class is under test. Each test method name must start with <kbd>test</kbd>, so that unittest will automatically pick it up and run it. To add more tests, just define more <kbd>test</kbd> methods. Each of these tests utilizes various assertions:</p>
<ul>
<li><kbd>assertEqual(first, second[, msg])</kbd>: Compares first and second expressions and fails if they don't have the same value. We can optionally print a special message if there is a failure.</li>
<li><kbd>assertTrue(expression[, msg])</kbd>: Tests the expression and fails if it is false. We can optionally print a special message if there is a failure.</li>
<li><kbd>assertFalse(expression[, msg])</kbd>: Tests the expression and fails if it is true. We can optionally print a special message if there is a failure.</li>
<li><kbd>assertRaises(exception, callable, ...)</kbd>: Runs the callable with any arguments, for the callable listed afterwards, and fails if it doesn't raise the exception.</li>
</ul>
<h3>There's more...</h3>
<p>Unittest provides many options for asserting, failing, and other convenient options. The following sections show some recommendations on how to pick and choose from these options.</p>
<h4>assertEquals is preferred over assertTrue and assertFalse</h4>
<p>When an <kbd>assertEquals</kbd>&nbsp;assertion fails, the first and second values are printed in the error report, giving a better feedback of what went wrong, whereas <kbd>assertTrue</kbd> and <kbd>assertFalse</kbd> simply report failure. Not all testable results fit this, but, if possible, use <kbd>assertEquals</kbd>.</p>
<p>It's important to understand the concept of equality. When comparing integers, strings, and other scalars, it's very simple. It doesn't work as well with collections such as dictionaries, lists, and sets. Complex, custom-defined objects may carry custom definitions of equality. These complex objects may require more fine-grained assertions. That is why it's probably a good idea to also include some test methods that directly target equality and inequality when working with custom objects.</p>
<h4>self.fail([msg]) can usually be rewritten with assertions</h4>
<p>Unittest has a <kbd>self.fail([msg])</kbd> operation that unconditionally causes the test to fail, along with an optional message. This was not shown earlier because it is not recommended for use.</p>
<p>The <kbd>fail</kbd> method is often used to detect certain situations such as exceptions. A common idiom is as follows:</p>
<pre><code class="lang-python">import unittest 
class BadTest(unittest.TestCase): 
  def test_no_roman_number(self): 
    value = RomanNumeralConverter(None) 
    try: 
      value.convert_to_decimal() 
      self.fail("Expected a TypeError") 
    except TypeError: 
      pass 
    if  __name__=="__main__": 
      unittest.main()</code></pre>
<p>This tests the same behavior as the earlier <kbd>test_no_roman_numeral</kbd>. The problem with this approach is that when the code is working properly the fail method is never executed. Code not executed regularly is at risk of becoming out of date and invalid. This will also interfere with coverage reports. Instead, it is better to use <kbd>assertRaises</kbd> as we used in the earlier examples. For other situations look at rewriting the test using the other assertions.</p>
<h4>Our version of Python can impact our options</h4>
<p>Python's official documentation on unittest shows many other assertions; however, they depend on the version of Python we are using. Some have been deprecated; others are only available in later versions, such as Python 3.6.</p>
<p>If our code must support multiple versions of Python then we must use the lowest common denominator. This recipe shows core assertions available in all versions since Python 3.6.</p>
<h2>Setting up and tearing down a test harness</h2>
<p>Unittest provides an easy mechanism to configure the state of the system when a piece of code is put through a test. It also allows us to clean things up afterward, if needed. This is commonly needed when a particular test case has repetitive steps used in every test method.</p>
<p>Barring any references to external variables or resources that carry state from one test method to the next, each test method starts from the same state.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will set up and teardown a test harness for each test method:</p>
<ol>
<li>Create a new file called <kbd>recipe2.py</kbd> for the code in this recipe.</li>
<li>Pick a class to test. In this case, we will use a slightly altered version of our Roman numeral converter, where the function, not the constructor, provides the input value to convert:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
        self.digit_map = {"M":1000, "D":500, "C":100,\
                         "L":50, "X":10, "V":5, "I":1} 
    def convert_to_decimal(self, roman_numeral):
        val = 0 
        for char in roman_numeral: 
            val += self.digit_map[char] 
        return val </code></pre>
<ol start="3">
<li>Create a class to test using the same name as the class under test with <kbd>Test</kbd> appended to the end:</li>
</ol>
<pre><code class="lang-python">import unittest 
class RomanNumeralConverterTest(unittest.TestCase): </code></pre>
<ol start="4">
<li>Create a <kbd>setUp</kbd> method that creates an instance of the class under test:</li>
</ol>
<pre><code class="lang-python">    def setUp(self): 
        print ("Creating a new RomanNumeralConverter...") 
        self.cvt =RomanNumeralConverter()</code></pre>
<ol start="5">
<li>Create a <kbd>tearDown</kbd> method that destroys the instance of the class under test:</li>
</ol>
<pre><code class="lang-python">     def tearDown(self): 
        print ("Destroying the RomanNumeralConverter...") 
        self.cvt = None </code></pre>
<ol start="6">
<li>Create all the test methods using <kbd>self.converter</kbd>:</li>
</ol>
<pre><code class="lang-python">     def test_parsing_millenia(self):
        self.assertEqual(1000,\
                         self.cvt.convert_to_decimal("M")) 
     def test_parsing_century(self): 
        self.assertEqual(100, \
                          self.cvt.convert_to_decimal("C")) 
     def test_parsing_half_century(self): 
        self.assertEqual(50,\
                         self.cvt.convert_to_decimal("L")) 
     def test_parsing_decade(self): 
        self.assertEqual(10,self.cvt.convert_to_decimal("X")) 
     def test_parsing_half_decade(self): 
        self.assertEqual(5,self.cvt.convert_to_decimal("V")) 
     def test_parsing_one(self): 
        self.assertEqual(1,self.cvt.convert_to_decimal("I")) 
     def test_empty_roman_numeral(self): 
        self.assertTrue(self.cvt.convert_to_decimal() == 0) 
        self.assertFalse(self.cvt.convert_to_decimal() &gt; 0) 
     def test_no_roman_numeral(self): 
        self.assertRaises(TypeError,\
                          self.cvt.convert_to_decimal,None)</code></pre>
<ol start="7">
<li>Make the entire script runnable and then use the test runner of unittest:</li>
</ol>
<pre><code class="lang-python">if __name__=="__main__": 
     unittest.main()</code></pre>
<ol start="8">
<li>Run the file from the command line, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000146.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In the first step, we picked a class to test. Next, we created a separate test class. By naming the test class <kbd>[class under test]Test</kbd>, it is easy to tell which class is under test.</p>
<p>Then, we defined a <kbd>setUp</kbd> method that unittest runs before every <kbd>Test</kbd> method. Next, we created a <kbd>tearDown</kbd> method that unittest runs after every <kbd>Test</kbd> method. In this case, we added a print statement in each of them to demonstrate unittest rerunning these two methods for every test method. In reality, it would probably add too much noise to our testing.</p>
<p>One deficiency of unittest is the lack of <kbd>setUpClass</kbd>/<kbd>tearDownClass</kbd> and <kbd>setUpModule</kbd>/<kbd>tearDownModule</kbd>, providing the opportunity to run code in greater scopes than at the test method level. This has been added to <kbd>unittest2</kbd>.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p><strong>Each test case can have one setUp and one tearDown method:&nbsp;</strong>Our <kbd>RomanNumeralConverter</kbd> is pretty simple and fits easily into a single test class. But the test class allows only one <kbd>setUp</kbd> method and one <kbd>tearDown</kbd> method. If different combinations of <kbd>setUp</kbd>/<kbd>tearDown</kbd> methods are needed for various test scenarios, then this is a cue to code more test classes. Just because we write a <kbd>setUp</kbd> method, doesn't mean we need a <kbd>tearDown</kbd> method. In our case, we could have skipped destroying the <kbd>RomanNumeralConverter</kbd>, because a new instance would be replacing it for every test method. It was really for demonstration purposes only. What are the other uses of those cases that need a <kbd>tearDown</kbd> method? Using a library that requires some sort of close operation is a ripe candidate for writing a <kbd>tearDown</kbd> method.</p>
</div>
</div>
<h2>Running test cases from the command line</h2>
<p>It is easy to adjust the test runner to print out every test method as it is run.</p>
<h3>How to do it...</h3>
<p>In the following steps, we will run test cases with more detailed output, giving us better insight into how things run:</p>
<ol start="1">
<li>Create a new file called <kbd>recipe3.py</kbd> for this recipe's code.</li>
<li>Pick a class to test. In this case, we will use our Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self, roman_numeral): 
        self.roman_numeral = roman_numeral 
        self.digit_map = {"M":1000, "D":500, "C":100, "L":50,\
                           "X":10,"V":5, "I":1} 
 
    def convert_to_decimal(self):
        val = 0 
        for char in self.roman_numeral:
            val += self.digit_map[char] 
        return val</code></pre>
<ol start="3">
<li>Create a test class using the same name as the class under test with <kbd>Test</kbd> appended to the end:</li>
</ol>
<pre><code class="lang-python">import unittest
class RomanNumeralConverterTest(unittest.TestCase): </code></pre>
<ol start="4">
<li>Create several test methods. For this recipe, the second test have been deliberately coded to fail:</li>
</ol>
<pre><code class="lang-python">def test_parsing_millenia(self): 
    value =RomanNumeralConverter("M") 
    self.assertEqual(1000, value.convert_to_decimal()) 

def test_parsing_century(self): 
    "This test method is coded to fail for demo."
     value =RomanNumeralConverter("C") 
     self.assertEqual(10, value.convert_to_decimal()) </code></pre>
<ol start="5">
<li>Define a test suite that automatically loads all the test methods, and then runs them with the higher level of verbosity:</li>
</ol>
<pre><code class="lang-python">if __name__== "__main__": 
    suite = unittest.TestLoader().loadTestsFromTestCase(\
              RomanNumeralConverterTest) 
    unittest.TextTestRunner(verbosity=2).run(suite) </code></pre>
<ol start="6">
<li>Run the file from the command line. Notice how, in this screenshot the test method that fails, prints out its Python docstring:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000136.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>A key part of automated testing is organizing the tests. The base units are called <strong>test cases</strong>. These can be combined together into <strong>test suites</strong>. Python's unittest module provides <kbd>TestLoader().loadTestsFromTestCase</kbd> to fetch all the <kbd>test*</kbd> methods automatically into a test suite. This test suite is then run through unittest's <kbd>TextTestRunner</kbd> with an increased level of verbosity.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p><kbd>TextTestRunner</kbd> is unittest's only test runner. Later in this book, we will look at other test tools that have different runners, including the one that plugs in a different unittest test runner.</p>
</div>
</div>
<p>The previous screenshot shows each method along with its module and class name, as well as success/failure.</p>
<h3>There's more...</h3>
<p>This recipe not only demonstrates how to turn up the verbosity of running tests, but also shows what happens when a test case fails. It renames the <kbd>test</kbd> method with the document string embedded in the <kbd>test</kbd> method, and prints the details later after all the test methods have been reported.</p>
<h2>Running a subset of test case methods</h2>
<p>Sometimes, it's convenient to run only a subset of test methods in a given test case. This recipe will show how to run either the whole test case, or pick a subset from the command line.</p>
<h3>How to do it...</h3>
<p>The following steps show how to code a command-line script to run subsets of tests:</p>
<ol>
<li>Create a new file named <kbd>recipe4.py</kbd> to put all the code for this recipe.</li>
<li>Pick a class to test. In this case, we will use our Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object):
    def __init__(self, roman_numeral): 
        self.roman_numeral = roman_numeral 
        self.digit_map = {"M":1000, "D":500,\
                        "C":100, "L":50, "X":10, "V":5, "I":1} 
 
    def convert_to_decimal(self):
        val = 0 
        for char in self.roman_numeral: 
            val+=self.digit_map[char]
        return val</code></pre>
<ol start="3">
<li>Create a test class using the same name as the class under test with <kbd>Test</kbd> appended to the end:</li>
</ol>
<pre><code class="lang-python">import unittest 
class RomanNumeralConverterTest(unittest.TestCase): </code></pre>
<ol start="4">
<li>Create several <kbd>test</kbd> methods:</li>
</ol>
<pre><code class="lang-python">    def test_parsing_millenia(self):
        value = RomanNumeralConverter("M") 
        self.assertEquals(1000, value.convert_to_decimal()) 
 
    def test_parsing_century(self):
        value = RomanNumeralConverter("C") 
        self.assertEquals(100, value.convert_to_decimal()) </code></pre>
<ol start="5">
<li>Write a main runner that either runs the entire test case or accepts a variable number of test methods:</li>
</ol>
<pre><code class="lang-python">if __name__= "__main__":
    import sys
    suite = unittest.TestSuite()
    if len(sys.argv) == 1:
        suite = unittest.TestLoader().loadTestsFromTestCase(\                                                                       RomanNumeralConverterTest) 
    else: 
        for test_name in sys.argv[1:]:
            suite.addTest(RomanNumeralConverterTest(test_name))

    unittest.TextTestRunner(verbosity=2).run(suite) </code></pre>
<ol start="6">
<li>Run the recipe with no extra command-line arguments and see it run all the tests, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000128.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>For this test case, we coded a couple of test methods. But instead of simply running all the tests, or defining a fixed list, we used Python's <kbd>sys</kbd> library to parse the command-line arguments. If there are no extra arguments, it runs the entire test case. If there are extra arguments, then they are assumed to be test method names. It uses unittest's inbuilt ability to specify test method names when instantiating <kbd>RomanNumeralConverterTest</kbd>.</p>
<h2>Chaining together a suite of tests</h2>
<p>Unittest makes it easy to chain together test cases into a <kbd>TestSuite</kbd>. A <kbd>TestSuite</kbd> can be run just like a <kbd>TestCase</kbd>, but it also provides additional functionality to add a single/multiple tests, and count them.</p>
<p>Why do we need this? Chaining together tests into a suite gives us the ability to pull together more than one module of test cases for a test run, as well as picking and choosing a subset of test cases. Up until now, we have generally run all the test methods from a single class. <kbd>TestSuite</kbd> gives us an alternative means to define a block of testing.</p>
<h3>How to do it...</h3>
<p>In the following steps, we will code multiple test case classes, and then load their test methods into suites so we can run them:</p>
<ol>
<li>Create a new file named <kbd>recipe5.py</kbd> to put our sample application and test cases.</li>
<li>Pick a class to test. In this case, we will use our Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
            self.digit_map = {"M":1000, "D":500,\
                        "C":100, "L":50, "X":10, "V":5, "I":1} 
 
    def convert_to_decimal(self, roman_numeral):
            val = 0 
            for char in roman_numeral: 
                val += self.digit_map[char] 
            return val </code></pre>
<ol start="3">
<li>Create two test classes with various test methods spread between them:</li>
</ol>
<pre><code class="lang-python">import unittest 
class RomanNumeralConverterTest(unittest.TestCase): 
    def setUp(self): 
        self.cvt = RomanNumeralConverter()
    def test_parsing_millenia(self): 
        self.assertEquals(1000, \ 
                    self.cvt.convert_to_decimal("M")) 
 
    def test_parsing_century(self): 
        self.assertEquals(100, \ 
                    self.cvt.convert_to_decimal("C")) 
 
class RomanNumeralComboTest(unittest.TestCase):
    def setUp(self):
        self.cvt=RomanNumeralConverter()
    def test_multi_millenia(self):
        self.assertEquals(4000,\
    def test_multi_add_up(self): 
        self.assertEquals(2010, \ 
        self.cvt.convert_to_decimal("MMX"))</code></pre>
<ol start="4">
<li>Create a test runner in a separate file named <kbd>recipe5_runner.py</kbd> that pulls in both test cases:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__": 
    import unittest 
    from recipe5 import * 
    suite1 = unittest.TestLoader().loadTestsFromTestCase( \  
                RomanNumeralConverterTest) 
    suite2 = unittest.TestLoader().loadTestsFromTestCase( \ 
                RomanNumeralComboTest) 
    suite = unittest.TestSuite([suite1, suite2])     
    unittest.TextTestRunner(verbosity=2).run(suite)</code></pre>
<ol start="5">
<li>Execute the test runner, and observe from this screenshot how tests are pulled in from both test cases.</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000117.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The unittest module provides a convenient way to find all the test methods in a <kbd>TestClass</kbd> and bundle them together as a suite using its <kbd>loadTestsFromTestCase</kbd>. To further the usage of test suites, we are able to combine these two suites together as a single suite using <kbd>unittest.TestSuite([list...])</kbd>. The <kbd>TestSuite</kbd> class is designed to act as a <kbd>TestCase</kbd> class does, even though it doesn't subclass <kbd>TestClass</kbd>, allowing us to run it using <kbd>TextTestRunner</kbd>. This recipe shows the verbosity turned up, allowing us to see exactly what test methods were run, and from what test case they came from.</p>
<h3>There's more...</h3>
<p>In this recipe, we ran the tests from a different file than where the test cases are defined. This is different than the previous recipes where the runnable code and the test case were contained in the same file. Since the runner is defining the tests we run, we can easily create more runners that combine different suites of tests.</p>
<h4>The name of the test case should be significant</h4>
<p>In the previous recipes, it has been advised to name the test case as <kbd>[class under test]Test</kbd>. This is to make it apparent to the reader that the class under test and the related test share an important relationship. Now that we are introducing another test case, we need to pick a different name. The name should explain clearly why these particular test methods are split out into a separate class. For this recipe, the methods are split out to show more complex combinations of Roman numerals.</p>
<h2>Defining test suites inside the test module</h2>
<p>Each test module can provide one or more methods that define a different test suite. One method can exercise all the tests in a given module, another method can define a particular subset.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will create some methods that define test suites using different means:</p>
<ol>
<li>Create a new file called <kbd>recipe6.py</kbd> to put our code for this recipe.</li>
<li>Pick a class to test. In this case, we will use our Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
        self.digit_map = {"M":1000, "D":500, "C":100, "L":50, "X":10, "V":5, "I":1} 
 
    def convert_to_decimal(self, roman_numeral): 
    val = 0 
    for char in roman_numeral: 
        val += self.digit_map[char] 
    return val </code></pre>
<ol start="3">
<li>Create a test class using the same name as the class under test with <kbd>Test</kbd> appended to the end:</li>
</ol>
<pre><code class="lang-python">import unittest 
class RomanNumeralConverterTest(unittest.TestCase): </code></pre>
<ol start="4">
<li>Write a series of test methods, including a <kbd>setUp</kbd> method that creates a new instance of the <kbd>RomanNumeralConverter</kbd> for each test method:</li>
</ol>
<pre><code class="lang-python">import unittest 
 
class RomanNumeralConverterTest(unittest.TestCase): 
    def setUp(self): 
        self.cvt = RomanNumeralConverter() 
 
    def test_parsing_millenia(self): 
        self.assertEquals(1000, \ 
             self.cvt.convert_to_decimal("M")) 
 
    def test_parsing_century(self): 
        self.assertEquals(100, \ 
            self.cvt.convert_to_decimal("C")) 
 
    def test_parsing_half_century(self): 
        self.assertEquals(50, \ 
            self.cvt.convert_to_decimal("L")) 
 
    def test_parsing_decade(self): 
        self.assertEquals(10, \ 
            self.cvt.convert_to_decimal("X")) 
 
    def test_parsing_half_decade(self): 
        self.assertEquals(5, self.cvt.convert_to_decimal("V")) 
 
    def test_parsing_one(self): 
        self.assertEquals(1, self.cvt.convert_to_decimal("I")) 
 
    def test_empty_roman_numeral(self):     
        self.assertTrue(self.cvt.convert_to_decimal("") == 0) 
        self.assertFalse(self.cvt.convert_to_decimal("") &gt; 0) 

    def test_no_roman_numeral(self): 
        self.assertRaises(TypeError, \ 
            self.cvt.convert_to_decimal, None) 
 
    def test_combo1(self): 
        self.assertEquals(4000, \ 
            self.cvt.convert_to_decimal("MMMM")) 
 
    def test_combo2(self): 
        self.assertEquals(2010, \ 
            self.cvt.convert_to_decimal("MMX")) 
 
    def test_combo3(self): 
        self.assertEquals(4668, \ 
            self.cvt.convert_to_decimal("MMMMDCLXVIII")) </code></pre>
<ol start="5">
<li>Create some methods in the recipe's module (but not in the test case) that define different test suites:</li>
</ol>
<pre><code class="lang-python">def high_and_low(): 
    suite = unittest.TestSuite() 
    suite.addTest(\ 
        RomanNumeralConverterTest("test_parsing_millenia"))    
    suite.addTest(\ 
        RomanNumeralConverterTest("test_parsing_one")) return suite 
def combos(): 
    return unittest.TestSuite(map(RomanNumeralConverterTest,\    
        ["test_combo1", "test_combo2", "test_combo3"])) 
def all(): 
    return unittest.TestLoader().loadTestsFromTestCase(\   
            RomanNumeralConverterTest) </code></pre>
<ol start="6">
<li>Create a runner that will iterate over each of these test suites and run them through unittest's <kbd>TextTestRunner</kbd>:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__": 
    for suite_func in [high_and_low, combos, all]: 
        print ("Running test suite '%s'" % suite_func.__name__)  
        suite = suite_func()    
        unittest.TextTestRunner(verbosity=2).run(suite)</code></pre>
<ol start="7">
<li>Run the combination of test suites, and see the results. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000108.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We pick a class to test and define a number of test methods that check things out. Then we define a few module-level methods such as, <kbd>high_and_low</kbd>, <kbd>combos</kbd>, and <kbd>all</kbd>, to define test suites. Two of them contain fixed subsets of methods while <kbd>all</kbd> dynamically loads the <kbd>test*</kbd> methods from the class. Finally, the main part of our module iterates over a listing of all these functions that generate suites to smoothly create and run them.</p>
<h3>There's more...</h3>
<p>All of our test suites were run from the recipe's main runner. But this probably wouldn't be the case for a real project. Instead, the idea is to define different suites, and code a mechanism to pick which suite to run. Each suite is geared towards a different purpose, and it is necessary to allow the developer to pick which suite to run. This can be done by coding a command-line script using Python's optparse module to define command-line flags to pick one of these suites.</p>
<h4>Test suite methods must be outside of the test class</h4>
<p>If we make these suite-defining methods members of the test class, we would have to instantiate the test class. Classes that extend <kbd>unittest.TestCase</kbd> have a specialized <kbd>init</kbd> method that doesn't work well with an instance that is created just to call a non-test method. That is why the methods are outside the test class. While these methods can be in other modules, it is very convenient to define them inside the module containing the test code, to keep things in proximity.</p>
<h4>Why have different suites?</h4>
<p>What if we started our project off by running all tests? Sounds like a good idea, right? But what if the time to run the entire test suite grew to over an hour? There is a certain threshold after which developers tend to stop running tests, and <em>nothing is worse than an un-run test</em><em>suite</em>. By defining subsets of tests, it is easy to run alternate suites during the day, and then perhaps run the comprehensive test suite once a day. Bear in mind the following:</p>
<ul>
<li><kbd>all</kbd> is the comprehensive suite</li>
<li><kbd>high_and_low</kbd> is an example of testing the edges</li>
<li><kbd>combos</kbd> is a random sampling of values used to show that things are generally working</li>
</ul>
<p>Defining our test suites is a judgment call. It's also worth re-evaluating each test suite every so often. If one test suite is getting too costly to run, consider moving some of its more expensive tests to another suite.</p>
<h4>optparse is being phased out and replaced by argparse</h4>
<p>While <kbd>optparse</kbd> is a convenient way to add command-line flags to Python scripts, it won't be available forever. Python 2.7 has deprecated this module and is continuing this development in <kbd>argparse</kbd>.</p>
<h2>Retooling old test code to run inside unittest</h2>
<p>Sometimes, we may have developed demo code to exercise our system. We don't have to rewrite it to run it inside unittest. Instead, it is easy to hook it up to the test framework and run it with some small changes.</p>
<h3>How to do it...</h3>
<p>With these steps, we will dive into capturing the test code that was written without using unittest, and repurposing it with minimal effort to run inside unittest:</p>
<ol>
<li>Create a file named <kbd>recipe7.py</kbd> to put our application code that we will be testing.</li>
<li>Pick a class to test. In this case, we will use our Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
        self.digit_map = {"M":1000, "D":500, "C":100, "L":50, "X":10, "V":5, "I":1} 
 
    def convert_to_decimal(self, roman_numeral): 
        val = 0 
        for char in roman_numeral: 
            val += self.digit_map[char] 
        return val </code></pre>
<ol start="3">
<li>Create a new file named <kbd>recipe7_legacy.py</kbd> to contain test code that doesn't use the unittest module.</li>
<li>Create a set of legacy tests that are coded, based on Python's <kbd>assert</kbd> function, not with unittest, along with a runner:</li>
</ol>
<pre><code class="lang-python">from recipe7 import * 
class RomanNumeralTester(object): 
  def   init  (self): 
    self.cvt = RomanNumeralConverter() 
  def simple_test(self):
    print ("+++ Converting M to 1000")
    assert self.cvt.convert_to_decimal("M") == 1000
  def combo_test1(self): 
    print ("+++ Converting MMX to 2010") 
    assert self.cvt.convert_to_decimal("MMXX") == 2010 
  def combo_test2(self): 
    print ("+++ Converting MMMMDCLXVIII to 4668") 
    val = self.cvt.convert_to_decimal("MMMMDCLXVII")         
    self.check(val, 4668) 
  def other_test(self): 
    print ("+++ Converting MMMM to 4000") 
    val = self.cvt.convert_to_decimal("MMMM") 
    self.check(val, 4000) 
  def check(self, actual, expected): 
    if (actual != expected): 
      raise AssertionError("%s doesn't equal %s" % \ 
            (actual,  expected)) 
  def test_the_system(self): 
    self.simple_test() 
    self.combo_test1() 
    self.combo_test2() 
    self.other_test() 
if __name == "__main__": 
  tester = RomanNumeralTester() 
  tester.test_the_system()</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This set of legacy tests is meant to represent legacy test code that our team has developed to exercise things before unittest was an option.</p>
</div>
</div>
<ol start="5">
<li>Run the legacy tests. What is wrong with this situation? Did all the test methods run? Have we caught all the bugs? Take a look at this screenshot:<br />
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000072.png" class="lazyload" /></p>
</li>
<li>Create a new file called <kbd>recipe7_pyunit.py</kbd>.</li>
<li>Create a unittest set of tests, wrapping each legacy test method inside unittest's <kbd>FunctionTestCase</kbd>:</li>
</ol>
<pre><code class="lang-python">from recipe7 import * 
from recipe7_legacy import * import unittest 
 
if __name__ == "__main__":  
    tester = RomanNumeralTester() 
    suite = unittest.TestSuite() 
    for test in [tester.simple_test, tester.combo_test1, \ 
            tester.combo_test2, tester.other_test]: 
        testcase = unittest.FunctionTestCase(test)   
        suite.addTest(testcase) 
    unittest.TextTestRunner(verbosity=2).run(suite)</code></pre>
<ol start="8">
<li>Run the unittest test. Did all the tests run this time? Which test failed? Where is the bug? Look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000118.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Python provides a convenient assert statement that tests a condition. When true, the code continues. When false, it raises an <kbd>AssertionError</kbd>. In the first test runner, we have several tests that check results using a mixture of <kbd>assert</kbd> statements or raising an <kbd>AssertionError</kbd>.</p>
<p>unittest provides a convenient class, <kbd>unittest.FunctionTestCase</kbd>, that wraps a bound function as a unittest test case. If an <kbd>AssertionError</kbd> is thrown, <kbd>FunctionTestCase</kbd> catches it, flags it as a test <em>failure</em>, and proceeds to the next test case. If any other type of exception is thrown, it will be flagged as a test error. In the second test runner, we wrap each of these legacy test methods with <kbd>FunctionTestCase</kbd>, and chain them together in a suite for unittest to run.</p>
<p>As seen by running the second test run, there is a bug lurking in the third test method. We were not aware of it because the test suite was prematurely interrupted.</p>
<p>Another deficiency of Python's assert statement is shown by the first failure, as seen in the previous screenshot. When an assert fails, there is little to no information about the values that were compared. All we have is the line of code where it failed. The second assert in that screenshot was more useful, because we coded a custom checker that threw a custom <kbd>AssertionError</kbd>.</p>
<h3>There's more...</h3>
<p>Unittest does more than just run tests. It has a built-in mechanism to trap errors and failures, and then it continues running as much of our test suite as possible. This helps, because we can shake out more errors and fix more things within a given test run. This is especially important when a test suite grows to the point of taking minutes or hours to run.</p>
<h4>Where are the bugs?</h4>
<p>They exist in the test methods, and fundamentally were made by making slight alterations to the Roman numeral being converted, as shown in the code:</p>
<pre><code class="lang-python">def combo_test1(self): 
    print ("+++ Converting MMX to 2010") 
    assert self.cvt.convert_to_decimal("MMXX") == 2010 
def combo_test2(self): 
    print ("+++ Converting MMMMDCLXVIII to 4668")
    val = self.cvt.convert_to_decimal("MMMMDCLXVII") 
    self.check(val, 4668) </code></pre>
<p>The <kbd>combo_test1</kbd> test method prints out that it is converting <kbd>MMX</kbd>, but actually tries to convert <kbd>MMXX</kbd>. The <kbd>combo_test2</kbd> test method prints out that it is converting <kbd>MMMMDCLXVIII</kbd>, but actually tries to convert <kbd>MMMMDCLXVII</kbd>.</p>
<p>This is a contrived example, but have you ever run into bugs just as small that drove you mad trying to track them down? The point is, showing how easy or hard it can be to track them down is based on how the values are checked. Python's <kbd>assert</kbd> statement isn't very effective at telling us what values are compared where. The customized <kbd>check</kbd> method is much better at pinpointing the problem with <kbd>combo_test2</kbd>.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This highlights the problem with having comments or print statements trying to reflect what the asserts do. They can easily get out of sync and the developer may face some problems trying to track down bugs. Avoiding this situation is known as the <strong>DRY</strong> principle (<strong>Don't Repeat Yourself</strong>).</p>
</div>
</div>
<h4>FunctionTestCase is a temporary measure</h4>
<p><kbd>FunctionTestCase</kbd>&nbsp;is a test case that provides an easy way to quickly migrate tests based on Python's <kbd>assert</kbd> statement, so they can be run with unittest. But things shouldn't stop there. If we take the time to convert <kbd>RomanNumeralTester</kbd> into a unittest <kbd>TestCase</kbd>, then we gain access to other useful features such as the various <kbd>assert*</kbd> methods that come with <kbd>TestCase</kbd>. It's a good investment. The <kbd>FunctionTestCase</kbd> just lowers the bar to migrate to unittest.</p>
<h2>Breaking down obscure tests into simple ones</h2>
<p>Unittest provides the means to test the code through a series of assertions. I have often felt the temptation to exercise many aspects of a particular piece of code within a single test method. If any part fails, it becomes obscured as to which part failed. It is preferable to split things up into several smaller test methods, so that when some part of the code under test fails, it is obvious.</p>
<h3>How to do it...</h3>
<p>With these steps, we will investigate what happens when we put too much into a single test method:</p>
<ol>
<li>Create a new file named&nbsp;<kbd>recipe8.py</kbd> to put out application code in for this recipe.</li>
</ol>
<ol start="2">
<li>Pick a class to test. In this case, we will use an alternative version of the Roman numeral converter, which converts both ways:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
        self.digit_map = {"M":1000, "D":500, "C":100, "L":50, "X":10, "V":5, "I":1} 
 
    def convert_to_decimal(self, roman_numeral): 
        val = 0 
        for char in roman_numeral: 
        val += self.digit_map[char] 
    return val 
 
    def convert_to_roman(self, decimal): 
        val = "" 
    while decimal &gt; 1000: 
        val += "M" 
        decimal -= 1000 
    while decimal &gt; 500: 
        val += "D"
        decimal -= 500 
    while decimal &gt; 100: 
        val += "C" 
        decimal -= 100 
    while decimal &gt; 50: 
        val += "L" 
        decimal -= 50 
    while decimal &gt; 10: 
        val += "X" 
        decimal -= 10 
    while decimal &gt; 5: 
        val += "V" 
        decimal -= 5 
    while decimal &gt; 1: 
        val += "I" 
        decimal -= 1 
    return val </code></pre>
<ol start="3">
<li>Create a new file called <kbd>recipe8_obscure.py</kbd> to put some longer test methods.</li>
<li>Create some test methods that combine several test assertions:</li>
</ol>
<pre><code class="lang-python">import unittest 
from recipe8 import * 
 
class RomanNumeralTest(unittest.TestCase): 
    def setUp(self): 
        self.cvt = RomanNumeralConverter() 
 
    def test_convert_to_decimal(self): 
        self.assertEquals(0, self.cvt.convert_to_decimal(""))     
        self.assertEquals(1, self.cvt.convert_to_decimal("I"))    
        self.assertEquals(2010, \ 
            self.cvt.convert_to_decimal("MMX")) 
        self.assertEquals(4000, \ 
            self.cvt.convert_to_decimal("MMMM")) 
    def test_convert_to_roman(self): 
        self.assertEquals("", self.cvt.convert_to_roman(0)) 
        self.assertEquals("II", self.cvt.convert_to_roman(2))     
        self.assertEquals("V", self.cvt.convert_to_roman(5))    
        self.assertEquals("XII", \ 
            self.cvt.convert_to_roman(12)) 
        self.assertEquals("MMX", \ 
            self.cvt.convert_to_roman(2010)) 
        self.assertEquals("MMMM", \ 
            self.cvt.convert_to_roman(4000))
 
if __name__ == "__main__":  
    unittest.main()</code></pre>
<ol start="5">
<li>Run the obscure tests. Why did it fail? Where is the bug? It reports that <kbd>II</kbd> is not equal to <kbd>I</kbd>, so something appears to be off. Is this the only bug? Create another file called <kbd>recipe8_clear.py</kbd> to create a more fine-grained set of test methods. Take a look at this screenshot:
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000132.png" class="lazyload" /></p>
</li>
</ol>
<ol start="6">
<li>Split up the assertions into separate test methods to give a higher fidelity of output:</li>
</ol>
<pre><code class="lang-python">import unittest 
from recipe8 import * 
 
class RomanNumeralTest(unittest.TestCase): 
    def setUp(self): 
        self.cvt = RomanNumeralConverter() 
 
    def test_to_decimal1(self): 
        self.assertEquals(0, self.cvt.convert_to_decimal("")) 
 
    def test_to_decimal2(self): 
        self.assertEquals(1, self.cvt.convert_to_decimal("I")) 
 
    def test_to_decimal3(self): 
        self.assertEquals(2010, \ 
            self.cvt.convert_to_decimal("MMX")) 
 
    def test_to_decimal4(self): 
        self.assertEquals(4000, \ 
            self.cvt.convert_to_decimal("MMMM")) 
 
    def test_convert_to_roman1(self): 
        self.assertEquals("", self.cvt.convert_to_roman(0)) 

    def test_convert_to_roman2(self): 
        self.assertEquals("II", self.cvt.convert_to_roman(2)) 

    def test_convert_to_roman3(self): 
        self.assertEquals("V", self.cvt.convert_to_roman(5)) 

    def test_convert_to_roman4(self): 
        self.assertEquals("XII", \ 
                    self.cvt.convert_to_roman(12)) 
 
    def test_convert_to_roman5(self): 
        self.assertEquals("MMX", \ 
                    self.cvt.convert_to_roman(2010)) 
 
    def test_convert_to_roman6(self): 
        self.assertEquals("MMMM", \ 
                    self.cvt.convert_to_roman(4000)) 
 
if __name__ == "__main__": 
unittest.main() </code></pre>
<ol start="8">
<li>Run the clearer test suite. Is it a bit clearer where the bug is? What did we trade in to get this higher degree of test failure? Was it worth the effort? Refer to this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000095.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In this case, we created a modified Roman numeral converter that converts both ways. We then started creating test methods to exercise things. Since each of these tests were a simple, one line assertion, it was convenient to put them all in the same test method.</p>
<p>In the second test case, we put each assertion into a separate test method. Running it exposes the fact that there are multiple bugs in this Roman numeral converter.</p>
<h3>There's more...</h3>
<p>When we started off writing tests, it was very convenient to bundle all these assertions into a single test method. After all, if everything is working, there is no harm, right? But what if everything does <em>not</em> work; what do we have to deal with? An obscure error report!</p>
<h4>Where is the bug?</h4>
<p>The obscured test runner may not be clear. All we have to go on is <kbd>II != I</kbd>&nbsp;which isn't much. The clue is that it is only off by one. The clear test runner gives more clues. We see that <kbd>V != IIII, XII != XI</kbd>, and some more. Each of these failures shows things being off by one.</p>
<p>The bug involves the various Boolean conditions in the while checks:</p>
<pre><code class="lang-python">while decimal &gt; 1000: 
while decimal &gt; 500: 
while decimal &gt; 100: 
while decimal &gt; 50: 
while decimal &gt; 10: 
while decimal &gt; 5: 
while decimal &gt; 1:</code></pre>
<p>Instead of testing greater than, it should test for <em>greater than</em> or <em>equal to</em>. This causes it to skip out of each Roman numeral before counting the last one.</p>
<h4>What is the right size for a test method?</h4>
<p>In this recipe, we broke things down to a single assertion per test. But I wouldn't advise thinking along these lines.</p>
<p>If we look a little closer, each test method also involves a single usage of the Roman numeral API. For the converter, there is only one result to examine when exercising the code. For other systems, the output may be more complex. It is completely warranted to use several assertions in the same test method to check the outcome by making that single call.</p>
<p>When we proceed to make more calls to the Roman numeral API, it should signal us to consider splitting it off into a new test method.</p>
<p>This raises the question: <em>What is a unit of code?</em> There has been much debate over what defines a unit of code, and what makes a good unit test. There are many opinions. Hopefully, reading this chapter and weighing it against the other test tactics covered throughout this book will help you enhance your own opinion and ultimately improve your own testing talent.</p>
<h4>Unittests versus integration tests</h4>
<p>Unittest can easily help us write both unittests as well as integration tests. Unittests exercise smaller blocks of code. When writing unittests, it is best to keep the testing as small and fine-grained as possible. Breaking testing up into lots of smaller tests is often a better approach to detecting and pinpointing bugs.</p>
<p>When we move up to a higher level (such as integration testing), it makes sense to test multiple steps in a single test method. But this is only recommended if there are adequate low-level unit tests. This will shed some light on whether it is broken at the unit level, or there exists a sequence of steps that causes the error.</p>
<p>Integration tests often extend to things such as external systems. For example, many argue that unit testing should never connect to a database, talk to an LDAP server, or interact with other systems.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Just because we are using unittest doesn't mean the tests we are writing are unit tests. Later in this book, we will visit the concept that unittest can be used to write many types of tests including integration tests, smoke tests, and other types as well.</p>
</div>
</div>
<h2>Testing the edges</h2>
<p>When we write automated tests, we pick the inputs and assert the expected outputs. It is important to test the limits of the inputs to make sure our code can handle good and bad inputs. This is also known as <strong>testing corner cases</strong>.</p>
<h3>How to do it...</h3>
<p>As we dig into this recipe, we will look for good boundaries to test against:</p>
<ol>
<li>Create a new file named <kbd>recipe9.py</kbd> for the code in this recipe.</li>
<li>Pick a class to test. In this recipe, we'll use another variation of our Roman numeral converter. This one doesn't process values greater than <kbd>4000</kbd>:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
      self.digit_map = {"M":1000, "D":500, "C":100, "L":50, "X":10, "V":5, "I":1} 
    def convert_to_decimal(self, roman_numeral): 
        val = 0 
        for char in roman_numeral: 
            val += self.digit_map[char] 
        if val &gt; 4000: 
        raise Exception("We don't handle values over 4000") 
    return val
 
    def convert_to_roman(self, decimal): 
        if decimal &gt; 4000: 
            raise Exception("We don't handle values over 4000") 
        val = "" 
        mappers = [(1000,"M"), (500,"D"), (100,"C"), (50,"L"), 
(10,"X"), (5,"V"), (1,"I")] 
        for (mapper_dec, mapper_rom) in mappers: 
            while decimal &gt;= mapper_dec: 
                val += mapper_rom 
                decimal -= mapper_dec 
        return val </code></pre>
<ol start="3">
<li>Create a test case that sets up an instance of the Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">import unittest 
 
class RomanNumeralTest(unittest.TestCase): 
    def setUp(self): 
      self.cvt = RomanNumeralConverter() </code></pre>
<ol start="4">
<li>Add several test methods that exercise the edges of converting to Roman numeral notation:</li>
</ol>
<pre><code class="lang-python">def test_to_roman_bottom(self): 
    self.assertEquals("I", self.cvt.convert_to_roman(1))  

def test_to_roman_below_bottom(self): 
    self.assertEquals("", self.cvt.convert_to_roman(0)) 

def test_to_roman_negative_value(self): 
    self.assertEquals("", self.cvt.convert_to_roman(-1)) 

def test_to_roman_top(self): 
    self.assertEquals("MMMM", \ 
                self.cvt.convert_to_roman(4000)) 

def test_to_roman_above_top(self): 
    self.assertRaises(Exception, \ 
                self.cvt.convert_to_roman, 4001) </code></pre>
<ol start="5">
<li>Add several test methods that exercise the edges of converting to decimal notation:</li>
</ol>
<pre><code class="lang-python">def test_to_decimal_bottom(self): 
    self.assertEquals(1, self.cvt.convert_to_decimal("I")) 

def test_to_decimal_below_bottom(self): 
    self.assertEquals(0, self.cvt.convert_to_decimal("")) 

def test_to_decimal_top(self):  
    self.assertEquals(4000, \ 
                self.cvt.convert_to_decimal("MMMM")) 

def test_to_decimal_above_top(self):      
    self.assertRaises(Exception, \ 
                self.cvt.convert_to_decimal, "MMMMI")</code></pre>
<ol start="6">
<li>Add some tests that exercise the tiers of converting decimals to Roman numerals:</li>
</ol>
<pre><code class="lang-python">def test_to_roman_tier1(self): 
    self.assertEquals("V", self.cvt.convert_to_roman(5)) 
 
def test_to_roman_tier2(self): 
    self.assertEquals("X", self.cvt.convert_to_roman(10)) 
 
def test_to_roman_tier3(self): 
    self.assertEquals("L", self.cvt.convert_to_roman(50)) 
 
def test_to_roman_tier4(self): 
    self.assertEquals("C", self.cvt.convert_to_roman(100)) 
 
def test_to_roman_tier5(self): 
    self.assertEquals("D", self.cvt.convert_to_roman(500)) 
 
def test_to_roman_tier6(self): 
    self.assertEquals("M", \ 
                self.cvt.convert_to_roman(1000)) </code></pre>
<ol start="7">
<li>Add some tests that input unexpected values to the Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">def test_to_roman_bad_inputs(self): 
    self.assertEquals("", self.cvt.convert_to_roman(None))     
    self.assertEquals("I", self.cvt.convert_to_roman(1.2)) 

def test_to_decimal_bad_inputs(self):   
    self.assertRaises(TypeError, \ 
                self.cvt.convert_to_decimal, None) 
    self.assertRaises(TypeError, \ 
                self.cvt.convert_to_decimal, 1.2) </code></pre>
<ol start="8">
<li>Add a unit test runner:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__": 
  unittest.main() </code></pre>
<ol start="9">
<li>Run the test case. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000040.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We have a specialized Roman numeral converter that only converts values up to <kbd>MMMM</kbd> or <kbd>4000</kbd>. We have written several test methods to exercise it. The immediate edges we write tests for are <kbd>1</kbd> and <kbd>4000</kbd>. We also write some tests for one step past that: <kbd>0</kbd> and <kbd>4001</kbd>. To make things complete, we also test against <kbd>-1</kbd>.</p>
<h3>There's more...</h3>
<p>A key part of the algorithm involves handling the various tiers of Roman numerals (5, 10, 50, 100, 500, and 1000). These could be considered <em>mini-edges</em>, so we wrote tests to check that the code handled those as well. Do you think we should test one past the mini-edges?</p>
<p>It's recommended we should. Many bugs erupt due to coding <em>greater than</em>, when it should be <em>greater than or equal</em> (or vice versa), and so on. Testing one past the boundary, in both directions, is the perfect way to make sure that things are working exactly as expected. We also need to check bad inputs, so we tried converting <kbd>None</kbd> and a <kbd>float</kbd>.</p>
<p>That previous statement raises an important question: <em>How many invalid types should</em> <em>we test against</em>? Because Python is dynamic, we can expect a lot of input types. So, what is reasonable? If our code hinges on a dictionary lookup, such as certain parts of our Roman numeral API does, then confirming that we correctly handle a <kbd>KeyError</kbd> would probably be adequate. We don't need to input lots of different types if they all result in a <kbd>KeyError</kbd>.</p>
<h4>Identifying the edges is important</h4>
<p>It's important to identify the edges of our system, because we need to know our software can handle these boundaries. We also need to know it can handle both sides of these boundaries that are good values and bad values. That is why we need to check <kbd>4000</kbd> and <kbd>4001</kbd> as well as <kbd>0</kbd> and <kbd>1</kbd>. This is a common place where software breaks.</p>
<h4>Testing for unexpected conditions</h4>
<p>Does this sound a little awkward? Expect the unexpected? Our code involves converting integers and strings back and forth. By unexpected, we mean types of inputs passed in when someone uses our library that doesn't understand the edges, or wires it to receive inputs that are wider ranging types than we expected to receive.</p>
<p>A common occurrence of misuse is when a user of our API is working against a collection, such as a list, and accidentally passes the entire list instead of a single value by iteration. Another, often seen situation is when a user of our API passes in <kbd>None</kbd> due to some other bug in their code. It's good to know that our API is resilient enough to handle this.</p>
<h2>Testing corner cases by iteration</h2>
<p>While developing code, new corner-case inputs are often discovered. Being able to capture these inputs in an iterable array makes it easy to add related test methods.</p>
<h3>How to do it...</h3>
<p>In this recipe, we will look at a different way to test corner cases:</p>
<ol>
<li>Create a new file called <kbd>recipe10.py</kbd> for our code in this recipe.</li>
<li>Pick a class to test. In this recipe, we'll use another variation of our Roman numeral converter. This one doesn't process values greater than <kbd>4000</kbd>:</li>
</ol>
<pre><code class="lang-python">class RomanNumeralConverter(object): 
    def __init__(self): 
        self.digit_map = {"M":1000, "D":500, "C":100, "L":50, "X":10, "V":5, "I":1} 
 
    def convert_to_decimal(self, roman_numeral): 
        val = 0 
        for char in roman_numeral: 
            val += self.digit_map[char] 
        if val &gt; 4000: 
            raise Exception(\ 
                "We don't handle values over 4000") 
        return val 

    def convert_to_roman(self, decimal): 
        if decimal &gt; 4000: 
            raise Exception(\ 
                "We don't handle values over 4000") 
        val = ""  
        mappers = [(1000,"M"), (500,"D"), (100,"C"), (50,"L"), 
(10,"X"), (5,"V"), (1,"I")] 
        for (mapper_dec, mapper_rom) in mappers: 
            while decimal &gt;= mapper_dec: 
                val += mapper_rom 
                decimal -= mapper_dec 
        return val </code></pre>
<ol start="3">
<li>Create a test class to exercise the Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">import unittest 
 
class RomanNumeralTest(unittest.TestCase): 
    def setUp(self): 
        self.cvt = RomanNumeralConverter()</code></pre>
<ol start="4">
<li>Write a test method that exercises the edges of the Roman numeral converter:</li>
</ol>
<pre><code class="lang-python">def test_edges(self): 
    r = self.cvt.convert_to_roman 
    d = self.cvt.convert_to_decimal 
    edges = [("equals", r, "I", 1),\ 
          ("equals", r, "", 0),\ 
          ("equals", r, "", -1),\ 
          ("equals", r, "MMMM", 4000),\ 
          ("raises", r, Exception, 4001),\ 
          ("equals", d, 1, "I"),\ 
          ("equals", d, 0, ""),\ 
          ("equals", d, 4000, "MMMM"),\
          ("raises", d, Exception, "MMMMI") 
         ] 
    [self.checkout_edge(edge) for edge in edges</code></pre>
<ol start="5">
<li>Create a test method that exercises the tiers converting from decimal to Roman numerals:</li>
</ol>
<pre><code class="lang-python">def test_tiers(self):
    r = self.cvt.convert_to_roman
    edges = [("equals", r, "V", 5),\
         ("equals", r, "VIIII", 9),\
         ("equals", r, "X", 10),\
         ("equals", r, "XI", 11),\
         ("equals", r, "XXXXVIIII", 49),\
         ("equals", r, "L", 50),\
         ("equals", r, "LI", 51),\
         ("equals", r, "LXXXXVIIII", 99),\
         ("equals", r, "C", 100),\
         ("equals", r, "CI", 101),\
         ("equals", r, "CCCCLXXXXVIIII", 499),\
         ("equals", r, "D", 500),\
         ("equals", r, "DI", 501),\
         ("equals", r, "M", 1000)\
        ]
    [self.checkout_edge(edge) for edge in edges]</code></pre>
<ol start="6">
<li>Create a test method that exercises a set of invalid inputs:</li>
</ol>
<pre><code class="lang-python">def test_bad_inputs(self): 
    r = self.cvt.convert_to_roman 
    d = self.cvt.convert_to_decimal 
    edges = [("equals", r, "", None),\ 
        ("equals", r, "I", 1.2),\ 
        ("raises", d, TypeError, None),\ 
        ("raises", d, TypeError, 1.2)\ 
       ] 
    [self.checkout_edge(edge) for edge in edges]</code></pre>
<ol start="7">
<li>Code a utility method that iterates over the edge cases and runs different assertions based on each edge:</li>
</ol>
<pre><code class="lang-python">def checkout_edge(self, edge): 
    if edge[0] == "equals": 
      f, output, input = edge[1], edge[2], edge[3]    
      print("Converting %s to %s..." % (input, output))    
      self.assertEquals(output, f(input)) 
    elif edge[0] == "raises": 
      f, exception, args = edge[1], edge[2], edge[3:]    
      print("Converting %s, expecting %s" % \ 
                      (args, exception)) 
      self.assertRaises(exception, f, *args)</code></pre>
<ol start="8">
<li>Make the script runnable by loading the test case into <kbd>TextTestRunner</kbd>:</li>
</ol>
<pre><code class="lang-python">  if __name__ == "__main__": 
    suite = unittest.TestLoader().loadTestsFromTestCase( \    
                RomanNumeralTest) 
    unittest.TextTestRunner(verbosity=2).run(suite)</code></pre>
<ol start="9">
<li>Run the test case, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000079.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We have a specialized Roman numeral converter that only converts values up to <kbd>MMMM</kbd> or <kbd>4000</kbd>. The immediate edges we write tests for are <kbd>1</kbd> and <kbd>4000</kbd>. We also write some tests for one step past that: <kbd>0</kbd> and <kbd>4001</kbd>. To make things complete, we also test against <kbd>-1</kbd>.</p>
<p>But we've written the tests a little differently. Instead of writing each test input/output combination as a separate test method, we capture the input and output values in a tuple that is embedded in a list. We then feed it to our test iterator, <kbd>checkout_edge</kbd>. Because we need both <kbd>assertEqual</kbd>&nbsp;and <kbd>assertRaise</kbd> calls, the tuple also includes either equals or raises to flag which assertion to use.</p>
<p>Finally, to make it flexibly handle the conversion of both Roman numerals and decimals, the handles on the <kbd>convert_to_roman</kbd> and <kbd>convert_to_decimal</kbd> functions of our Roman numeral API are embedded in each tuple as well.</p>
<p>As shown in the following highlighted parts, we grab a handle on <kbd>convert_to_roman</kbd>&nbsp;and store it in <kbd>r</kbd>. Then we embed it in the third element of the highlighted tuple, allowing the <kbd>checkout_edge</kbd> function to call it when needed:</p>
<pre><code class="lang-python">def test_bad_inputs(self): 
    r = self.cvt.convert_to_roman 
    d = self.cvt.convert_to_decimal 
    edges = [("equals", r, "", None),\ 
         ("equals", r, "I", 1.2),\ 
         ("raises", d, TypeError, None),\ 
         ("raises", d, TypeError, 1.2)\ 
        ] 
 
    [self.checkout_edge(edge) for edge in edges] </code></pre>
<h3>There's more...</h3>
<p>A key part of the algorithm involves handling the various tiers of Roman numerals (5, 10, 50, 100, 500, and 1000). These could be considered <em>mini-edges</em>, so we wrote a separate test method that has a list of input/output values to check those out as well. In the <em>Testing the edges</em> recipe, we didn't include testing before and after these mini-edges, for example,&nbsp;<kbd>4</kbd> and <kbd>6</kbd> for <kbd>5</kbd>. Now that it only takes one line of data to capture this test, we have it in this recipe. The same was done for all the others (except 1000).</p>
<p>Finally, we need to check bad inputs, so we created one more test method where we try to convert <kbd>None</kbd> and a <kbd>float</kbd> to and from a Roman numeral.</p>
<h4>Does this defy the recipe&nbsp;&ndash; breaking down obscure tests into simple ones?</h4>
<p>In a way, it does. If something goes wrong in one of the test data entries, then that entire test method will have failed. That is one reason why this recipe split things up into three test methods instead of one big test method to cover them all. This is a judgment call about when it makes sense to view inputs and outputs as more data than test method. If you find the same sequence of test steps occurring repeatedly, consider if it makes sense to capture the values in some sort of table structure, such as a list used in this recipe.</p>
<h4>How does this compare with the recipe&nbsp;&ndash; testing the edges?</h4>
<p>In case it wasn't obvious, these are the exact same tests used in the <em>Testing the edges</em> recipe. The question is, which version do you find more readable? Both are perfectly acceptable. Breaking things up into separate methods makes it more fine-grained and easier to spot if something goes wrong. Collecting things together into a data structure, the way we did in this recipe, makes it more succinct and could spur us on to write more test combinations as we did for the conversion tiers.</p>
<p>In my own opinion, when testing algorithmic functions that have simple inputs and outputs, it's more suitable to use this recipe's mechanism to code an entire battery of test inputs in this concise format. For example, a mathematical function, a sorting algorithm, or perhaps a transform function.</p>
<p>When testing functions that are more logical and imperative, the other recipe may be more useful. For example, functions that interact with a database, cause changes in the state of the system, or other types of side effects that aren't encapsulated in the return value would be hard to capture using this recipe.</p>
<h4>See also</h4>
<ul>
<li><em>Breaking down obscure tests into simple ones</em></li>
<li><em>Testing the edges</em></li>
</ul>

</div>


<!--Chapter 2-->

<div class="chapter" data-chapter-number="2">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 2 </span></div>
<h1 class="chaptertitle">Running Automated Test Suites with Nose</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>



<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Getting nosy with testing</li>
<li>Embedding nose inside Python</li>
<li>Writing a nose extension to pick tests based on regular expressions</li>
<li>Writing a nose extension to generate a CSV report</li>
<li>Writing a project-level script that lets you run different test suites</li>
</ul>
<h2>Introduction</h2>
<p>In the previous chapter, we looked at several ways to utilize unittest in creating automated tests. Now, we will look at different ways to gather tests together and run them. Nose is a useful utility that was built to discover tests and run them. It is flexible, can be run from either the command-line or embedded inside scripts, and is extensible through plugin. Due to its embeddable nature and high-level tools, such as project scripts, it can be built with testing as an option.</p>
<p>What does nose offer that unittest does not? Key things include automatic test discovery and a useful plugin API. There are many nose plugins that provide everything from specially formatted test reports to integration with other tools. We will explore this in more detail in this chapter and in later parts of this book.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>For more information about nose refer to:&nbsp;<a href="http://somethingaboutorange.com/mrl/projects/nose">http://somethingaboutorange.com/mrl/projects/nose</a>.</p>
</div>
</div>
<p>We need to activate our virtual environment and then install nose for the recipes in this chapter.</p>
<p>Create a virtual environment, activate it, and verify that the tools are working:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000094.png" class="lazyload" /></p>
<p>Next, use <kbd>pip&nbsp;install nose</kbd>, as shown in the following screenshot:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000054.png" class="lazyload" /></p>
<h2>Getting nosy with testing</h2>
<p>Nose automatically discovers tests when fed with a package, a module, or a file.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will explore how nose automatically finds test cases and runs them:</p>
<ol>
<li>Create a new file called <kbd>recipe11.py</kbd> to store all the code for this recipe.</li>
<li>Create a class to test. For this recipe, we will use a shopping cart application that lets us load items and then calculate the bill:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
      def __init__(self):
          self.items = []
      def add(self, item, price):
          self.items.append(Item(item, price))
          return self
     def item(self, index):
          return self.items[index-1].item
     def price(self, index):
          return self.items[index-1].price
     def total(self, sales_tax):
          sum_price = sum([item.price for item in self.items])
          return sum_price*(1.0 + sales_tax/100.0)
     def __len__(self):
          return len(self.items)
class Item(object):
     def __init__(self, item, price):
          self.item = item
          self.price = price</code></pre>
<ol start="3">
<li>Create a test case that exercises the various parts of the shopping cart application:</li>
</ol>
<pre><code class="lang-python">import unittest
class ShoppingCartTest(unittest.TestCase):
     def setUp(self):
        self.cart = ShoppingCart().add("tuna sandwich", 15.00)
     def test_length(self):
        self.assertEquals(1, len(self.cart))
     def test_item(self):
        self.assertEquals("tuna sandwich", self.cart.item(1))
     def test_price(self):
        self.assertEquals(15.00, self.cart.price(1))
     def test_total_with_sales_tax(self):
        self.assertAlmostEquals(16.39,
        self.cart.total(9.25), 2)</code></pre>
<ol start="4">
<li>Use the command-line <kbd>nosetests</kbd> tool to run this recipe by filename and also by module:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000000.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We started off by creating a simple application that lets us load up a <kbd>ShoppingCart</kbd> with <kbd>Items</kbd>. This application lets us look up each item and its price. Finally, we can calculate the total billing amount including the sales tax.</p>
<p>Next, we coded some test methods to exercise all these features using unittest.</p>
<p>Finally, we used the command-line <kbd>nosetests</kbd> tool, which discovers test cases and automatically runs them. This saved us from hand coding a test runner to load test suites.</p>
<h3>There's more...</h3>
<p>Why is it so important to not write the test runner? What do we gain by using <kbd>nosetests</kbd>? After all, unittest gives us the ability to embed an auto-discovering test runner like this:</p>
<pre><code class="lang-python">if __name__ == "__main__": 
    unittest.main()</code></pre>
<p>Would the same block of code work if the tests are spread across several modules? No, because <kbd>unittest.main()</kbd> only looks in the current module. To grow into multiple modules, we need to start loading tests using unittest's <kbd>loadTestsFromTestCase</kbd> method or other customized suites. It doesn't matter how we assemble suites. When we risk missing test cases, <kbd>nosetests</kbd> conveniently lets us search for all tests, or a subset of tests if needed.</p>
<p>A common situation on projects is to spread out test cases between lots of modules. Instead of writing one big test case, we typically break things up into smaller test cases based on various setups, scenarios, and other logical groupings. It's a common practice to split up test cases based on which module is being tested. The point is that manually loading all the test cases for a real-world test suite can become labor-intensive.</p>
<h4>Nose is extensible</h4>
<p>Auto-discovery of tests isn't the only reason to use nose. Later in this chapter, we will explore how we can write a plugin to customize what it discovers and also the output of a test run.</p>
<h4>Nose is embeddable</h4>
<p>All the functionality nose provides can be utilized either by the command-line, or from inside a Python script. We will also explore this further in this chapter.</p>
<h3>See also</h3>
<p>The <em>Asserting&nbsp;</em><em>the&nbsp;</em><em>basics</em> recipe in Chapter&nbsp;1,&nbsp;<em>Using Unittest to Develop Basic Tests</em>.</p>
<h2>Embedding nose inside Python</h2>
<p>It's very convenient to embed nose inside a Python script. This lets us create higher-level test tools besides allowing the developer to add testing to an existing tool.</p>
<h3>How to do it...</h3>
<p>With these steps, we will explore using nose's API inside a Python script to run some tests:</p>
<ol>
<li>Create a new file called <kbd>recipe12.py</kbd> to contain the code from this recipe.</li>
<li>Create a class to test. For this recipe, we will use a shopping cart application that lets us load items and then calculate the bill:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
   def __init__(self):
      self.items = []
   def add(self, item, price):
      self.items.append(Item(item, price))
      return self
   def item(self, index):
      return self.items[index-1].item
   def price(self, index):
      return self.items[index-1].price
   def total(self, sales_tax):
      sum_price = sum([item.price for item in self.items])
      return sum_price*(1.0 + sales_tax/100.0)
   def __len__(self):
      return len(self.items)
class Item(object):
   def __init__(self, item, price):
      self.item = item
      self.price = price</code></pre>
<ol start="3">
<li>Create a test case with several test methods:</li>
</ol>
<pre><code class="lang-python">import unittest
class ShoppingCartTest(unittest.TestCase):
   def setUp(self): 
      self.cart = ShoppingCart().add("tuna sandwich", 15.00)
   def test_length(self):
      self.assertEquals(1, len(self.cart))
   def test_item(self):
      self.assertEquals("tuna sandwich", self.cart.item(1))
   def test_price(self):
      self.assertEquals(15.00, self.cart.price(1))
   def test_total_with_sales_tax(self):
      self.assertAlmostEquals(16.39,
      self.cart.total(9.25), 2)</code></pre>
<ol start="4">
<li>Create a script named <kbd>recipe12_nose.py</kbd> to use nose's API to run tests.</li>
<li>Make the script runnable and use nose's <kbd>run()</kbd> method to run selected arguments:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import nose
    nose.run(argv=["", "recipe12", "--verbosity=2"])</code></pre>
<ol start="6">
<li>Run the test script from the command line and see the verbose output:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000044.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In the test running code, we are using <kbd>nose.run()</kbd>. With no arguments, it simply picks up on <kbd>sys.argv</kbd> and acts like the command-line <kbd>nosetests</kbd>. But in this recipe, we are plugging in the name of the current module along with increased verbosity.</p>
<h3>There's more...</h3>
<p>Unittest has <kbd>unittest.main()</kbd>, which discovers and runs test cases as well. How is this different? <kbd>unittest.main()</kbd> is geared to discover test cases in the same module where it is run. The&nbsp;<kbd>nose.run()</kbd>&nbsp;function is geared to let us pass in command-line arguments or load them programmatically.</p>
<p>For example, look at the following steps; we must complete them to turn up verbosity with unittest:</p>
<pre><code class="lang-python">if __name__ == "__main__": 
    import unittest 
    from recipe12 import * 
    suite = unittest.TestLoader().loadTestsFromTestCase( 
                                        ShoppingCartTest) 
    unittest.TextTestRunner(verbosity=2).run(suite) </code></pre>
<p>We had to import the test cases, use a test loader to create a test suite, and then run it through <kbd>TextTestRunner</kbd>.</p>
<p>To do the same thing with nose, this is all we need:</p>
<pre><code class="lang-python">if __name__ == "__main__": 
    import nose 
    nose.run(argv=["", "recipe12", "--verbosity=2"]) </code></pre>
<p>This is much more succinct. Any command-line options we could use with <kbd>nosetests</kbd>&nbsp;can&nbsp;be used here. This comes in handy when we use the the nose plugin, which we will explore in more detail in this chapter and throughout the rest of the book.</p>
<h2>Writing a nose extension to pick tests based on regular expressions</h2>
<p>Out-of-the-box test tools such as nose are very useful. But eventually, we reach a point where the options don't match our needs. Nose has the powerful ability to code custom plugins, and this gives us the ability to fine-tune nose to meet our needs. This recipe will help us write a plugin that allows us to selectively choose test methods by matching their method names using a regular expression when we run <kbd>nosetests</kbd>.</p>
<h3>Getting ready</h3>
<p>We need to have <kbd>easy_install</kbd> loaded in order to install the nose plugin that we are about to create. If you don't already have it, please visit <a href="http://pypi.python.org/pypi/setuptools">http://pypi.python.org/pypi/setuptools</a> to download and install the package as indicated at the site.</p>
<p>If you have just installed it now, then you will have to do the following:</p>
<ul>
<li>Rebuild your <kbd>virtualenv</kbd> used for running code samples in this book</li>
<li>Reinstall <kbd>nose</kbd> using <kbd>pip</kbd></li>
</ul>
<h3>How to do it...</h3>
<p>With the following steps, we will code a nose plugin that picks test methods to run by using a regular expression:</p>
<ol>
<li>Create a new file called <kbd>recipe13.py</kbd> to store the code for this recipe.</li>
<li>Create a shopping cart application that we can build some tests around:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
   def __init__(self):
     self.items = []
   def add(self, item, price):
     self.items.append(Item(item, price))
     return self
   def item(self, index):
     return self.items[index-1].item
   def price(self, index):
     return self.items[index-1].price
   def total(self, sales_tax):
     sum_price = sum([item.price for item in self.items])
     return sum_price*(1.0 + sales_tax/100.0)
   def __len__(self):
     return len(self.items)
class Item(object):
   def __init__(self, item, price):
     self.item = item
     self.price = price</code></pre>
<ol start="3">
<li>Create a test case that contains several test methods, including one that does not start with the word <kbd>test</kbd>:</li>
</ol>
<pre><code class="lang-python">import unittest
class ShoppingCartTest(unittest.TestCase):
   def setUp(self):
     self.cart = ShoppingCart().add("tuna sandwich", 15.00)
   def length(self):
     self.assertEquals(1, len(self.cart))
   def test_item(self):
     self.assertEquals("tuna sandwich", self.cart.item(1))
   def test_price(self):
     self.assertEquals(15.00, self.cart.price(1))
   def test_total_with_sales_tax(self):
     self.assertAlmostEquals(16.39,
     self.cart.total(9.25), 2)</code></pre>
<ol start="4">
<li>Run the module using <kbd>nosetests</kbd> from the command line, with <kbd>verbosity</kbd> turned on. How many test methods get run? How many test methods did we define?</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000034.png" class="lazyload" /></p>
<ol start="5">
<li>Create a new file called <kbd>recipe13_plugin.py</kbd> to write a nose plugin for this recipe.</li>
</ol>
<ol start="6">
<li>Capture a handle to <kbd>sys.stderr</kbd> to support debugging and verbose output:</li>
</ol>
<pre><code class="lang-python">import sys 
err = sys.stderr </code></pre>
<ol start="7">
<li>Create a nose plugin named <kbd>RegexPicker</kbd> by subclassing <kbd>nose.plugins.Plugin</kbd>:</li>
</ol>
<pre><code class="lang-python">import nose
import re
from nose.plugins import Plugin
class RegexPicker(Plugin):
   name = "regexpicker"
   def __init__(self):
      Plugin.__init__(self)
      self.verbose = False</code></pre>
<p>Our nose plugin requires a class-level name. This is used to define the&nbsp;<kbd>with-&lt;name&gt;</kbd> command-line option.</p>
<ol start="8">
<li>Override <kbd>Plugin.options</kbd> and add an option to provide the pattern on the command line:</li>
</ol>
<pre><code class="lang-python">def options(self, parser, env):
    Plugin.options(self, parser, env)
    parser.add_option("--re-pattern",
       dest="pattern", action="store",
       default=env.get("NOSE_REGEX_PATTERN", "test.*"),
       help=("Run test methods that have a method name matching this regular expression"))</code></pre>
<ol start="9">
<li>Override <kbd>Plugin.configuration</kbd> by having it fetch the pattern and verbosity:</li>
</ol>
<pre><code class="lang-python">def configure(self, options, conf):
     Plugin.configure(self, options, conf)
     self.pattern = options.pattern
     if options.verbosity &gt;= 2:
        self.verbose = True
        if self.enabled:
           err.write("Pattern for matching test methods is %sn" % self.pattern)</code></pre>
<p>When we extend <kbd>Plugin</kbd>, we inherit some other features, such as&nbsp;<kbd>self.enabled</kbd>, which is switched on when <kbd>-with-&lt;name&gt;</kbd> is used with nose.</p>
<ol start="10">
<li>Override <kbd>Plugin.wantedMethod</kbd>&nbsp;so that it accepts test methods that match our regular expression:</li>
</ol>
<pre><code class="lang-python">def wantMethod(self, method):
   wanted =
     re.match(self.pattern, method.func_name) is not None
   if self.verbose and wanted:
      err.write("nose will run %sn" % method.func_name)
   return wanted</code></pre>
<p>Write a test runner that programmatically tests our plugin by running the same test case that we ran earlier:</p>
<pre><code class="lang-python">if __name__ == "__main__":
     args = ["", "recipe13", "--with-regexpicker", "--re-pattern=test.*|length", "--verbosity=2"]
     print "With verbosity..."
     print "===================="
     nose.run(argv=args, plugins=[RegexPicker()])
     print "Without verbosity..."
     print "===================="
     args = args[:-1]
     nose.run(argv=args, plugins=[RegexPicker()])</code></pre>
<ol start="11">
<li>Execute the test runner. Looking at the results in the following screenshot, how many test methods ran this time?</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000023.png" class="lazyload" /></p>
<ol start="12">
<li>Create a <kbd>setup.py</kbd> script that allows us to install and register our plugin with <kbd>nosetests</kbd>:</li>
</ol>
<pre><code class="lang-python">import sys
try:
        import ez_setup
        ez_setup.use_setuptools()
except ImportError:
        pass
from setuptools import setup
setup(
        name="RegexPicker plugin",
        version="0.1",
        author="Greg L. Turnquist",
        author_email="Greg.L.Turnquist@gmail.com",
        description="Pick test methods based on a regular expression",
        license="Apache Server License 2.0",
        py_modules=["recipe13_plugin"],
        entry_points = {
            'nose.plugins': [
                'recipe13_plugin = recipe13_plugin:RegexPicker'
               ]
        }
)</code></pre>
<ol start="13">
<li>Install our new plugin:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000122.png" class="lazyload" /></p>
<ol start="14">
<li>Run <kbd>nosetests</kbd> using <kbd>--with-regexpicker</kbd> from the command line:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000001.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Writing a nose plugin has some requirements. First of all, we need the class-level <kbd>name</kbd> attribute. It is used in several places, including defining the command-line switch to invoke our plugin, <kbd>--with-&lt;name&gt;</kbd>.</p>
<p>Next, we write <kbd>options</kbd>. There is no requirement to override <kbd>Plugin.options</kbd>, but in this case we need a way to supply our plugin with the regular expression. To avoid destroying the useful machinery of <kbd>Plugin.options</kbd>, we call it first, and then add a line for our extra parameter using <kbd>parser.add_option</kbd>:</p>
<ul>
<li>The first, unnamed arguments are string versions of the parameter, and we can specify multiple ones. We could have had <kbd>-rp</kbd> and <kbd>-re-pattern</kbd> if we had wanted to.</li>
<li><kbd>Dest</kbd>: This is the name of the attribute that stores the results (see configure).</li>
<li><kbd>Action</kbd>: This specifies what to do with the value of the parameter (store, append, and so on).</li>
<li><kbd>Default</kbd>: This specifies what value to store when none are provided (notice we use <kbd>test.*</kbd> to match standard unittest behavior).</li>
<li><kbd>Help</kbd>: This provides help info to print out on the command line.</li>
</ul>
<p>Nose uses Python's <kbd>optparse.OptionParser</kbd> library to define options.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>To find out more about Python's <kbd>optparse.OptionParser</kbd>, please refer to <a href="http://docs.python.org/library/optparse.html">http://docs.python.org/library/optparse.html</a>.</p>
</div>
</div>
<p>Then, we write <kbd>configure</kbd>. There is also no requirement to override <kbd>Plugin.configure</kbd>. Because we had an extra option, <kbd>--pattern</kbd>, we need to harvest it. We also want to turn on a flag driven by <kbd>verbosity</kbd>, a standard nose option.</p>
<p>There are many things we can do when writing a nose plugin. In our case, we wanted to zero in on <strong>test&nbsp;</strong><strong>selection</strong>. There are several ways to load tests, including by module, and filename. After loading, they are then run through a method where they are voted in or out. These voters are called <kbd>want*</kbd> methods and they include <kbd>wantModule</kbd>, <kbd>wantName</kbd>, <kbd>wantFunction</kbd>, and <kbd>wantMethod</kbd>, as well some others. We implemented <kbd>wantMethod</kbd> where we test <kbd>method.func_name</kbd> matches our pattern using Python's <kbd>re</kbd> module. <kbd>want*</kbd> methods have three return value types:</p>
<ul>
<li><kbd>True</kbd>: This test is wanted.</li>
<li><kbd>False</kbd>: This test is not wanted (and will not be considered by another plugin).</li>
<li><kbd>None</kbd>: The plugin does not care whether another plugin (or nose) gets to choose.</li>
</ul>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This is succinctly achieved by not returning anything from the <kbd>want*</kbd> method.</p>
</div>
</div>
<p><kbd>wantMethod</kbd> only looks at functions defined inside classes. <kbd>nosetests</kbd> is geared to find tests by many different methods and is not confined to just searching subclasses of <kbd>unittest.TestCase</kbd>. If tests are found in the module, but not as class methods, then this pattern matching is not utilized. For this plugin to be more robust, we would need lot of different tests and probably need to override the other <kbd>want*</kbd> test selectors.</p>
<h3>There's more...</h3>
<p>This recipe just scratches the surface on plugin functionality. It focuses on the test selection process.</p>
<p>Later in this chapter, we will explore generating a specialized report. This involves using other plugin hooks that gather information after each test is run as well as generating a report after the test suite is exhausted. Nose provides a robust set of hooks allowing detailed customization to meet our changing needs.</p>
<p>Plugins should subclass <kbd>nose.plugins.Plugin</kbd>.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>There is a lot of valuable machinery built into <kbd>Plugin</kbd>. Subclassing is the recommended means of developing a plugin. If you don't so this, you may have to add on methods and attributes you didn't realize were needed by nose (and that come for free when you subclass).</p>
</div>
</div>
<p>It's a good rule of thumb to subclass the parts of the nose API that we are plugging into instead of overriding.</p>
<p>The online documentation for the nose API is a little incomplete. It tends to assume too much knowledge. If we override and our plugin doesn't work correctly, it may be difficult to debug what is happening.</p>
<p>Do not subclass <kbd>nose.plugins.IPluginInterface</kbd>.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>This class is used for documentation purposes only. It provides information about each of the hooks our plugin can access. But it is not designed for subclassing real plugins.</p>
</div>
</div>
<h2>Writing a nose extension to generate a CSV report</h2>
<p>This recipe will help us write a plugin that generates a custom report listing successes and failures in a CSV file. It is used to demonstrate how to gather information after each test method completes.</p>
<h3>Getting ready</h3>
<p>We need to have <kbd>easy_install</kbd> loaded in order to install the nose plugin we are about to create. If you don't already have it, please visit <a href="http://pypi.python.org/pypi/setuptools">http://pypi.python.org/pypi/setuptools</a> to download and install the package as indicated on the site.</p>
<p>If you have just installed it now, then you will have to do the following:</p>
<ul>
<li>Rebuild your <kbd>virtualenv</kbd> used for running code samples in this book</li>
<li>Reinstall nose using <kbd>easy_install</kbd></li>
</ul>
<h3>How to do it...</h3>
<ol>
<li>Create a new file named <kbd>recipe14.py</kbd> to store the code for this recipe.</li>
<li>Create a shopping cart application that we can build some tests around:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
   def __init__(self):
     self.items = [] 
   def add(self, item, price):
     self.items.append(Item(item, price))
     return self
   def item(self, index):
     return self.items[index-1].item
   def price(self, index):
     return self.items[index-1].price
   def total(self, sales_tax):
     sum_price = sum([item.price for item in self.items])
     return sum_price*(1.0 + sales_tax/100.0)
   def __len__(self):
     return len(self.items)
class Item(object):
   def __init__(self, item, price):
     self.item = item
     self.price = price</code></pre>
<ol start="3">
<li>Create a test case that contains several test methods, including one deliberately set to fail:</li>
</ol>
<pre><code class="lang-python">import unittest
class ShoppingCartTest(unittest.TestCase):
    def setUp(self):
      self.cart = ShoppingCart().add("tuna sandwich", 15.00)
    def test_length(self):
      self.assertEquals(1, len(self.cart))
    def test_item(self):
      self.assertEquals("tuna sandwich", self.cart.item(1))
    def test_price(self):
      self.assertEquals(15.00, self.cart.price(1))
    def test_total_with_sales_tax(self):
      self.assertAlmostEquals(16.39,
      self.cart.total(9.25), 2)
    def test_assert_failure(self):
      self.fail("You should see this failure message in the report.")</code></pre>
<ol start="4">
<li>Run the module using <kbd>nosetests</kbd> from the command line. Looking at the output in the following screenshot, does it appear that a CSV report exists?</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000151.png" class="lazyload" /></p>
<ol start="5">
<li>Create a new file called <kbd>recipe14_plugin.py</kbd> to store our new nose plugin.</li>
<li>Create a nose plugin named <kbd>CsvReport</kbd> by subclassing <kbd>nose.plugins.Plugin</kbd>:</li>
</ol>
<pre><code class="lang-python">import nose
import re
from nose.plugins import Plugin
class CsvReport(Plugin):
    name = "csv-report"
    def __init__(self):
      Plugin.__init__(self)
      self.results = []</code></pre>
<p>Our nose plugin requires a class-level <kbd>name</kbd>. This is used to define the <kbd>-with-&lt;name&gt;</kbd> command-line option.</p>
<ol start="7">
<li>Override <kbd>Plugin.options</kbd> and add an option to provide the report's filename on the command line:</li>
</ol>
<pre><code class="lang-python">def options(self, parser, env):
  Plugin.options(self, parser, env)
  parser.add_option("--csv-file",
    dest="filename", action="store",
    default=env.get("NOSE_CSV_FILE", "log.csv"),
    help=("Name of the report"))</code></pre>
<ol start="8">
<li>Override <kbd>Plugin.configuration</kbd> by having it fetch the filename from options:</li>
</ol>
<pre><code class="lang-python">def configure(self, options, conf):
  Plugin.configure(self, options, conf)
  self.filename = options.filename</code></pre>
<p>When we extend <kbd>Plugin</kbd>, we inherit some other features, such as&nbsp;<kbd>self.enabled</kbd>, which is switched on when <kbd>-with-&lt;name&gt;</kbd> is used with nose.</p>
<ol start="9">
<li>Override <kbd>addSuccess</kbd>, <kbd>addFailure</kbd>, and <kbd>addError</kbd> to collect the results in an internal list:</li>
</ol>
<pre><code class="lang-python">def addSuccess(self, *args, **kwargs):
  test = args[0]
  self.results.append((test, "Success"))
def addError(self, *args, **kwargs):
  test, error = args[0], args[1]
  self.results.append((test, "Error", error))
def addFailure(self, *args, **kwargs):
  test, error = args[0], args[1]
  self.results.append((test, "Failure", error))</code></pre>
<ol start="10">
<li>Override <kbd>finalize</kbd> to generate the CSV report:</li>
</ol>
<pre><code class="lang-python">def finalize(self, result):
   report = open(self.filename, "w")
   report.write("Test,Success/Failure,Detailsn")
   for item in self.results:
       if item[1] == "Success":
           report.write("%s,%sn" % (item[0], item[1]))
       else:
           report.write("%s,%s,%sn" % (item[0],item[1], item[2][1]))
    report.close()</code></pre>
<ol start="11">
<li>Write a test runner that programmatically tests our plugin by running the same test case that we ran earlier:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
   args = ["", "recipe14", "--with-csv-report", "--csv-file=recipe14.csv"]
nose.run(argv=args, plugin=[CsvReport()])</code></pre>
<ol start="12">
<li>Execute the test runner. Looking at the output in the next screenshot, is there a test report now?</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000139.png" class="lazyload" /></p>
<ol start="13">
<li>Open up and view the report using your favorite spreadsheet:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000085.png" class="lazyload" /></p>
<ol start="14">
<li>Create a <kbd>setup.py</kbd> script that allows us to install and register our plugin with&nbsp;<kbd>nosetests</kbd>:</li>
</ol>
<pre><code class="lang-python">import sys
try:
   import ez_setup
   ez_setup.use_setuptools()
except ImportError:
   pass
from setuptools import setup
setup(
   name="CSV report plugin",
   version="0.1",
   author="Greg L. Turnquist",
   author_email="Greg.L.Turnquist@gmail.com",
   description="Generate CSV report",
   license="Apache Server License 2.0",
   py_modules=["recipe14_plugin"],
   entry_points = {
       'nose.plugins': [
           'recipe14_plugin = recipe14_plugin:CsvReport'
         ]
   }
)</code></pre>
<ol start="15">
<li>Install our new plugin:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000051.png" class="lazyload" /></p>
<ol start="16">
<li>Run <kbd>nosetests</kbd> using <kbd>--with-csv-report</kbd> from the command line:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000147.png" class="lazyload" /></p>
<p>In the previous screenshot, notice how we have the previous log file, <kbd>recipe14.csv</kbd>, and the new one, <kbd>log.csv</kbd>.</p>
<h3>How it works...</h3>
<p>Writing a nose plugin has some requirements. First of all, we need the class-level <kbd>name</kbd> attribute. It is used in several places including defining the command-line switch to invoke our plugin, <kbd>--with-&lt;name&gt;</kbd>.</p>
<p>Next, we write <kbd>options</kbd>. There is no requirement to override <kbd>Plugin.options</kbd>. But in this case, we need a way to supply our plugin with the name of the CSV report it will write. To avoid destroying the useful machinery of <kbd>Plugin.options</kbd>, we call it first, and then add a line for our extra parameter using <kbd>parser.add_option</kbd>:</p>
<ul>
<li>The first, unnamed arguments are string versions of the parameter</li>
<li><kbd>dest</kbd>: This is the name of the attribute that stores the results (see configure)</li>
<li><kbd>action</kbd>: This tells what to do with the value of the parameter (store, append, and so on)</li>
<li><kbd>default</kbd>: This tells what value to store when none is provided</li>
<li><kbd>help</kbd>: This provides help info to print out on the command line</li>
</ul>
<p>Nose uses Python's <kbd>optparse.OptionParser</kbd> library to define options.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>To learn more about <kbd>optparse.OptionParser</kbd>, visit <a href="http://docs.python.org/optparse.html">http://docs.python.org/optparse.html</a>.</p>
</div>
</div>
<p>Then, we write <kbd>configure</kbd>. There is also no requirement to override <kbd>Plugin.configure</kbd>. Because we had an extra option, <kbd>--csv-file</kbd>, we need to harvest it.</p>
<p>In this recipe, we want to capture the test case and the error report whenever a test method completes. To do this, we implement <kbd>addSuccess</kbd>, <kbd>addFailure</kbd>, and <kbd>addError</kbd>, because nose varies in what arguments are sent to these methods when called either programmatically or by the command-line, so we must use Python's <kbd>*args</kbd>:</p>
<ul>
<li>The first slot of this tuple contains the <kbd>test</kbd>, an instance of <kbd>nose.case.Test</kbd>. Simply printing it is sufficient for our needs.</li>
<li>The second slot of this tuple contains the <kbd>error</kbd>, an instance of the 3-tuple for <kbd>sys.exc_info()</kbd>. It is only included for <kbd>addFailure</kbd> and <kbd>addError</kbd>.</li>
<li>No more slots of this tuple are documented on nose's website. We generally ignore them.</li>
</ul>
<h3>There's more...</h3>
<p>This recipe digs a little deeper into the plugin's functionality. It focuses on processing done after a test method succeeds, fails, or causes an error. In our case, we just gather the results to put into a report. We could do other things, such as capture stack traces, send email failures to the development team, or send a page to the QA team letting them know a test suite is complete.</p>
<p>For more details about writing a nose plugin, read the <em>Writing&nbsp;</em><em>a&nbsp;</em><em>nose&nbsp;</em><em>extension</em>&nbsp;recipe&nbsp;to pick tests based on regular expressions.</p>
<h2>Writing a project-level script that lets you run different test suites</h2>
<p>Python, with its multi-paradigm nature, makes it easy to build applications and provide scripting support.</p>
<p>This recipe will help us explore building a project-level script that allows us to run different test suites. We will also show some extra command-line options to create hooks for packaging, publishing, registering, and writing automated documentation.</p>
<h3>How to do it...</h3>
<ol>
<li>Create a script called <kbd>recipe15.py</kbd> that parses a set of options using Python's <kbd>getopt</kbd> library:</li>
</ol>
<pre><code class="lang-python">import getopt
import glob
import logging
import nose
import os
import os.path
import pydoc
import re
import sys
def usage():
    print
    print "Usage: python recipe15.py [command]"
    print
    print "t--help"
    print "t--test"
    print "t--suite [suite]"
    print "t--debug-level [info|debug]"
    print "t--package"
    print "t--publish"
    print "t--register"
    print "t--pydoc"
    print
try:
    optlist, args = getopt.getopt(sys.argv[1:],
                    "ht", 
                    ["help", "test", "suite=",
                    "debug-level=", "package",
                    "publish", "register", "pydoc"])
except getopt.GetoptError:
    # print help information and exit:
    print "Invalid command found in %s" % sys.argvusage()
    sys.exit(2)</code></pre>
<ol start="2">
<li>Create a function that maps to <kbd>-test</kbd>:</li>
</ol>
<pre><code class="lang-python">def test(test_suite, debug_level):
    logger = logging.getLogger("recipe15")
    loggingLevel = debug_level
    logger.setLevel(loggingLevel)
    ch = logging.StreamHandler()
    ch.setLevel(loggingLevel)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s -
%(message)s")
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    nose.run(argv=["", test_suite, "--verbosity=2"])</code></pre>
<ol start="3">
<li>Create stub functions that support <kbd>package</kbd>, <kbd>publish</kbd>, and <kbd>register</kbd>:</li>
</ol>
<pre><code class="lang-python">def package():
    print "This is where we can plug in code to run " +
    "setup.py to generate a bundle."
def publish():
    print "This is where we can plug in code to upload " +
          "our tarball to S3 or some other download site."
def register():
    print "setup.py has a built in function to " +
          "'register' a release to PyPI. It's " +
          "convenient to put a hook in here."
    # os.system("%s setup.py register" % sys.executable)</code></pre>
<ol start="4">
<li>Create a function to auto-generate docs using Python's <kbd>pydoc</kbd> module:</li>
</ol>
<pre><code class="lang-python">def create_pydocs():
    print "It's useful to use pydoc to generate docs."
    pydoc_dir = "pydoc"
    module = "recipe15_all"
    __import__(module)
    if not os.path.exists(pydoc_dir):
        os.mkdir(pydoc_dir)
    cur = os.getcwd()
    os.chdir(pydoc_dir)
    pydoc.writedoc("recipe15_all")
    os.chdir(cur)</code></pre>
<ol start="5">
<li>Add some code that defines debug levels and then parses options to allow the user to override:</li>
</ol>
<pre><code class="lang-python">debug_levels = {"info":logging.INFO, "debug":logging.DEBUG}
# Default debug level is INFO
debug_level = debug_levels["info"]
for option in optlist:
    if option[0] in ("--debug-level"):
        # Override with a user-supplied debug level
        debug_level = debug_levels[option[1]]</code></pre>
<ol start="6">
<li>Add some code that scans the command-line options for <kbd>-help</kbd>&nbsp;and, if it's found, exits the script:</li>
</ol>
<pre><code class="lang-python"># Check for help requests, which cause all other
# options to be ignored.
for option in optlist:
if option[0] in ("--help", "-h"):
   usage()
   sys.exit(1)</code></pre>
<ol start="7">
<li>Finish it by iterating through each of the command-line options and invoking the other functions based on which options are picked:</li>
</ol>
<pre><code class="lang-python"># Parse the arguments, in order
for option in optlist:
    if option[0] in ("--test"):
       print "Running recipe15_checkin tests..."
       test("recipe15_checkin", debug_level)
    if option[0] in ("--suite"):
       print "Running test suite %s..." % option[1]
       test(option[1], debug_level)
    if option[0] in ("--package"):
       package()
    if option[0] in ("--publish"):
       publish()
    if option[0] in ("--register"):
       register()
    if option[0] in ("--pydoc"):
       create_pydocs()</code></pre>
<ol start="8">
<li>Run the <kbd>recipe15.py</kbd> script with <kbd>-help</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000101.png" class="lazyload" /></p>
<ol start="9">
<li>Create a new file called <kbd>recipe15_checkin.py</kbd> to create a new test suite.</li>
<li>Reuse the test cases from the <em>Getting&nbsp;</em><em>nosy&nbsp;</em><em>with&nbsp;</em><em>testing</em>&nbsp;recipe&nbsp;to define a <kbd>check</kbd><kbd>in</kbd> test suite:</li>
</ol>
<pre><code class="lang-python">import recipe11 
 
class Recipe11Test(recipe11.ShoppingCartTest): 
    pass </code></pre>
<ol start="11">
<li>Run the <kbd>recipe15.py</kbd> script, using <kbd>-test -package -publish -register -pydoc</kbd>. In the following screenshot, do you notice how it exercises each option in the same sequence as it was supplied on the command line?</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000055.png" class="lazyload" /></p>
<ol start="12">
<li>Inspect the report generated in the <kbd>pydoc</kbd> directory:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000082.png" class="lazyload" /></p>
<ol start="13">
<li>Create a new file named <kbd>recipe15_all.py</kbd> to define another new test suite.</li>
<li>Reuse the test code from the earlier recipes of this chapter to define an <kbd>all</kbd> test suite:</li>
</ol>
<pre><code class="lang-python">import recipe11
import recipe12
import recipe13
import recipe14
class Recipe11Test(recipe11.ShoppingCartTest):
    pass
class Recipe12Test(recipe12.ShoppingCartTest):
    pass
class Recipe13Test(recipe13.ShoppingCartTest):
    pass
class Recipe14Test(recipe14.ShoppingCartTest):
    pass</code></pre>
<ol start="15">
<li>Run the <kbd>recipe15.py</kbd> script with <kbd>-suite=recipe15_all</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000110.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This script uses Python's <kbd>getopt</kbd> library, which is modeled after the C programming language's <kbd>getopt()</kbd> function. This means we use the API to define a set of commands, and then we iterate over the options, calling the corresponding functions:</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Visit <a href="http://docs.python.org/library/getopt.html">http://docs.python.org/library/getopt.html</a> for more details on the&nbsp;<kbd>getopt</kbd> library.</p>
</div>
</div>
<ul>
<li><kbd>usage</kbd>: This is a function that provides help to the user.</li>
<li><kbd>key</kbd>: The option definitions are included in the following block:</li>
</ul>
<pre><code class="lang-python">optlist, args = getopt.getopt(sys.argv[1:],
                "ht",
                ["help", "test", "suite=",
                "debug-level=", "package",
                "publish", "register", "pydoc"])</code></pre>
<p>We parse everything in the arguments except the first, as this is the executable itself:</p>
<ul>
<li><kbd>"ht"</kbd> defines short options: <kbd>-h</kbd> and <kbd>-t</kbd>.</li>
<li>The list defines long options. Those with <kbd>"="</kbd> accept an argument. Those without it are flags.</li>
<li>If an option is received that isn't on the list, an exception is thrown; we print out <kbd>usage()</kbd>&nbsp;and then exit.</li>
<li><kbd>Test</kbd>: This activates loggers, which can be very useful if our app uses Python's <kbd>logging</kbd> library.</li>
<li><kbd>Package</kbd>: This generates tarballs. We created a stub, but it can be handy to provide a shortcut by running <kbd>setup.py sdist|bdist</kbd>.</li>
<li><kbd>Publish</kbd>: Its function is to push tarballs to the deployment site. We created a stub, but deploying it to an S3 site or somewhere else is useful.</li>
<li><kbd>Register</kbd>: This is the register with PyPI. We created a stub, but it would be handy to provide a shortcut to running <kbd>setup.py register</kbd>.</li>
<li><kbd>create_pydocs</kbd>: These are auto-generated docs. Generating HTML files based on code is very convenient.</li>
</ul>
<p>With each of these functions defined, we can iterate over the options that were parsed. For this script, there is a sequence as follows:</p>
<ol>
<li>Check whether there is a debugging override. We default to <kbd>logging.INFO</kbd>, but provide the ability to switch to <kbd>logging.DEBUG</kbd>.</li>
<li>Check whether&nbsp;<kbd>-h</kbd> or <kbd>-help</kbd> was called. If so, print out the <kbd>usage()</kbd> information and then exit with no more parsing.</li>
<li>Finally, iterate over the options and call their corresponding functions.</li>
</ol>
<p>To exercise things, we first called this script with the <kbd>-help</kbd> option. That printed out the command choices we had.</p>
<p>Then we called it with all the options to demonstrate the features. The script is coded to exercise a <kbd>check in</kbd> suite when we use <kbd>-test</kbd>. This short test suite simulates running a quicker test that's designed to see whether things look alright.</p>
<p>Finally, we called the script with <kbd>-suite=recipe15_all</kbd>. This test suite simulates running a more complete test suite that typically takes longer.</p>
<h3>There's more...</h3>
<p>The features this script provides could easily be handled by commands that are already built. We looked at <kbd>nosetests</kbd> earlier in this chapter and saw how it can flexibly take arguments to pick tests.</p>
<p>Using <kbd>setup.py</kbd> to generate tarballs and register releases is also a commonly used feature in the Python community.</p>
<p>So, why write this script? Because we can tap all these features with a single command script, <kbd>setup.py</kbd> contains a prebuilt set of commands that involve bundling and uploading to the Python Project Index. Doing other tasks, such as generating <strong>pydocs</strong>, deploying to a location such as an Amazon S3 bucket, or any other system-level task, is not included. This script demonstrates how easy it is to wire in other command-line options and link them with the project management functions.</p>
<p>We can also conveniently embed the usage of <kbd>pydoc</kbd>. Basically, any Python library that serves project management needs can be embedded as well.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>On an existing project, I developed a script to provide a unified way to embed version info into a templated <kbd>setup.py</kbd> as well as documentation generated by <kbd>pydoc</kbd>, <kbd>sphinx</kbd>, and <kbd>DocBook</kbd>. The script saved me from having to remember all the commands needed to manage the project.<br />Why didn't I extend <kbd>distutils</kbd> to create my own commands? It was a matter of taste. I preferred using <kbd>getopt</kbd> and working outside the framework of <kbd>distutils</kbd> instead of creating and registering new sub-commands.</p>
</div>
</div>
<h4>Why use getopt instead of optparse?</h4>
<p>Python has several options for handling command-line option parsing. <kbd>getopt</kbd> is possibly the simplest. It is meant to quickly allow defining short and long options, but it has limits. It requires custom coding help output, as we did with the usage function.</p>
<p>It also requires custom handling of the arguments. <kbd>optparse</kbd> provides more sophisticated options, such as better handling of arguments and auto-built help. But it also requires more code to get functional. <kbd>optparse</kbd> is also scheduled to be replaced by <kbd>argparse</kbd> in the future.</p>
<p>It is left as an exercise for you to write an alternative version of this script using <kbd>optparse</kbd>, to assess which one is a better solution.</p>

</div>


<!--Chapter 3-->


<div class="chapter" data-chapter-number="3">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 3 </span></div>
<h1 class="chaptertitle">Creating Testable Documentation with doctest</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>

<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Documenting the basics</li>
<li>Catching stack traces</li>
<li>Running a&nbsp;doctest&nbsp;from the command line</li>
<li>Coding a test harness for doctest</li>
<li>Filtering out test noise</li>
<li>Printing out all your documentation including a status report</li>
<li>Testing the edges</li>
<li>Testing corner cases by iteration</li>
<li>Getting nosy with doctest</li>
<li>Updating the project-level script to run this chapter's doctests</li>
</ul>
<h2>Introduction</h2>
<p>Python provides a useful ability to embed comments inside functions that are accessible from a Python shell. These are known as <strong>docstrings</strong>.</p>
<p>A docstring provides the ability to embed not only information, but also code samples that are runnable.</p>
<p>There is an old adage that says&nbsp;<em>comments&nbsp;</em><em>aren't&nbsp;</em><em>code</em>.&nbsp;This is because comments don't undergo syntax checks and are often not maintained. Thus, the information they carry can lose its value over time. <kbd>doctest</kbd> counters this by turning comments into code, which can serve many useful purposes.</p>
<p>In this chapter, we will explore different ways to use <kbd>doctest</kbd> to develop testing, documentation, and project support. No special setup is required, as <kbd>doctest</kbd> is part of Python's standard libraries.</p>
<h2>Documenting the basics</h2>
<p>Python provides an out-of-the-box capability to put comments in code, known as docstrings. Docstrings can be read when looking at the source and also when inspecting the code interactively from a Python shell. In this recipe, we will demonstrate how these interactive docstrings can be used as runnable tests.</p>
<p>What does this provide? It offers easy-to-read code samples for the users. Not only are the code samples readable, they are also runnable, meaning we can ensure the documentation stays up-to-date.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will create an application combined with runnable docstring comments, and see how to execute these tests:</p>
<ol>
<li>Create a new file named <kbd>recipe16.py</kbd> to put all the code we write for this recipe.</li>
</ol>
<ol start="2">
<li>Create a function that converts base-10 numbers to any other base using recursion:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    import math
    def _convert(remaining_value, base, exp):
        def stringify(value):
            if value &gt; 9:
                return chr(value + ord('a')-10)
            else:
                return str(value)
        if remaining_value &gt;= 0 and exp &gt;= 0:
            factor = int(math.pow(base, exp))
            if factor &lt;= remaining_value:
                multiple = remaining_value / factor
                return stringify(multiple) + \
                  _convert(remaining_value-multiple*factor, \
                    base, exp-1)
        else:
            return "0" + \
                _convert(remaining_value, base, exp-1)
        else:
            return ""
    return "%s/%s" % (_convert(value, base, \
                int(math.log(value, base))), base)</code></pre>
<ol start="3">
<li>Add a docstring just below the external function, as shown in the highlighted section of the following code. This docstring declaration includes several examples of using the function:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen
    &gt;&gt;&gt; convert_to_basen(1, 2)
    '1/2'
    &gt;&gt;&gt; convert_to_basen(2, 2)
    '10/2'
    &gt;&gt;&gt; convert_to_basen(3, 2)
    '11/2'
    &gt;&gt;&gt; convert_to_basen(4, 2)
    '100/2'
    &gt;&gt;&gt; convert_to_basen(5, 2)
    '101/2'
    &gt;&gt;&gt; convert_to_basen(6, 2)
    '110/2'
    &gt;&gt;&gt; convert_to_basen(7, 2)
    '111/2'
    &gt;&gt;&gt; convert_to_basen(1, 16)
    '1/16'
    &gt;&gt;&gt; convert_to_basen(10, 16)
    'a/16'
    &gt;&gt;&gt; convert_to_basen(15, 16)
    'f/16'
    &gt;&gt;&gt; convert_to_basen(16, 16)
    '10/16'
    &gt;&gt;&gt; convert_to_basen(31, 16)
    '1f/16'
    &gt;&gt;&gt; convert_to_basen(32, 16)
    '20/16'
    """
    import math</code></pre>
<ol start="4">
<li>Add a test runner block that invokes Python's <kbd>doctest</kbd> module:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import doctest
    doctest.testmod()</code></pre>
<ol start="5">
<li>From an interactive Python shell, import the recipe and view its documentation. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000063.png" class="lazyload" /></p>
<ol start="6">
<li>Run the code from the command line. In the following screenshot, notice how nothing is printed. This is what happens when all the tests pass. Look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000014.png" class="lazyload" /></p>
<ol start="7">
<li>Run the code from the command line with <kbd>-v</kbd> to increase verbosity. In the following screenshot, we see a piece of the output, showing what was run and what was expected. This can be useful when debugging <kbd>doctest</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000047.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The <kbd>doctest</kbd> module looks for blocks of Python inside docstrings and runs it like real code. <kbd>&gt;&gt;&gt;</kbd> is the same prompt we see when we use the interactive Python shell. The line&nbsp;following <kbd>&gt;&gt;&gt;</kbd>&nbsp;shows the expected output. <kbd>doctest</kbd> runs the statements it sees and then compares the actual output with the expected output.</p>
<p>Later in this chapter, we will see how to catch things such as stack traces, errors, and also add extra code that equates to a test fixture.</p>
<h3>There's more...</h3>
<p><kbd>doctest</kbd> is very picky when matching expected output with actual results:</p>
<ul>
<li>An extraneous space or tab can cause things to break.</li>
<li>Structures such as dictionaries are tricky to test, because Python doesn't guarantee the order of items. On each test run, the items could be stored in a different order. Simply printing out a dictionary is bound to break.</li>
<li>It is strongly advised not to include object references in expected outputs. These values also vary every time the test is run.</li>
</ul>
<h2>Catching stack traces</h2>
<p>It's a common fallacy that we should write tests only for successful code paths. We also need to code against error conditions including the ones that generate stack traces. With this recipe, we will explore how stack traces are pattern-matched in doc testing, which allows us to confirm expected errors.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will see how to use <kbd>doctest</kbd> to verify error conditions:</p>
<ol>
<li>Create a new file called <kbd>recipe17.py</kbd> for all our code in this recipe.</li>
</ol>
<ol start="2">
<li>Create a function that converts base-10 numbers to any other base using recursion:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    import math
    def _convert(remaining_value, base, exp):
        def stringify(value):
            if value &gt; 9:
                return chr(value + ord('a')-10)
            else:
                return str(value)
        if remaining_value &gt;= 0 and exp &gt;= 0:
            factor = int(math.pow(base, exp))
            if factor &lt;= remaining_value:
                multiple = remaining_value / factor
                return stringify(multiple) + \
                    _convert(remaining_value-multiple*factor, \
                                base, exp-1)
            else:
                return "0" + \
                    _convert(remaining_value, base, exp-1)
        else:
            return ""
    return "%s/%s" % (_convert(value, base, \
                int(math.log(value, base))), base)</code></pre>
<ol start="3">
<li>Add a docstring just below the external function declaration that includes two examples that are expected to generate stack traces:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen.

    &gt;&gt;&gt; convert_to_basen(0, 2)
    Traceback (most recent call last):
        ...
    ValueError: math domain error

    &gt;&gt;&gt; convert_to_basen(-1, 2)
    Traceback (most recent call last):
        ...
    ValueError: math domain error
    """
    import math</code></pre>
<ol start="4">
<li>Add a test runner block that invokes Python's <kbd>doctest</kbd> module:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import doctest
    doctest.testmod()</code></pre>
<ol start="5">
<li>Run the code from the command line. In the following screenshot, notice how nothing is printed. This is what happens when all the tests pass:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000073.png" class="lazyload" /></p>
<ol start="6">
<li>Run the code from the command line with <kbd>-v</kbd> to increase verbosity. In the following screenshot, we can see that <kbd>0</kbd> and <kbd>-1</kbd> generate math domain errors. This is due to using <kbd>math.log</kbd> to find the starting exponent:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000026.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The <kbd>doctest</kbd> module looks for blocks of Python inside docstrings and runs it like real code. <kbd>&gt;&gt;&gt;</kbd> is the same prompt we see when we use the interactive Python shell.&nbsp;The line&nbsp;following&nbsp;<kbd>&gt;&gt;&gt;</kbd>&nbsp;shows the expected output. <kbd>doctest</kbd> runs the statements it sees and then compares the actual output with the expected output.</p>
<p>With regard to stack traces, there is a lot of detailed information provided in the stack trace. Pattern matching the entire trace is ineffective. By using the ellipsis, we are able to skip the intermediate parts of the stack trace and just match on the distinguishing part: <kbd>ValueError: math domain error</kbd>.</p>
<p>This is valuable, because our users will not only see the way it handles good values, but will also observe what errors to expect when bad values are provided.</p>
<h2>Running a&nbsp;doctest&nbsp;from the command line</h2>
<p>We have seen how to develop tests by embedding runnable fragments of code in docstrings. But for each of these tests, we had to make the module runnable. What if we wanted to run something else other than our <kbd>doctest</kbd> from the command line? We would have to get rid of the <kbd>doctest.testmod()</kbd> statements!</p>
<p>The good news is that, starting with Python 2.6, there is a command-line option to run a specific module using <kbd>doctest</kbd> without coding a runner.</p>
<p>The&nbsp;<kbd>python -m doctest -v example.py</kbd>&nbsp;command will import <kbd>example.py</kbd> and run it through <kbd>doctest.testmod()</kbd>. According to the documentation, this may fail if the module is part of a package and imports other sub modules.</p>
<h3>How to do it...</h3>
<p>In the following steps, we will create a simple application. We will add some doctests and then run them from the command line without writing a special test runner:</p>
<ol>
<li>Create a new file called <kbd>recipe18.py</kbd> to store the code written for this recipe.</li>
</ol>
<ol start="2">
<li>Create a function that converts base-10 numbers to any other base using recursion:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    import math
    def _convert(remaining_value, base, exp):
        def stringify(value):
            if value &gt; 9:
                return chr(value + ord('a')-10)
            else:
                return str(value)
        if remaining_value &gt;= 0 and exp &gt;= 0:
            factor = int(math.pow(base, exp))
            if factor &lt;= remaining_value:
                multiple = remaining_value / factor
                return stringify(multiple) + \
                  _convert(remaining_value-multiple*factor, \
                                base, exp-1)
            else:
                return "0" + \
                       _convert(remaining_value, base, exp-1)
        else:
            return ""
    return "%s/%s" % (_convert(value, base, \
                         int(math.log(value, base))), base)</code></pre>
<ol start="3">
<li>Add a docstring just below the external function declaration that includes some of the tests:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen.
    
    &gt;&gt;&gt; convert_to_basen(10, 2)
    '1010/2'
    
    &gt;&gt;&gt; convert_to_basen(15, 16)
    'f/16'
    
    &gt;&gt;&gt; convert_to_basen(0, 2)
    Traceback (most recent call last):
        ...
    ValueError: math domain error
    
    &gt;&gt;&gt; convert_to_basen(-1, 2)
    Traceback (most recent call last):
        ...
    ValueError: math domain error
    """
    import math</code></pre>
<ol start="4">
<li>Run the code from the command line using <kbd>-m doctest</kbd>. As shown in the following screenshot, no output indicates that all the tests have passed:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000133.png" class="lazyload" /></p>
<ol start="5">
<li>Run the code from the command line with <kbd>-v</kbd> to increase verbosity. What happens if we forget to include <kbd>-m doctest</kbd>? Using the <kbd>-v</kbd> option helps us to avoid this by giving us a warm fuzzy&nbsp;feeling that our tests are working. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000006.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In the previous chapter, we were using the <kbd>__main__</kbd> block of a module to run other test suites. What if we wanted to do the same here? We would have to pick whether <kbd>__main__</kbd> would be for unittest tests, doctests, or both! What if we didn't even want to run testing through <kbd>__main__</kbd>, but instead run our application?</p>
<p>That is why Python added the option of invoking testing right from the command line using <kbd>-m doctest</kbd>.</p>
<p>Don't you want to <em>know</em> whether your tests are running or working? Is the test suite really doing what it promised? With other tools, we usually have to embed print statements, or deliberate failures just to know things are being trapped properly. Doesn't it appear that the&nbsp;<kbd>-v</kbd> option in <kbd>doctest</kbd> provides a convenient quick glance at what's happening?</p>
<h2>Coding a test harness for doctest</h2>
<p>The tests we have written so far are very simple, because the function we are testing is simple. There are two inputs and one output with no side effects. No objects have to be created. This isn't the most common use case for us. Often, we have objects that interact with other objects.</p>
<p>The <kbd>doctest</kbd> module supports creating objects, invoking methods, and checking results. With this recipe, we will explore this in more detail.</p>
<p>An important aspect of <kbd>doctest</kbd> is that it finds individual instances of docstrings, and runs them in a local context. Variables declared in one docstring cannot be used in another docstring.</p>
<h3>How to do it...</h3>
<ol>
<li>Create a new file called <kbd>recipe19.py</kbd> to contain the code from this recipe.</li>
</ol>
<p>&nbsp;</p>
<ol start="2">
<li>Write a simple shopping cart application:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
    def __init__(self):
        self.items = []
    def add(self, item, price):
        self.items.append(Item(item, price))
        return self
    def item(self, index):
        return self.items[index-1].item
    def price(self, index):
        return self.items[index-1].price
    def total(self, sales_tax):
        sum_price = sum([item.price for item in self.items])
        return sum_price*(1.0 + sales_tax/100.0)
    def __len__(self):
        return len(self.items)
class Item(object):
    def __init__(self, item, price):
        self.item = item
        self.price = price</code></pre>
<ol start="3">
<li>Insert a docstring at the top of the module, before the <kbd>ShoppingCart</kbd> class declaration:</li>
</ol>
<pre><code class="lang-python">"""
This is documentation for the this entire recipe.
With it, we can demonstrate usage of the code.

&gt;&gt;&gt; cart = ShoppingCart().add("tuna sandwich", 15.0)
&gt;&gt;&gt; len(cart)
1
&gt;&gt;&gt; cart.item(1)
'tuna sandwich'
&gt;&gt;&gt; cart.price(1)
15.0
&gt;&gt;&gt; print (round(cart.total(9.25), 2))
16.39
"""
class ShoppingCart(object):
...</code></pre>
<ol start="4">
<li>Run the recipe using <kbd>-m doctest</kbd> and <kbd>-v</kbd> for verbosity:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000153.png" class="lazyload" /></p>
<ol start="5">
<li>Copy all the code we just wrote from <kbd>recipe19.py</kbd> into a new file called <kbd>recipe19b.py</kbd>.</li>
<li>Inside <kbd>recipe19b.py</kbd>, add another docstring to <kbd>item</kbd>, which attempts to reuse the <kbd>cart</kbd> variable defined at the top of the module:</li>
</ol>
<pre><code class="lang-python">def item(self, index):
    """
    &gt;&gt;&gt; cart.item(1)
    'tuna sandwich'
    """
    return self.items[index-1].item</code></pre>
<ol start="7">
<li>Run this variant of the recipe. Why does it fail? Wasn't <kbd>cart</kbd> declared in the earlier docstring? Look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000142.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The <kbd>doctest</kbd> module looks for every docstring. For each docstring it finds, it creates a shallow copy of the module's global variables and then runs the code and checks results. Apart from that, every variable created is locally scoped and then cleaned up when the test is complete. This means that our second docstring that was added later cannot see the <kbd>cart</kbd> that was created in our first docstring. That is why the second run failed.</p>
<p>There is no equivalent to a <kbd>setUp</kbd> method as we used with some of the unittest recipes. If there is no <kbd>setUp</kbd> option with <kbd>doctest</kbd>, then what value is this recipe? It highlights a key limitation of <kbd>doctest</kbd> that all developers must understand before using it.</p>
<h3>There's more...</h3>
<p>The <kbd>doctest</kbd> module provides an incredibly convenient way to add testability to our documentation. But this is not a substitute for a full-fledged testing framework, such as unittest. As noted earlier, there is no equivalent to a <kbd>setUp</kbd>. There is also no syntax checking of the Python code embedded in the docstrings.</p>
<p>Mixing the right level of a <kbd>doctest</kbd> with unittest (or any other testing framework we may pick) is a matter of judgment.</p>
<h2>Filtering out test noise</h2>
<p>Various options help <kbd>doctest</kbd> ignore noise, such as whitespace, in test cases. This can be useful, because it allows us to structure the expected outcome in a better way, to ease reading for the users.</p>
<p>We can also flag some tests that can be skipped. This can be used where we want to document known issues, but haven't yet patched the system.</p>
<p>Both of these situations can easily be construed as noise when we are trying to run comprehensive testing but are focused on other parts of the system. In this recipe, we will dig in to ease the strict checking done by <kbd>doctest</kbd>. We will also look at how to ignore entire tests, whether it's for temporary or permanent needs.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will experiment with filtering out test results and easing certain restrictions of <kbd>doctest</kbd>:</p>
<ol>
<li>Create a new file called <kbd>recipe20.py</kbd> for the code from this recipe.</li>
</ol>
<ol start="2">
<li>Create a recursive function that converts base-10 numbers into other bases:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    import math
    def _convert(remaining_value, base, exp):
        def stringify(value):
            if value &gt; 9:
                return chr(value + ord('a')-10)
            else:
                return str(value)

        if remaining_value &gt;= 0 and exp &gt;= 0:
            factor = int(math.pow(base, exp))
            if factor &lt;= remaining_value:
                multiple = remaining_value / factor
                return stringify(multiple) + \
                  _convert(remaining_value-multiple*factor, \
                                base, exp-1)
            else:
                return "0" + \
                       _convert(remaining_value, base, exp-1)
        else:
            return ""
    return "%s/%s" % (_convert(value, base, \
                         int(math.log(value, base))), base)</code></pre>
<ol start="3">
<li>Add a docstring that includes a test to exercise a range of values as well as document a future feature that is not yet implemented:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen.

    &gt;&gt;&gt; [convert_to_basen(i, 16) for i in range(1,16)] #doctest:
+NORMALIZE_WHITESPACE
    ['1/16', '2/16', '3/16', '4/16', '5/16', '6/16', '7/16', '8/16',
    '9/16',  'a/16', 'b/16', 'c/16', 'd/16', 'e/16', 'f/16']

    FUTURE: Binary may support 2's complement in the future, but not
now.
    &gt;&gt;&gt; convert_to_basen(-10, 2) #doctest: +SKIP
    '0110/2'
    """
    import math</code></pre>
<ol start="4">
<li>Add a test runner:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import doctest
    doctest.testmod()</code></pre>
<ol start="5">
<li>Run the test case in verbose mode, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000096.png" class="lazyload" /></p>
<ol start="6">
<li>Copy the code from <kbd>recipe20.py</kbd> into a new file called <kbd>recipe20b.py</kbd>.</li>
</ol>
<ol start="7">
<li>Edit <kbd>recipe20b.py</kbd> by updating the docstring to include another test exposing that our function doesn't convert <kbd>0</kbd>:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen.
    &gt;&gt;&gt; [convert_to_basen(i, 16) for i in range(1,16)] #doctest:
+NORMALIZE_WHITESPACE
    ['1/16', '2/16', '3/16', '4/16', '5/16', '6/16', '7/16', '8/16',
    '9/16',  'a/16', 'b/16', 'c/16', 'd/16', 'e/16', 'f/16']
    FUTURE: Binary may support 2's complement in the future, but not
now.
    &gt;&gt;&gt; convert_to_basen(-10, 2) #doctest: +SKIP
    '0110/2'
    BUG: Discovered that this algorithm doesn't handle 0. Need to patch
it.
    TODO: Renable this when patched.
    &gt;&gt;&gt; convert_to_basen(0, 2)
    '0/2'
    """
    import math</code></pre>
<ol start="8">
<li>Run the test case. Notice what is different about this version of the recipe and why it fails? Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000123.png" class="lazyload" /></p>
<ol start="9">
<li>Copy the code from <kbd>recipe20b.py</kbd> into a new file called <kbd>recipe20c.py</kbd>.</li>
</ol>
<ol start="10">
<li>Edit <kbd>recipe20c.py</kbd> and update the docstring, indicating that we will skip the test for now:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base): 
    """Convert a base10 number to basen. 
      
    &gt;&gt;&gt; [convert_to_basen(i, 16) for i in range(1,16)] #doctest: +NORMALIZE_WHITESPACE 
    ['1/16', '2/16', '3/16', '4/16', '5/16', '6/16', '7/16', '8/16', 
    '9/16',  'a/16', 'b/16', 'c/16', 'd/16', 'e/16', 'f/16'] 
 
    FUTURE: Binary may support 2's complement in the future, but not now. 
    &gt;&gt;&gt; convert_to_basen(-10, 2) #doctest: +SKIP 
    '0110/2' 
 
    BUG: Discovered that this algorithm doesn't handle 0. Need to patch it. 
    TODO: Renable this when patched. 
    &gt;&gt;&gt; convert_to_basen(0, 2) #doctest: +SKIP 
    '0/2' 
    """ 
    import math</code></pre>
<ol start="11">
<li>Run the test case. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000002.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In this recipe, we revisit the function for converting from base-10 to any base numbers. The first test shows it being run over a range. Normally, Python would fit this array of results on one line. To make it more readable, we spread the output across two lines. We also put some arbitrary spaces between the values to make the columns line up better.</p>
<p>This is something that <kbd>doctest</kbd> definitely would <em>not</em> support, due to its strict pattern matching nature. By using <kbd>#doctest: +NORMALIZE_WHITESPACE</kbd>, we are able to ask <kbd>doctest</kbd> to ease this restriction. There are still constraints. For example, the first value in the expected array cannot have any whitespace in front of it (<em>believe me, I tried for maximum readability!</em>) But wrapping the array to the next line no longer breaks the test.</p>
<p>We also have a test case that is really meant as documentation only. It indicates a future requirement that shows how our function would handle negative binary values. By adding <kbd>#doctest: +SKIP</kbd>, we are able to command <kbd>doctest</kbd> to skip this particular instance.</p>
<p>Finally, we see a scenario where we discover that our code doesn't handle <kbd>0</kbd>. As the algorithm gets the highest exponent by taking a logarithm, there is a math problem. We capture this edge case with a test. We then confirm that the code fails in classic <strong>test-</strong><strong>driven&nbsp;</strong><strong>design</strong> (<strong>TDD</strong>) fashion. The final step would be to fix the code to handle this edge case. But we decide, in a somewhat contrived fashion, that we don't have enough time in the current sprint to fix the code. To avoid breaking our <strong>continuous&nbsp;</strong><strong>integration</strong> (<strong>CI</strong>) server, we mark the test with a <kbd>TO-DO</kbd> statement and add <kbd>#doctest: +SKIP</kbd>.</p>
<h3>There's more...</h3>
<p>Both situations that we have marked up with <kbd>#doctest: +SKIP</kbd>&nbsp;are cases where eventually we will want to remove the <kbd>SKIP</kbd> tag and have them run. There may be other situations where we will never remove <kbd>SKIP</kbd>. Demonstrations of code that have big fluctuations may not be readily testable without making them unreadable. For example, functions that return dictionaries are harder to test because the order of results vary. We can bend it to pass a test, but we may lose the value of the documentation in order to present it to the reader.</p>
<h2>Printing out all your documentation including a status report</h2>
<p>Since this chapter has been about both documentation and testing, let's build a script that takes a set of modules and prints out a complete report, showing all documentation as well as running any given tests.</p>
<p>This is a valuable recipe, because it shows us how to use Python's APIs to harvest a code-driven runnable report. This means the documentation is accurate and up to date, reflecting the current state of our code.</p>
<h3>How to do it...</h3>
<p>In the following steps, we will write an application and some <kbd>doctests</kbd>. Then we will build a script to harvest a useful report:</p>
<ol>
<li>Create a new file called <kbd>recipe21_report.py</kbd> to contain the script that harvests our report.</li>
<li>Start creating a script by importing Python's <kbd>inspect</kbd> library as the basis for drilling down into a module: <kbd>from inspect import*</kbd>.</li>
<li>Add a function that focuses on either printing out an item's <kbd>__doc__</kbd> string or prints out that no&nbsp;documentation was found:</li>
</ol>
<pre><code class="lang-python">def print_doc(name, item):
    if item.__doc__:
        print "Documentation for %s" % name
        print "-------------------------------"
        print item.doc
        print "-------------------------------"
    else:
        print "Documentation for %s - None" % name</code></pre>
<ol start="4">
<li>Add a function that prints out the documentation based on a given module. Make sure this function looks for classes, methods, and functions, and prints out their docs:</li>
</ol>
<pre><code class="lang-python">def print_docstrings(m, prefix=""):
    print_doc(prefix + "module %s" % m.__name__, m)
    
    for (name, value) in getmembers(m, isclass):
        if name == '__class__': continue
        print_docstrings(value, prefix=name + ".")
    for (name, value) in getmembers(m, ismethod):
        print_doc("%s%s()" % (prefix, name), value)
    for (name, value) in getmembers(m, isfunction):
        print_doc("%s%s()" % (prefix, name), value)</code></pre>
<ol start="5">
<li>Add a runner that parses the command-line string, and iterates over each provided module:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import sys
    import doctest
    
    for arg in sys.argv[1:]:
        if arg.startswith("-"): continue
        print "==============================="
        print "== Processing module %s" % arg
        print "==============================="
        m = __import__(arg)
        print_docstrings(m)
        print "Running doctests for %s" % arg
        print "-------------------------------"
        doctest.testmod(m)</code></pre>
<ol start="6">
<li>Create a new file,&nbsp;<kbd>recipe21.py</kbd>, to contain an application with tests that we will run the earlier script against.</li>
</ol>
<ol start="7">
<li>In <kbd>recipe21.py</kbd>, create a shopping cart app and fill it with docstrings and <kbd>doctests</kbd>. This is documentation for the entire recipe. With it, we can demonstrate usage of the code:</li>
</ol>
<pre><code class="lang-python">&gt;&gt;&gt; cart = ShoppingCart().add("tuna sandwich", 15.0)
&gt;&gt;&gt; len(cart)
1
&gt;&gt;&gt; cart.item(1)
'tuna sandwich'
&gt;&gt;&gt; cart.price(1)
15.0
&gt;&gt;&gt; print round(cart.total(9.25), 2)
16.39
"""

class ShoppingCart(object):
    """
    This object is used to store the goods.
    It conveniently calculates total cost including
    tax.
    """
    def __init__(self):
        self.items = []
    def add(self, item, price):
        "Add an item to the internal list."
        self.items.append(Item(item, price))
        return self
    def item(self, index):
        "Look up the item. The cart is a 1-based index."
        return self.items[index-1].item
    def price(self, index):
        "Look up the price. The cart is a 1-based index."
        return self.items[index-1].price
    def total(self, sales_tax):
        "Add up all costs, and then apply a sales tax."
        sum_price = sum([item.price for item in self.items])
        return sum_price*(1.0 + sales_tax/100.0)
    def __len__(self):
        "Support len(cart) operation."
        return len(self.items)

class Item(object):
    def __init__(self, item, price):
        self.item = item
        self.price = price</code></pre>
<ol start="8">
<li>Run the report script against this module using <kbd>-v</kbd>, and look at the screen's output:</li>
</ol>
<pre><code class="lang-python">===============================
== Processing module recipe21
===============================
Documentation for module recipe21
-------------------------------
This is documentation for the this entire recipe.
With it, we can demonstrate usage of the code.
&gt;&gt;&gt; cart = ShoppingCart().add("tuna sandwich", 15.0)
&gt;&gt;&gt; len(cart)
1
&gt;&gt;&gt; cart.item(1)
'tuna sandwich'
&gt;&gt;&gt; cart.price(1)
15.0
&gt;&gt;&gt; print round(cart.total(9.25), 2)
16.39
-------------------------------
Documentation for Item.module Item - None
Documentation for Item.__init__() - None
Documentation for ShoppingCart.module ShoppingCart
-------------------------------
    This object is used to store the goods.
    It conveniently calculates total cost including
    tax.
&hellip;
Running doctests for recipe21
-------------------------------
Trying:
    cart = ShoppingCart().add("tuna sandwich", 15.0)
Expecting nothing
ok
Trying:
    len(cart)
Expecting:
    1
ok
5 tests in 10 items.
5 passed and 0 failed.
Test passed.</code></pre>
<h3>How it works...</h3>
<p>This script is tiny, yet it harvests a lot of useful information.</p>
<p>By using Python's standard <kbd>inspect</kbd> module, we are able to drill down starting at the module level. The reflective way to look up a docstring is by accessing the <kbd>__doc__</kbd> property of an object. This is contained in modules, classes, methods, and functions. They exist in other places, but we limited our focus for this recipe.</p>
<p>We ran it in verbose mode to show that the tests were actually executed. We hand parsed the command-line options, but <kbd>doctest</kbd> automatically looks for <kbd>-v</kbd> to decide whether or not to turn on verbose output. To prevent our module processor from catching this and trying to process it as another module, we added a line to skip any <kbd>-xyz</kbd> style flags:</p>
<pre><code class="lang-python">        if arg.startswith("-"): continue </code></pre>
<h3>There's more...</h3>
<p>We could spend more time enhancing this script. For example, we could dump this out with an HTML markup, making it viewable in a web browser. We could also find third-party libraries to export it in other ways.</p>
<p>We could also work on refining where it looks for docstrings and how it handles them. In our case, we just printed them to the screen. A more reusable approach would be to return some type of structure containing all the information. Then, the caller can decide whether to print to screen, encode it in HTML, or generate a PDF document.</p>
<p>This isn't necessary, because this recipe's focus is on seeing how to mix these powerful out-of-the-box options Python provides into a quick and useful tool.</p>
<h2>Testing the edges</h2>
<p>Tests need to exercise the boundaries of our code up to and beyond the range limits. In this recipe, we will dig into defining and testing edges with <kbd>doctest</kbd>.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will see how to write code that tests the edges of our software:</p>
<ol>
<li>Create a new file named <kbd>recipe22.py</kbd> and use it to place all of our code for this recipe.</li>
</ol>
<ol start="2">
<li>Create a function that converts base-10 numbers to anything between base-2 and base-36:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    if base &lt; 2 or base &gt; 36:
        raise Exception("Only support bases 2-36")
    
    import math
    def _convert(remaining_value, base, exp):
        def stringify(value):
            if value &gt; 9:
                return chr(value + ord('a')-10)
            else:
                return str(value)
        
        if remaining_value &gt;= 0 and exp &gt;= 0:
            factor = int(math.pow(base, exp))
            if factor &lt;= remaining_value:
                multiple = remaining_value / factor
                return stringify(multiple) + \
                  _convert(remaining_value-multiple*factor, \
                                base, exp-1)
            else:
                return "0" + \
                       _convert(remaining_value, base, exp-1)
        else:
            return ""

    return "%s/%s" % (_convert(value, base, \
                         int(math.log(value, base))), base)</code></pre>
<ol start="3">
<li>Add a docstring just below our function declaration that includes tests showing base-2 edges, base-36 edges, and the invalid base-37:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen.

    These show the edges for base 2.
    &gt;&gt;&gt; convert_to_basen(1, 2)
    '1/2'
    &gt;&gt;&gt; convert_to_basen(2, 2)
    '10/2'
    &gt;&gt;&gt; convert_to_basen(0, 2)
    Traceback (most recent call last):
       ...
    ValueError: math domain error

    These show the edges for base 36.
    &gt;&gt;&gt; convert_to_basen(1, 36)
    '1/36'
    &gt;&gt;&gt; convert_to_basen(35, 36)
    'z/36'
    &gt;&gt;&gt; convert_to_basen(36, 36)
    '10/36'
    &gt;&gt;&gt; convert_to_basen(0, 36)
    Traceback (most recent call last):
       ...
    ValueError: math domain error

    These show the edges for base 37.
    &gt;&gt;&gt; convert_to_basen(1, 37)
    Traceback (most recent call last):
       ...
    Exception: Only support bases 2-36
    &gt;&gt;&gt; convert_to_basen(36, 37)
    Traceback (most recent call last):
       ...
    Exception: Only support bases 2-36
    &gt;&gt;&gt; convert_to_basen(37, 37)
    Traceback (most recent call last):
       ...
    Exception: Only support bases 2-36
    &gt;&gt;&gt; convert_to_basen(0, 37)   
    Traceback (most recent call last):
       ...
    Exception: Only support bases 2-36
    """
    if base &lt; 2 or base &gt; 36:</code></pre>
<ol start="4">
<li>Add a test runner:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import doctest
    doctest.testmod()</code></pre>
<ol start="5">
<li>Run the recipe, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000103.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This version has a limit of handling base-2 through base-36.</p>
<p>For base-36, it uses <kbd>a</kbd> to <kbd><em>z</em></kbd>. This is compared to base-16 that uses <kbd>a</kbd> to <kbd>f</kbd>. <kbd>35</kbd> in base-10 is represented as <kbd>z</kbd> in base-36.</p>
<p>We include several tests, including <kbd>1</kbd> for base-2 and base-36. We also test the maximum value before rolling over, and the next value, to show the rollover. For base-2, this is <kbd>1</kbd> and <kbd>2</kbd>. For base-36, this is <kbd>35</kbd> and <kbd>36</kbd>.</p>
<p>As we have also included tests for 0 to show that our function doesn't handle this for any base, we also test base-37, which is invalid as well.</p>
<h3>There's more...</h3>
<p>It's important that our software works for valid inputs. It's just as important that our software works as expected for invalid inputs. We have documentation that can be viewed by our users when using our software that documents these edges. And, thanks to Python's <kbd>doctest</kbd> module, we can test it and make sure that our software performs correctly.</p>
<h3>See also</h3>
<p>The <em>Testing&nbsp;</em><em>the&nbsp;</em><em>edges</em> section mentioned in Chapter 1, <em>Using Unittest to Develop Basic Tests</em>.</p>
<h2>Testing corner cases by iteration</h2>
<p>Corner cases will appear as we continue to develop our code. By capturing corner cases in an iterable list, there is less code to write for capturing another test scenario. This can increase our efficiency at testing new scenarios.</p>
<h3>How to do it...</h3>
<ol>
<li>Create a new file called <kbd>recipe23.py</kbd>, and use it to store all our code for this recipe.</li>
<li>Create a function that converts base-10 to any other base:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    import math

    def _convert(remaining_value, base, exp):
        def stringify(value):
            if value &gt; 9:
                return chr(value + ord('a')-10)
            else:
                return str(value)

        if remaining_value &gt;= 0 and exp &gt;= 0:
            factor = int(math.pow(base, exp))
            if factor &lt;= remaining_value:
                multiple = remaining_value / factor
                return stringify(multiple) + \
                  _convert(remaining_value-multiple*factor, \
                                base, exp-1)
            else:
                return "0" + \
                       _convert(remaining_value, base, exp-1)
        else:
            return ""

    return "%s/%s" % (_convert(value, base, \
                         int(math.log(value, base))), base)</code></pre>
<ol start="3">
<li>Add some instances of&nbsp;<kbd>doctest</kbd> that include an array of input values to generate an array of expected outputs. Include one failure:</li>
</ol>
<pre><code class="lang-python">def convert_to_basen(value, base):
    """Convert a base10 number to basen.

    Base 2
    &gt;&gt;&gt; inputs = [(1,2,'1/2'), (2,2,'11/2')]
    &gt;&gt;&gt; for value,base,expected in inputs:
    ...     actual = convert_to_basen(value,base)
    ...     assert actual == expected, 'expected: %s actual: %s' %
(expected, actual)

    &gt;&gt;&gt; convert_to_basen(0, 2)
    Traceback (most recent call last):
       ...
    ValueError: math domain error

    Base 36.
    &gt;&gt;&gt; inputs = [(1,36,'1/36'), (35,36,'z/36'), (36,36,'10/36')]
    &gt;&gt;&gt; for value,base,expected in inputs:
    ...     actual = convert_to_basen(value,base)
    ...     assert actual == expected, 'expected: %s actual: %s' %
(expected, value)

    &gt;&gt;&gt; convert_to_basen(0, 36)
    Traceback (most recent call last):
       ...
    ValueError: math domain error
    """
    import math</code></pre>
<ol start="4">
<li>Add a test runner:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
    import doctest
    doctest.testmod()</code></pre>
<ol start="5">
<li>Run the recipe:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000091.png" class="lazyload" /></p>
<p>In the previous screenshot, the key information is on this line: <kbd>AssertionError: expected: 11/2 actual: 10/2</kbd>. Is this test failure a bit contrived? Sure it is. But seeing a test case that shows useful output is not. It's important to verify that our tests give us enough information to fix either the tests or the code.</p>
<h3>How it works...</h3>
<p>We created an array with each entry containing both the input data as well as the expected output. This provides us an easy way to glance at a set of test cases.</p>
<p>Then, we iterated over each test case, calculated the actual value, and ran it through a Python <kbd>assert</kbd>. An important part that is needed is the custom message <kbd>'expected: %s actual: %s'</kbd>. Without it, we would never get the information that tells us which test case failed.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p><strong>What if one test case fails?</strong><br />If one of the tests in the array fails, then that code block exits and skips over the rest of the tests. This is the trade-off for having a more succinct set of tests.</p>
</div>
</div>
<h4>Does this type of test fit better into doctest or unittest?</h4>
<p>Here are some criteria to help you decide whether it's worth putting these tests in <kbd>doctest</kbd>:</p>
<ul>
<li>Is the code easy to comprehend at a glance?</li>
<li>Is there clear, succinct, useful information when users view the docstrings?</li>
</ul>
<p>If there is little value of having this in the documentation, and it clutters the code, then that is a strong hint that this test block belongs to a separate test module.</p>
<h3>See also</h3>
<p>The <em>Testing&nbsp;</em><em>corner&nbsp;</em><em>cases&nbsp;</em><em>by&nbsp;</em><em>iteration</em> section of Chapter&nbsp;1, <em>Using Unittest to Develop Basic Tests</em></p>
<h2>Getting nosy with doctest</h2>
<p>Up to this point, we have been either appending modules with a test runner, or we have typed <kbd>python -m doctest &lt;module&gt;</kbd> on the command line to exercise our tests.</p>
<p>In the previous chapter, we introduced the powerful&nbsp;<kbd>nose</kbd>&nbsp;library (refer to&nbsp;<a href="http://somethingaboutorange.com/mrl/projects/nose">http://somethingaboutorange.com/mrl/projects/nose</a> for more details).</p>
<p>For a quick recap, nose has the following features:</p>
<ul>
<li>Provides us with the convenient test discovering tool <kbd>nosetests</kbd></li>
<li>Is pluggable, with a huge ecosystem of plugins available</li>
<li>Includes a built-in plugin targeted at finding doctests and running them</li>
</ul>
<h3>Getting ready</h3>
<p>We need to activate our virtual environment (<kbd>virtualenv</kbd>) and then install nose for this recipe:</p>
<ol>
<li>Create a virtual environment, activate it, and verify the tools are working. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000015.png" class="lazyload" /></p>
<ol start="2">
<li>Using <kbd>pip</kbd>, install <kbd>nose</kbd>, as shown in the screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000124.png" class="lazyload" /></p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This recipe assumes you have built all of the previous recipes in this chapter. If you have built only some of them, your results may appear different.</p>
</div>
</div>
<h3>How to do it...</h3>
<ol>
<li>Run <kbd>nosetests -with-doctest</kbd> against all the modules in this folder. You may notice that it prints a very short <kbd>.....F.F...F</kbd>, indicating that three tests have failed.</li>
<li>Run <kbd>nosetests -with-doctest -v</kbd> to get a more verbose output. In the following screenshot, notice how the tests that failed are the same ones that failed for the previous recipes in this chapter. It is also valuable to see the <kbd>&lt;module&gt;.&lt;method&gt;</kbd> format with either <kbd>ok</kbd> or <kbd>FAIL</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000066.png" class="lazyload" /></p>
<ol start="3">
<li>Run <kbd>nosetests -with-doctest</kbd> against both the <kbd>recipe19.py</kbd> file as well as the <kbd>recipe19</kbd> module, in different combinations, as shown in the screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000059.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p><kbd>nosetests</kbd> is targeted at discovering test cases and then running them. With this plugin, when it finds a docstring, it uses the <kbd>doctest</kbd> library to programmatically test it.</p>
<p>The <kbd>doctest</kbd> plugin is built around the assumption that doctests are not in the same package as other tests, such as unittest. This means it will only run doctests found from non-test packages.</p>
<p>There isn't a whole lot of complexity to <kbd>nosetests</kbd> nor in using them, <kbd>nosetests</kbd> is meant to be an easy-to-use tool that puts testing at your fingertips. In this recipe, we have seen how to use <kbd>nosetests</kbd> to get a hold of all the doctest we have built so far in this chapter.</p>
<p>&nbsp;</p>
<h2>Updating the project-level script to run this chapter's doctests</h2>
<p>This recipe will help us explore building a project-level script that allows us to run different test suites. We will also focus on how to run it in our <kbd>doctest</kbd>.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will craft a command-line script to allow us to manage a project that includes running <kbd>doctest</kbd>:</p>
<ol>
<li>Create a new file called <kbd>recipe25.py</kbd> to put all the code for this recipe.</li>
</ol>
<ol start="2">
<li>Add code that parses a set of options using Python's <kbd>getopt</kbd> library:</li>
</ol>
<pre><code class="lang-python">import getopt
import glob
import logging
import nose
import os
import os.path
import re
import sys
def usage():
    print ()
    print ("Usage: python recipe25.py [command]")
    print ()
    print ("\t--help")
    print ("\t--doctest")
    print ("\t--suite [suite]")
    print ("\t--debug-level [info|debug]")
    print ("\t--package")
    print ("\t--publish")
    print ("\t--register")
    print ()

try:
    optlist, args = getopt.getopt(sys.argv[1:],
            "h",
           ["help", "doctest", "suite=", \
            "debug-level=", "package", \
            "publish", "register"])
except getopt.GetoptError:
    # print help information and exit:
    print "Invalid command found in %s" % sys.argv
    usage()
    sys.exit(2)</code></pre>
<ol start="3">
<li>Create a function that maps to <kbd>-test</kbd>:</li>
</ol>
<pre><code class="lang-python">def test(test_suite, debug_level):
    logger = logging.getLogger("recipe25")
    loggingLevel = debug_level
    logger.setLevel(loggingLevel)
    ch = logging.StreamHandler()
    ch.setLevel(loggingLevel)
    formatter = logging.Formatter("%(asctime)s - %(name)s - 
(levelname)s - %(message)s")
    ch.setFormatter(formatter)
    logger.addHandler(ch)
   
    nose.run(argv=["", test_suite, "--verbosity=2"])</code></pre>
<ol start="4">
<li>Create a function that maps to <kbd>-doctest</kbd>:</li>
</ol>
<pre><code class="lang-python">def doctest(test_suite=None):
    args = ["", "--with-doctest"]
    if test_suite is not None:
        print ("Running doctest suite %s" % test_suite)
        args.extend(test_suite.split(','))
        nose.run(argv=args)
    else:
        nose.run(argv=args)</code></pre>
<ol start="5">
<li>Create stub functions that support <kbd>package</kbd>, <kbd>publish</kbd>, and <kbd>register</kbd>:</li>
</ol>
<pre><code class="lang-python">def package(): 
    print ("This is where we can plug in code to run " + \
          "setup.py to generate a bundle.")

def publish():
    print ("This is where we can plug in code to upload " + \
          "our tarball to S3 or some other download site.")

def register():
    print ("setup.py has a built in function to " + \
          "'register' a release to PyPI. It's " + \
          "convenient to put a hook in here.")
    # os.system("%s setup.py register" % sys.executable)</code></pre>
<ol start="6">
<li>Add some code that detects if the option list is empty. If so, have it print out the help menu and exit the script:</li>
</ol>
<pre><code class="lang-python">if len(optlist) == 0:
    usage()
    sys.exit(1)</code></pre>
<ol start="7">
<li>Add some code that defines debug levels and then parses options to allow the user to override:</li>
</ol>
<pre><code class="lang-python">debug_levels = {"info":logging.INFO, "debug":logging.DEBUG}
# Default debug level is INFO
debug_level = debug_levels["info"]

for option in optlist:
    if option[0] in ("--debug-level"):
        # Override with a user-supplied debug level
        debug_level = debug_levels[option[1]]</code></pre>
<ol start="8">
<li>Add some code that scans the command-line options for <kbd>-help</kbd>, and, if found, exits the script:</li>
</ol>
<pre><code class="lang-python"># Check for help requests, which cause all other
# options to be ignored.
for option in optlist:
    if option[0] in ("--help", "-h"):
    usage()
    sys.exit(1)</code></pre>
<ol start="9">
<li>Add code that checks if <kbd>--doctest</kbd> has been picked. If so, have it specifically scan <kbd>--suite</kbd> and run it through the&nbsp;<kbd>doctest()</kbd>&nbsp;method. Otherwise, run <kbd>-suite</kbd> through the&nbsp;&nbsp;<kbd>test()</kbd>&nbsp;method:</li>
</ol>
<pre><code class="lang-python">ran_doctests = False
for option in optlist:
    # If --doctest is picked, then --suite is a
    # suboption.
    if option[0] in ("--doctest"):
        suite = None
        for suboption in optlist:
            if suboption[0] in ("--suite"):
                suite = suboption[1]
        print "Running doctests..."
        doctest(suite)
        ran_doctests = True

if not ran_doctests:
    for option in optlist:
        if option[0] in ("--suite"):
            print "Running test suite %s..." % option[1]
            test(option[1], debug_level)</code></pre>
<ol start="10">
<li>Finish it by iterating through each of the command-line options, and invoking the other functions based on the options that are picked:</li>
</ol>
<pre><code class="lang-python"># Parse the arguments, in order
for option in optlist:
    if option[0] in ("--package"):
        package()

    if option[0] in ("--publish"):
        publish()

    if option[0] in ("--register"):
        register()</code></pre>
<ol start="11">
<li>Run the script with <kbd>--help</kbd>, as shown in the screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000049.png" class="lazyload" /></p>
<ol start="12">
<li>Run the script with <kbd>--doctest</kbd>. Notice the first few lines of output in the following screenshot. It shows how the tests have passed and failed along with detailed output. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000086.png" class="lazyload" /></p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The output is much longer. It has been trimmed for the sake of brevity.</p>
</div>
</div>
<ol start="13">
<li>Run the script with <kbd>-doctest -suite=recipe16,recipe17.py</kbd>, as shown in the screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000028.png" class="lazyload" /></p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>We deliberately used <kbd>recipe16.py</kbd> and <kbd>recipe17.py</kbd> to demonstrate that it works with both module names and filenames.</p>
</div>
</div>
<h3>How it works...</h3>
<p>This script uses Python's <kbd>getopt</kbd> library, which is modeled after the&nbsp;<kbd>getopt()</kbd> function (refer to <a href="http://docs.python.org/library/getopt.html">http://docs.python.org/library/getopt.html</a> for more details).</p>
<p>We have wired the following functions:</p>
<ul>
<li><kbd>Usage</kbd>: A function to provide help to the user.</li>
<li><kbd>Key</kbd>: The key option definitions are included in the following block:</li>
</ul>
<pre><code class="lang-python">optlist, args = getopt.getopt(sys.argv[1:],
        "h",
       ["help", "doctest", "suite=", \
        "debug-level=", "package", \
        "publish", "register"])</code></pre>
<ul>
<li>
<ul>
<li>We parse everything in the arguments except the first, it being the executable itself.</li>
<li><kbd>"h"</kbd> defined the short option: <kbd>-h</kbd>.</li>
<li>The list defines long options. Those with <kbd>"="</kbd> accept an argument. Those without are flags.</li>
<li>If an option is received that isn't in the list, an exception is thrown, we print out <kbd>usage()</kbd>, and then exit.</li>
</ul>
</li>
</ul>
<ul>
<li><kbd>doctest</kbd>: It runs modules through nose using <kbd>-with-doctest</kbd>.</li>
<li><kbd>package</kbd>, <kbd>pubilsh</kbd>, and <kbd>register</kbd>: These are just like the functions described in the previous chapter.</li>
</ul>
<p>With each of these functions defined, we can now iterate over the options that were parsed. For this script, there is a sequence:</p>
<ol>
<li>Check whether there is a debugging override. We default to <kbd>logging.INFO</kbd>, but we provide the ability to switch to <kbd>logging.DEBUG</kbd>.</li>
<li>Check whether&nbsp;<kbd>-h</kbd> or <kbd>-help</kbd> was called. If so, print out the <kbd>usage()</kbd> information and then exit with no more parsing.</li>
<li>Because <kbd>-suite</kbd> can be used either by itself to run unittest tests, or as a suboption for <kbd>-doctest</kbd>, we have to parse through things and figure out whether&nbsp;<kbd>-doctest</kbd> was used.</li>
<li>Finally, iterate over the options, and call their corresponding functions.</li>
</ol>
<p>To exercise things, we first called this script with the <kbd>-help</kbd> option that printed out the command choices we had.</p>
<p>Then we called it with <kbd>-doctest</kbd> to see how it handled finding all the doctests in this folder. In our case, we found all the recipes for this chapter including three test failures.</p>
<p>Finally, we called the script with <kbd>-doctest -suite=recipe16,recipe17.py</kbd>. This shows how we can pick a subset of tests delineated by the comma. With this example, we see that nose can process either by the module name (<kbd>recipe16.py</kbd>) or by the filename (<kbd>recipe17.py</kbd>).</p>
<h3>There's more...</h3>
<p>The features this script provides could easily be handled by commands that are already built. We looked at <kbd>nosetests</kbd> with <kbd>doctest</kbd> earlier in this chapter and saw how it can take arguments to pick tests flexibly.</p>
<p>Using <kbd>setup.py</kbd> to generate tarballs and register releases is also a commonly used feature in the Python community.</p>
<p>So why write this script? Because, we can exploit all these features with a single command.</p>
<p>&nbsp;</p>

</div>



<!--Chapter 4-->


<div class="chapter" data-chapter-number="4">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 4 </span></div>
<h1 class="chaptertitle">Testing Customer Stories with Behavior-Driven Development</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>

<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Naming tests that sound like sentences and stories</li>
<li>Testing separate doctest documents</li>
<li>Writing a testable story with doctest</li>
<li>Writing a testable novel with doctest</li>
<li>Writing a testable story with Voidspace Mock and nose</li>
<li>Writing a testable story with mockito and nose</li>
<li>Writing a testable story with Lettuce</li>
<li>Using Should DSL to write succinct assertions with Lettuce</li>
<li>Updating the project-level script to run this chapter's BDD tests</li>
</ul>
<h2>Introduction</h2>
<p><strong>Behavior-driven development</strong> (<strong>BDD</strong>) was created as a response to <strong>test-driven development</strong> (<strong>TDD</strong>) by Dan North. It focuses on writing automated tests in a natural language that non-programmers can read.</p>
<blockquote>
<p>"Programmers wanted to know where to start, what to test and what not to test, how much to test in one go, what to call their tests, and how to understand why a test fails. The deeper I got into TDD, the more I felt that my own journey had been less of a wax-on, wax-off process of gradual mastery than a series of blind alleys. I remember thinking, 'If only someone had told me that!' far more often than I&nbsp;thought, 'Wow, a door has opened.' I decided it must be possible to present TDD in a way that gets straight to the good stuff and avoids all the pitfalls."</p>
<p>&nbsp;&ndash;&nbsp;Dan North</p>
</blockquote>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>To discover more about Dan North, please visit: <a href="https://dannorth.net/introducing-bdd/">https://dannorth.net/introducing-bdd/</a>.</p>
</div>
</div>
<p>The tests that we have written in prior unit test recipes had a style of <kbd>testThis</kbd>&nbsp;and <kbd>testThat</kbd>. BDD takes the approach of getting out of speaking programmers and instead shifting to a more customer-oriented perspective.</p>
<p>Dan North goes on to point out how Chris Stevenson wrote a specialized test runner for Java's JUnit that printed test results in a different way. Let's take a look at the following test code:</p>
<pre><code class="lang-python">public class FooTest extends TestCase  {
    public void testIsASingleton() {}
    public void testAReallyLongNameIsAGoodThing() {}
}</code></pre>
<p>This code, when run through AgileDox (<a href="http://agiledox.sourceforge.net/">http://agiledox.sourceforge.net/</a>), will print out in the following format:</p>
<pre><code class="lang-python">Foo
-is a singleton
-a really long name is a good thing</code></pre>
<p>AgileDox does several things such as the following:</p>
<ul>
<li>It prints out the test name with the test suffix dropped</li>
<li>It strips out the test prefix from each test method</li>
<li>It converts the remainder into a sentence</li>
</ul>
<p>AgileDox is a Java tool, so we won't be exploring it in this chapter. But there are many Python tools available, and we will look at some, including doctest, Voidspace Mock, <kbd>mockito</kbd>, and Lettuce. All of these tools give us the means to write tests in a more natural language and empower customers, QA, and test teams to develop story-based tests.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>All the tools and styles of BDD could easily fill up an entire book. This chapter intends to introduce the philosophy of BDD along with some strong, stable tools used to effectively test our system's behavior.</p>
</div>
</div>
<p>For this chapter, let's use the same shopping cart application for each recipe. Create a file called <kbd>cart.py</kbd> and add the following code:</p>
<pre><code class="lang-python">class ShoppingCart(object):
    def __init__(self):
       self.items = []
    def add(self, item, price):
       for cart_item in self.items:
           # Since we found the item, we increment
           # instead of append
           if cart_item.item == item: 
              cart_item.q += 1
              return self
       # If we didn't find, then we append 
       self.items.append(Item(item, price))
       return self
    def item(self, index):
        return self.items[index-1].item
    def price(self, index):
        return self.items[index-1].price * self.items[index-1].q
    def total(self, sales_tax):
        sum_price=sum([item.price*item.q for item in self.items])
        return sum_price*(1.0 + sales_tax/100.0)
    def __len__(self):
        return sum([item.q for item in self.items])
class Item(object):
    def __int__(self,item,price,q=1):
        self.item=item
        self.price=price
        self.q=q</code></pre>
<p>Consider the following, regarding this shopping cart:</p>
<ul>
<li>It is one-based, meaning the first item and price are at <kbd>[1]</kbd>, not <kbd>[0]</kbd></li>
<li>It includes the ability to have multiples of the same item</li>
<li>It will calculate total price and then add taxes</li>
</ul>
<p>This application isn't complex. Instead, it provides us opportunities throughout this chapter to test various customer stories and scenarios that aren't necessarily confined to simple unit testing.</p>
<h2>Naming tests that sound like sentences and stories</h2>
<p>Test methods should read like sentences, and test cases should read like titles of chapters. This is part of BDD's philosophy of making tests easy to read for non-programmers.</p>
<h3>Getting ready</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will explore how to write a custom <kbd>nose</kbd> plugin that provides results in a BDD-style report:</p>
<ol>
<li>Create a file called <kbd>recipe26.py</kbd> to contain our test cases.</li>
<li>Create a unittest test, where the test case represents a cart with one item, and the test methods read like sentences:</li>
</ol>
<pre><code class="lang-python">import unittest
from cart import *
class CartWithOneItem(unittest.TestCase):
      def setUp(self):
          self.cart = ShoppingCart().add("tuna sandwich", 15.00)
      def test_when_checking_the_size_should_be_one_based(self):
          self.assertEquals(1, len(self.cart))
      def test_when_looking_into_cart_should_be_one_based(self):
          self.assertEquals("tuna sandwich", self.cart.item(1))
          self.assertEquals(15.00, self.cart.price(1))
      def test_total_should_have_in_sales_tax(self):
          self.assertAlmostEquals(15.0*1.0925, \
                              self.cart.total(9.25), 2)</code></pre>
<ol start="3">
<li>Add a unittest test, where the test case represents a cart with two items, and the test methods read like sentences:</li>
</ol>
<pre><code class="lang-python">class CartWithTwoItems(unittest.TestCase):
     def setUp(self):
         self.cart = ShoppingCart()\
                       .add("tuna sandwich", 15.00)\
                       .add("rootbeer", 3.75) 
    def test_when_checking_size_should_be_two(self):
        self.assertEquals(2, len(self.cart))
    def test_items_should_be_in_same_order_as_entered(self):
       self.assertEquals("tuna sandwich", self.cart.item(1))
       self.assertAlmostEquals(15.00, self.cart.price(1), 2)
       self.assertEquals("rootbeer", self.cart.item(2)) 
       self.assertAlmostEquals(3.75, self.cart.price(2), 2)
   def test_total_price_should_have_in_sales_tax(self):
       self.assertAlmostEquals((15.0+3.75)*1.0925,self.cart.total(9.25),2)</code></pre>
<ol start="4">
<li>Add a unittest test, where the test case represents a cart with no items, and the test methods read like sentences:</li>
</ol>
<pre><code class="lang-python">class CartWithNoItems(unittest.TestCase): 
    def setUp(self):
       self.cart = ShoppingCart()
   def test_when_checking_size_should_be_empty(self): 
      self.assertEquals(0, len(self.cart))
   def test_finding_item_out_of_range_should_raise_error(self):
      self.assertRaises(IndexError, self.cart.item, 2)
   def test_finding_price_out_of_range_should_raise_error(self): 
      self.assertRaises(IndexError, self.cart.price, 2)
   def test_when_looking_at_total_price_should_be_zero(self):
      self.assertAlmostEquals(0.0, self.cart.total(9.25), 2)
   def test_adding_items_returns_back_same_cart(self): 
      empty_cart = self.cart
      cart_with_one_item=self.cart.add("tuna sandwich",15.00)
      self.assertEquals(empty_cart, cart_with_one_item) 
      cart_with_two_items = self.cart.add("rootbeer", 3.75) 
      self.assertEquals(empty_cart, cart_with_one_item)
      self.assertEquals(cart_with_one_item, cart_with_two_items)</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>BDD encourages using very descriptive sentences for method names. Several of these method names were shortened to fit the format of this book, and yet some were still too long.</p>
</div>
</div>
<ol start="5">
<li>Create another file called <kbd>recipe26_plugin.py</kbd> to contain our customized BDD runner.</li>
<li>Create a <kbd>nose</kbd> plugin that can be used as <kbd>&ndash;with-bdd</kbd> to print out results:</li>
</ol>
<pre><code class="lang-python">import sys
err = sys.stderr
import nose
import re
from nose.plugins import Plugin
class BddPrinter(Plugin): 
     name = "bdd"
     def __init__(self): 
         Plugin.__init__(self) 
         self.current_module = None</code></pre>
<ol start="7">
<li>Create a handler that prints out either the module or the test method, with extraneous information stripped out:</li>
</ol>
<pre><code class="lang-python">def beforeTest(self, test): 
    test_name = test.address()[-1]
    module, test_method = test_name.split(".") 
    if self.current_module != module:
       self.current_module = module
    fmt_mod = re.sub(r"([A-Z])([a-z]+)", r"\1\2 ", module)
    err.write("\nGiven %s" % fmt_mod[:-1].lower()) 
    message = test_method[len("test"):]
    message = " ".join(message.split("_")) err.write("\n- %s" % message)</code></pre>
<ol start="8">
<li>Create a handler for success, failure, and error messages:</li>
</ol>
<pre><code class="lang-python">def addSuccess(self, *args, **kwargs): 
    test = args[0]
    err.write(" : Ok")
def addError(self, *args, **kwargs): 
    test, error = args[0], args[1] 
    err.write(" : ERROR!\n")
def addFailure(self, *args, **kwargs): 
    test, error = args[0], args[1] 
    err.write(" : Failure!\n")</code></pre>
<ol start="9">
<li>Create a new file called <kbd>recipe26_plugin.py</kbd> to contain a test runner for exercising this recipe.</li>
<li>Create a test runner that pulls in the test cases and runs them through <kbd>nose</kbd>, printing out results in an easy-to-read fashion:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__": 
   import nose
   from recipe26_plugin import *
   nose.run(argv=["", "recipe26", "--with-bdd"], plugins=[BddPrinter()])</code></pre>
<ol start="11">
<li>Run the test runner. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000019.png" class="lazyload" /></p>
<ol start="12">
<li>Introduce a couple of bugs in the test cases, and rerun the test runner to see how this alters the output:</li>
</ol>
<pre><code class="lang-python">    def test_when_checking_the_size_should_be_one_based(self):
        self.assertEquals(2, len(self.cart))
...
    def test_items_should_be_in_same_order_as_entered(self): 
        self.assertEquals("tuna sandwich", self.cart.item(1)) 
        self.assertAlmostEquals(14.00, self.cart.price(1), 2) 
        self.assertEquals("rootbeer", self.cart.item(2)) 
        self.assertAlmostEquals(3.75, self.cart.price(2), 2)</code></pre>
<ol start="13">
<li>Run the tests again. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000008.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The test cases are written as nouns, describing the object being tested. <kbd>CartWithTwoItems</kbd> describes a series of test methods centered on a shopping cart that is prepopulated with two items.</p>
<p>The test methods are written like sentences. They are strung together with underscores instead of spaces. They have to be prefixed with <kbd>test_</kbd>, so that unittest will pick them up. <kbd>test_items_should_be_in_the_same_order_as_entered</kbd>&nbsp;should represent items&nbsp;that&nbsp;should be in the same order as entered.</p>
<p>The idea is that we should be able to quickly understand what is being tested by putting these two together: Given a cart with two items, the items should be in the same order as entered.</p>
<p>While we could read through the test code with this thought process, mentally subtracting out the cruft of underscores and the <kbd>test</kbd> prefix, this can become a real cognitive load for us. To make it easier, we coded a quick <kbd>nose</kbd> plugin that split up the camel-case tests and replaced the underscores with spaces. This led to the useful report format.</p>
<p>Using this type of quick tool encourages us to write detailed test methods that will be easy to read on output. The feedback, not just to us but to our test team and customers, can be very effective at fostering communications, confidence in software, and help with generating new test stories.</p>
<h3>There's more...</h3>
<p>The example test methods shown here were deliberately shortened to fit the format of the book. Don't try to make them as short as possible. Instead, try to describe the expected output.</p>
<p>The plugin isn't installable.&nbsp;This plugin was coded to quickly generate a report. To make it reusable, especially with <kbd>nosetests</kbd>.</p>
<p>&nbsp;</p>
<h2>Testing separate doctest documents</h2>
<p>BDD doesn't require that we use any particular tool. Instead, it's more focused on the approach to testing. That is why it's possible to use Python <kbd>doctest</kbd>&nbsp;to write BDD test scenarios. <kbd>doctest</kbd> isn't restricted to the module's code. With this recipe, we will explore creating independent text files to run through Python's <kbd>doctest</kbd> library.</p>
<p>If this is <kbd>doctest</kbd>, why wasn't it included in the previous chapter's recipes? Because the context of writing up a set of tests in a separate test document fits more naturally into the philosophy of BDD than with testable docstrings that are available for introspection when working with a library.</p>
<h3>Getting ready&nbsp;</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will explore capturing various test scenarios in <kbd>doctest</kbd> files and then running them:</p>
<ol>
<li>Create a file called <kbd>recipe27_scenario1.doctest</kbd> that contains <kbd>doctest</kbd>-style type tests to exercise the shopping cart:</li>
</ol>
<pre><code class="lang-python">This is a way to exercise the shopping cart 
from a pure text file containing tests.
First, we need to import the modules 
&gt;&gt;&gt; from cart import *
Now, we can create an instance of a cart 
&gt;&gt;&gt; cart = ShoppingCart()
Here we use the API to add an object. Because it returns back the cart, we have to deal with the output
&gt;&gt;&gt; cart.add("tuna sandwich", 15.00) #doctest:+ELLIPSIS 
&lt;cart.ShoppingCart object at ...&gt;
Now we can check some other outputs
&gt;&gt;&gt; cart.item(1) 
'tuna sandwich' 
&gt;&gt;&gt; cart.price(1) 
15.0
&gt;&gt;&gt; cart.total(0.0) 
15.0</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Notice that there are no quotes surrounding the text.</p>
</div>
</div>
<ol start="2">
<li>Create another scenario in the<kbd>recipe27_scenario2.doctest</kbd>&nbsp;file that tests the boundaries of the shopping cart, as shown here:</li>
</ol>
<pre><code class="lang-python">This is a way to exercise the shopping cart 
from a pure text file containing tests.
First, we need to import the modules 
&gt;&gt;&gt; from cart import *
Now, we can create an instance of a cart 
&gt;&gt;&gt; cart = ShoppingCart()
Now we try to access an item out of range, expecting an exception.
&gt;&gt;&gt; cart.item(5)
Traceback (most recent call last): 
...
IndexError: list index out of range
We also expect the price method to fail in a similar way.
&gt;&gt;&gt; cart.price(-2)
Traceback (most recent call last): 
...
IndexError: list index out of range</code></pre>
<ol start="3">
<li>Create a file called <kbd>recipe27.py</kbd> and put in the test runner code that finds files ending in <kbd>.doctest</kbd> and runs them through the&nbsp;<kbd>testfile</kbd> method within <kbd>doctest</kbd>:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
   import doctest
   from glob import glob
   for file in glob("recipe27*.doctest"):
      print ("Running tests found in %s" % file) 
      doctest.testfile(file)</code></pre>
<ol start="4">
<li>Run the test suite. Take a look at this code:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000056.png" class="lazyload" /></p>
<ol start="5">
<li>Run the test suite with <kbd>-v</kbd>, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000144.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p><kbd>doctest</kbd> provides the convenient <kbd>testfile</kbd> function that will exercise a block of pure text as if it were contained inside a docstring. This is why no quotations are needed compared to when we had multiple <kbd>doctest</kbd> inside docstrings. The text files aren't docstrings.</p>
<p>In fact, if we include triple quotes around the text, the tests won't work correctly. Let's take the first scenario&mdash;put <kbd>"""</kbd> at the top and bottom of the file, and save it as <kbd>recipe27_bad_ scenario.txt</kbd>. Now, let's create a file called <kbd>recipe27.py</kbd>, and create an alternate test runner that runs our bad scenario, as shown here:</p>
<pre><code class="lang-python">if __name__ == "__main__":
   import doctest
   doctest.testfile("recipe27_bad_scenario.txt")

</code></pre>
<p>&nbsp;We get the following error message:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000112.png" class="lazyload" /></p>
<p>It has confused the tail-end triple quotes as part of the expected output. It's best to just leave them out.</p>
<h3>There's more...</h3>
<p>What is so great about moving docstrings into separate files? Isn't this the same thing that we were doing in the&nbsp;<em>Creating</em> <em>testable</em> <em>documentation</em> <em>with</em> <em>doctest</em> recipe discussed in Chapter 2, <em>Running Automated Test Suites with Nose</em>? Yes and no. Yes, it's technically the same thing: <kbd>doctest</kbd> is exercising blocks of code embedded in the&nbsp;test.</p>
<p>But BDD is more than simply a technical solution. It is driven by the philosophy of <em>customer-readable</em> <em>scenarios</em>. BDD aims to test the behavior of the system. The behavior is often defined by customer-oriented scenarios. Getting a hold of these scenarios is strongly encouraged when our customer can easily understand the scenarios that we have captured. It is further enhanced when the customer can see what passes and fails and, in turn, sees a realistic status of what has been accomplished.</p>
<p>By decoupling our test scenarios from the code and putting them into separate files, we have the key ingredient to making readable tests for our customers using <kbd>doctest</kbd>.</p>
<h4>Doesn't this defy the usability of docstrings?</h4>
<p>In Chapter 2,&nbsp;<em>Running Automated Test Suites with Nose,</em> there are several recipes that show how convenient it is to embed examples of code usage in docstrings. They are convenient, because we can read the docstrings from an interactive Python shell. What do you think is different about pulling some of this out of the code into separate scenario files? Do you think there are some <kbd>doctest</kbd> that would be useful in docstrings and others that may serve us better in separate scenario files?</p>
<h2>Writing a testable story with doctest</h2>
<p>Capturing a succinct story in a <kbd>doctest</kbd> file is the key to BDD. Another aspect of BDD is providing a readable report including the results.</p>
<h3>Getting ready</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will see how to write a custom <kbd>doctest</kbd> runner to make our own report:</p>
<ol>
<li>Create a new file called <kbd>recipe28_cart_with_no_items.doctest</kbd> to contain our <kbd>doctest</kbd> scenario.</li>
<li>Create a <kbd>doctest</kbd> scenario that exercises the shopping cart, as shown here:</li>
</ol>
<pre><code class="lang-python">This scenario demonstrates a testable story.
First, we need to import the modules 
&gt;&gt;&gt; from cart import *
&gt;&gt;&gt; cart = ShoppingCart()
#when we add an item
&gt;&gt;&gt; cart.add("carton of milk", 2.50) #doctest:+ELLIPSIS 
&lt;cart.ShoppingCart object at ...&gt;
#the first item is a carton of milk 
&gt;&gt;&gt; cart.item(1)
'carton of milk'
#the first price is $2.50 
&gt;&gt;&gt; cart.price(1)
2.5
#there is only one item 
&gt;&gt;&gt; len(cart)
This shopping cart lets us grab more than one of a particular item.
#when we add a second carton of milk
&gt;&gt;&gt; cart.add("carton of milk", 2.50) #doctest:+ELLIPSIS 
&lt;cart.ShoppingCart object at ...&gt;
#the first item is still a carton of milk 
&gt;&gt;&gt; cart.item(1)
'carton of milk'
#but the price is now $5.00 
&gt;&gt;&gt; cart.price(1)
5.0
#and the cart now has 2 items 
&gt;&gt;&gt; len(cart)
2
#for a total (with 10% taxes) of $5.50 
&gt;&gt;&gt; cart.total(10.0)
5.5</code></pre>
<ol start="3">
<li>Create a new file called <kbd>recipe28.py</kbd> to contain our custom <kbd>doctest</kbd> runner.</li>
<li>Create a customer <kbd>doctest</kbd> runner by sub classing <kbd>DocTestRunner</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">import doctest
class BddDocTestRunner(doctest.DocTestRunner): 
      """
      This is a customized test runner. It is meant 
      to run code examples like DocTestRunner,
      but if a line preceeds the code example 
      starting with '#', then it prints that 
      comment.
      If the line starts with '#when', it is printed 
      out like a sentence, but with no outcome.
      If the line starts with '#', but not '#when'
      it is printed out indented, and with the outcome.
      """</code></pre>
<ol start="5">
<li>Add a <kbd>report_start</kbd> function that looks for comments starting with <kbd>#</kbd> before an example, as shown in this code:</li>
</ol>
<pre><code class="lang-python">def report_start(self, out, test, example):
    prior_line = example.lineno-1
    line_before = test.docstring.splitlines()[prior_line] 
    if line_before.startswith("#"):
       message = line_before[1:]
       if line_before.startswith("#when"):
          out("* %s\n" % message) 
          example.silent = True 
          example.indent = False
       else:
         out(" - %s: " % message) 
         example.silent = False 
         example.indent = True
   else:
     example.silent = True 
     example.indent = False

   doctest.DocTestRunner(out, test, example)</code></pre>
<ol start="6">
<li>Add a <kbd>report_success</kbd> function that conditionally prints out <kbd>ok</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">def report_success(self, out, test, example, got):
    if not example.silent:
       out("ok\n")
    if self._verbose:
       if example.indent: out(" ") 
          out("&gt;&gt;&gt; %s\n" % example.source[:-1])</code></pre>
<ol start="7">
<li>Add a <kbd>report_failure</kbd> function that conditionally prints out <kbd>FAIL</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">def report_failure(self, out, test, example, got):
    if not example.silent:
       out("FAIL\n")
    if self._verbose:
       if example.indent: out(" ") 
           out("&gt;&gt;&gt; %s\n" % example.source[:-1])</code></pre>
<ol start="8">
<li>Add a runner that replaces <kbd>doctest.DocTestRunner</kbd> with our customer runner, and then looks for <kbd>doctest</kbd> files to run, as shown here:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
   from glob import glob
   doctest.DocTestRunner = BddDocTestRunner
   for file in glob("recipe28*.doctest"):
       given = file[len("recipe28_"):]
       given = given[:-len(".doctest")]
       given = " ".join(given.split("_"))
       print ("===================================")
       print ("Given a %s..." % given)
       print ("===================================")
       doctest.testfile(file)</code></pre>
<ol start="9">
<li>Use the runner to exercise our scenario. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000126.png" class="lazyload" /></p>
<ol start="10">
<li>Use the runner to exercise our scenario with <kbd>-v</kbd>, as shown in this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000017.png" class="lazyload" /></p>

<ol start="11">
<li>Alter the test scenario so that one of the expected outcomes fail by using this code:</li>
</ol>
<pre><code class="lang-python">#there is only one item 
&gt;&gt;&gt; len(cart)
4668</code></pre>

<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>Notice we have changed the expected outcome from <kbd>1</kbd> to <kbd>4668</kbd>, to guarantee a failure.</p>
</div>
</div>
<ol start="12">
<li>Use the runner with <kbd>-v</kbd> again, and see the results. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000105.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p><kbd>doctest</kbd> provides a convenient means to write a testable scenario. For starters, we wrote up a series of behaviors we wanted the shopping cart application to prove. To polish things up, we added lot of detailed comments so that anyone reading this document can clearly understand things.</p>
<p>This provides us with a testable scenario. However, it leaves us short of one key thing: <em>a</em> <em>succinct</em> <em>report</em>.</p>
<p>Unfortunately, <kbd>doctest</kbd> won't print out all these detailed comments for us.</p>
<p>To make this usable from a BDD perspective, we need the ability to embed selective comments that get printed out when the test sequence runs. To do that, we will subclass <kbd>doctest.DocTestRunner</kbd> and insert our version of the handling of the docstring.</p>
<h3>There's more...</h3>
<p><kbd>DocTestRunner</kbd> conveniently gives us a handle on the docstring as well as the exact line number where the code example starts. We coded our <kbd>BddDocTestRunner</kbd> to look at the line preceding it, and check to see whether it started with <kbd>#</kbd>, our custom marker for a piece of text to print out during a test run.</p>
<p>A <kbd>#when</kbd> comment is considered a cause. In other words, a <kbd>when</kbd> causes one or more <em>effects</em>. While <kbd>doctest</kbd> will still verify the code involved with a <kbd>when</kbd>; for BDD purposes, we don't really care about the outcome, so we silently ignore it.</p>
<p>Any other <kbd>#</kbd> comments are considered effects. For each of these, we strip out the <kbd>#</kbd>&nbsp;and then print the sentence indented, so we can easily see which <kbd>when</kbd> it is tied to. Finally, we print out either <kbd>ok</kbd> or <kbd>FAIL</kbd> to indicate the results.</p>
<p>This means we can add all the detail we want to the documentation. But for blocks of tests, we can add statements that will be printed as either <em>causes</em> (<kbd>#when</kbd>) or effects (<kbd>#anything else</kbd>).</p>
<h2>Writing a testable novel with doctest</h2>
<p>Running a series of story tests showcases your code's expected behavior. We have previously seen in the <em>Writing</em> <em>a</em> <em>testable</em> <em>story</em> <em>with</em> <em>doctest</em> recipe how to build a testable story and have it generate a useful report.</p>
<p>With this recipe, we will see how to use this tactic to string together multiple testable stories to form a testable novel.</p>
<h3>Getting ready</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter.</p>
<p>We will also reuse the <kbd>BddDocTestRunner</kbd> defined in this chapter's <em>Writing</em> <em>a</em> <em>testable</em> <em>story</em> <em>with</em> <em>doctest</em> recipe. But we will slightly alter it by implementing the following steps.</p>
<h3>How to do it...</h3>
<ol>
<li>Create a new file called <kbd>recipe29.py</kbd>.</li>
<li>Copy the code containing the <kbd>BddDocTestRunner</kbd> from the <em>Writing</em> <em>a</em> <em>testable</em> <em>story</em> <em>with</em> <em>doctest</em> recipe into <kbd>recipe29.py</kbd>.</li>
<li>Alter the <kbd>__main__</kbd> runnable to only search for this recipe's <kbd>doctest</kbd> scenarios, as shown in this code:</li>
</ol>
<pre><code class="lang-python">if __name__ == "__main__":
   from glob import glob
   doctest.DocTestRunner = BddDocTestRunner
   for file in glob("recipe29*.doctest"):
       given = file[len("recipe29_"):] 
       given = given[:-len(".doctest")]
       given = " ".join(given.split("_"))
       print ("===================================")
       print ("Given a %s..." % given)
       print ("===================================")
       doctest.testfile(file)</code></pre>
<ol start="4">
<li>Create a new file called <kbd>recipe29_cart_we_will_load_with_identical_items.doctest</kbd>.</li>
<li>Add a scenario to it that tests the cart by adding two instances of the same object:</li>
</ol>
<pre><code class="lang-python">&gt;&gt;&gt; from cart import *
&gt;&gt;&gt; cart = ShoppingCart()
#when we add an item
&gt;&gt;&gt; cart.add("carton of milk", 2.50) #doctest:+ELLIPSIS
&lt;cart.ShoppingCart object at ...&gt;
#the first item is a carton of milk
&gt;&gt;&gt; cart.item(1)
'carton of milk'
#the first price is $2.50
&gt;&gt;&gt; cart.price(1)
2.5
#there is only one item
&gt;&gt;&gt; len(cart)
1
This shopping cart let's us grab more than one of a particular item.
#when we add a second carton of milk
&gt;&gt;&gt; cart.add("carton of milk", 2.50) #doctest:+ELLIPSIS
&lt;cart.ShoppingCart object at ...&gt;
#the first item is still a carton of milk
&gt;&gt;&gt; cart.item(1) 
'carton of milk'
#but the price is now $5.00
&gt;&gt;&gt; cart.price(1)
5.0
#and the cart now has 2 items
&gt;&gt;&gt; len(cart)
2
#for a total (with 10% taxes) of $5.50
&gt;&gt;&gt; cart.total(10.0)
5.5

</code></pre>
<ol start="6">
<li>Create another file called <kbd>recipe29_cart_we_will_load_with_two_different_items.docstest</kbd>.</li>
<li>In that file, create another scenario that tests the cart by adding two different instances, as shown in the following code:</li>
</ol>
<pre><code class="lang-python">&gt;&gt;&gt; from cart import *
&gt;&gt;&gt; cart = ShoppingCart()
#when we add a carton of milk...
&gt;&gt;&gt; cart.add("carton of milk", 2.50) #doctest:+ELLIPSIS 
&lt;cart.ShoppingCart object at ...&gt;
#when we add a frozen pizza...
&gt;&gt;&gt; cart.add("frozen pizza", 3.00) #doctest:+ELLIPSIS
 &lt;cart.ShoppingCart object at ...&gt;
#the first item is the carton of milk
&gt;&gt;&gt; cart.item(1)
'carton of milk'
#the second item is the frozen pizza
&gt;&gt;&gt; cart.item(2)
'frozen pizza'
#the first price is $2.50
&gt;&gt;&gt; cart.price(1)
2.5
#the second price is $3.00
&gt;&gt;&gt; cart.price(2)
3.0
#the total with no tax is $5.50
&gt;&gt;&gt; cart.total(0.0)
5.5
#the total with 10% tax is $6.05
&gt;&gt;&gt; print (round(cart.total(10.0), 2) )
6.05</code></pre>
<ol start="8">
<li>Create a new file called <kbd>recipe29_cart_that_we_intend_to_keep_empty.doctest</kbd>.</li>
<li>In that file, create a third scenario that tests the cart by adding nothing and yet tries to access values outside the range, as shown in this code:</li>
</ol>
<pre><code class="lang-python">&gt;&gt;&gt;from cart import *
#when we create an empty shopping cart 
&gt;&gt;&gt; cart = ShoppingCart()
#accessing an item out of range generates an exception
&gt;&gt;&gt; cart.item(5)
Traceback (most recent call last):
...
IndexError: list index out of range
#accessing a price with a negative index causes an exception
&gt;&gt;&gt; cart.price(-2)
Traceback (most recent call last):
...
IndexError: list index out of range
#calculating a price with no tax results in $0.00
&gt;&gt;&gt; cart.total(0.0)
0.0
#calculating a price with a tax results in $0.00
&gt;&gt;&gt; cart.total(10.0)
0.0</code></pre>
<ol start="10">
<li>Use the runner to execute our scenarios. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000075.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We reuse the test runner developed in the previous recipe. The key is extending the scenarios to ensure that we have complete coverage of the expected scenarios.</p>
<p>We need to be sure that we can handle the following:</p>
<ul>
<li>A cart with two identical items</li>
<li>A cart with two different items</li>
<li>The degenerate situation of an empty shopping cart</li>
</ul>
<h3>There's more...</h3>
<p>A valuable part of writing tests is picking useful names. In our situation, each testable story started with an empty cart. However, if we named each scenario <em>given an empty cart</em>, it would cause an overlap and not result in a very effective report.</p>
<p>So, instead, we named them based on our story's intention:</p>
<pre><code class="lang-python">recipe29_cart_we_will_load_with_identical_items.doctest
recipe29_cart_we_will_load_with_two_different_items.doctest
recipe29_cart_that_we_intend_to_keep_empty.doctest</code></pre>
<p>This leads to:</p>
<ul>
<li>Given a cart we will load with identical items</li>
<li>Given a cart we will load with two different items</li>
<li>Given a cart that we intend to keep empty</li>
</ul>
<p>The purpose of these scenarios is much clearer.</p>
<p>Naming scenarios are much like certain aspects of software development that are more a craft than a science. Tuning the performance tends to be more scientific, because it involves an iterative process of measurement and adjustment. But naming scenarios along with their causes and effects tends to be more of a craft. It involves communicating with all the stakeholders including QA and customers, so everyone can read and understand the stories.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Don't be intimidated. Be ready to embrace change<br />Start writing your stories. Make them work. Then share them with your stakeholders. Feedback is important, and that is the purpose of using story-based testing. Be ready for criticism and suggested changes.<br /><br />Be ready for more story requests. In fact, don't be surprised if some of your customers or QA want to write their own stories. That is a positive sign.<br /><br />If you are new to this type of customer interaction, don't worry. You will develop valuable communication skills and build a solid professional relationship with your stakeholders. And at the same time, your code quality will certainly improve.</p>
</div>
</div>
<h2>Writing a testable story with Voidspace Mock and nose</h2>
<p>When our code interacts with other classes through methods and attributes, these are referred to as collaborators. Mocking out collaborators using Voidspace Mock (<a href="http://www.voidspace.org.uk/python/mock/">http://www.voidspace.org.uk/python/mock/</a>), created by Michael Foord, provides a key tool for BDD. Mocks provide canned behavior compared to stubs, which provide canned states. While mocks by themselves don't define BDD, their usage keenly overlaps the ideas of BDD.</p>
<p>To further demonstrate the behavioral nature of the tests, we will also use the <kbd>spec</kbd>&nbsp;plugin found in the <kbd>pinocchio</kbd> project (<a href="http://darcs.idyll.org/~t/projects/pinocchio/doc">http://darcs.idyll.org/~t/projects/pinocchio/doc</a>).</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>As stated on the project's website, Voidspace Mock is experimental. This book was written using version 0.7.0 beta 3. There is the risk that more API changes will occur before reaching a stable 1.0 version. Given this project's high quality, excellent documentation, and many articles in the blogosphere, I strongly feel it deserves a place in this book.</p>
</div>
</div>
<h3>Getting ready</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter with some slight modifications:</p>
<ol>
<li>Create a new file called <kbd>recipe30_cart.py</kbd>, and copy all the code from <kbd>cart.py</kbd> created in the introduction of this chapter.</li>
<li>Alter <kbd>__init__</kbd> to add an extra <kbd>storer</kbd> attribute used for persistence:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
     def __init__(self, storer=None):
        self.items = []
        self.storer = storer</code></pre>
<ol start="3">
<li>Add a <kbd>store</kbd> method that uses the <kbd>storer</kbd> to save the cart:</li>
</ol>
<pre><code class="lang-python">    def store(self):
        return self.storer.store_cart(self)</code></pre>
<ol start="4">
<li>Add a <kbd>retrieve</kbd> method that updates the internal <kbd>items</kbd> by using the <kbd>storer</kbd>:</li>
</ol>
<pre><code class="lang-python">    def restore(self, id):
       self.items = self.storer.retrieve_cart(id).items 
       return self</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The specifics of the API of the <kbd>storer</kbd> will be given further down in this recipe.</p>
</div>
</div>
<p>We need to activate our virtual environment and then install Voidspace Mock for this recipe:</p>
<ol>
<li>Create a virtual environment, activate it, and verify the tools are working. Take a look at the following screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000013.png" class="lazyload" /></p>
<ol start="2">
<li>Install Voidspace Mock by typing <kbd>pip install mock</kbd>.</li>
<li>Install the latest version of Pinocchio by typing <kbd>pip install http://darcs.idyll.org/~t/projects/pinocchio-latest.tar.gz</kbd>.</li>
<li>This version of Pinocchio raises some warnings. To prevent them, we also need to install <kbd>figleaf</kbd> by typing <kbd>pip install figleaf</kbd>.</li>
</ol>
<h3>How to do it...</h3>
<p>With the following steps, we will explore how to use mock to write a testable story:</p>
<ol>
<li>In <kbd>recipe30_cart.py</kbd>, create a <kbd>DataAccess</kbd> class with empty methods for storing and retrieving shopping carts:</li>
</ol>
<pre><code class="lang-python">class DataAccess(object):
     def store_cart(self,cart):
         pass
     def retrieve_cart(self,id):
         pass</code></pre>
<ol start="2">
<li>Create a new file called <kbd>recipe30.py</kbd> to write the test code.</li>
<li>Create an automated unittest that exercises the cart by mocking out the methods of <kbd>DataAccess</kbd>:</li>
</ol>
<pre><code class="lang-python">import unittest
from copy import deepcopy 
from recipe30_cart import *
from mock import Mock
class CartThatWeWillSaveAndRestoreUsingVoidspaceMock(unittest. TestCase):
      def test_fill_up_a_cart_then_save_it_and_restore_it(self):
          # Create an empty shopping cart
          cart = ShoppingCart(DataAccess())
          # Add a couple of items 
          cart.add("carton of milk", 2.50) 
          cart.add("frozen pizza", 3.00)
          self.assertEquals(2, len(cart))
          # Create a clone of the cart for mocking 
          # purposes.
          original_cart = deepcopy(cart)
          # Save the cart at this point in time into a database 
          # using a mock
          cart.storer.store_cart = Mock()
          cart.storer.store_cart.return_value = 1 
          cart.storer.retrieve_cart = Mock() 
          cart.storer.retrieve_cart.return_value = original_cart
          id = cart.store()
          self.assertEquals(1, id)
          # Add more items to cart 
          cart.add("cookie dough", 1.75) 
          cart.add("ginger ale", 3.25)
          self.assertEquals(4, len(cart))
          # Restore the cart to the last point in time 
          cart.restore(id)
          self.assertEquals(2, len(cart))
          cart.storer.store_cart.assert_called_with(cart)
          cart.storer.retrieve_cart.assert_called_with(1)</code></pre>
<ol start="4">
<li>Run the test using <kbd>nosetests</kbd> with the <kbd>spec</kbd> plugin:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000077.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Mocks are test doubles that confirm method calls, which is the <em>behavior</em>. This is different from stubs, which provide canned data, allowing us to confirm states.</p>
<p>Many mocking libraries are based on the <em>record</em>/<em>replay</em> pattern. They first require the test case to <em>record</em> every behavior the mock will be subjected to when used. Then we plug the mock into our code, allowing our code to invoke calls against it. Finally, we execute <em>replay</em>, and the Mock library compares the method calls we expected with the ones that actually happened.</p>
<p>A common issue with record/replay mocking is that, if we miss a single method call, our test fails. Capturing all the method calls can become very challenging when trying to mock out third-party systems, or dealing with variable calls that may be tied to complex system states.</p>
<p>The Voidspace Mock library differs by using the <em>action</em>/<em>assert</em> pattern. We first generate a mock and define how we want it to react to certain <em>actions</em>. Then, we plug it into our code, allowing our code to operate against it. Finally, we <em>assert</em> what happened to the mock, only picking the operations we care about. There is no requirement to assert every behavior experienced by the mock.</p>
<p>Why is this important? Record/replay requires that we record the method calls that are made by our code, third-party system, and all the other layers in the call chain. Frankly, we may not need this level of confirmation of behavior. Often, we are primarily interested in the top layer of interaction. Action/assert lets us cut back on the behavior calls we care about. We can set up our mock to generate the necessary top level actions and essentially ignore the lower level calls, which a record/replay mock would force us to record.</p>
<p>In this recipe, we mocked the <kbd>DataAccess</kbd> operations <kbd>store_cart</kbd> and <kbd>retrieve_cart</kbd>. We defined their <kbd>return_value</kbd>, and at the end of the test we asserted that they were called the following:</p>
<pre><code class="lang-python">cart.storer.store_cart.assert_called_with(cart)
cart.storer.retrieve_cart.assert_called_with(1)</code></pre>
<p>&nbsp;<kbd>cart.storer</kbd> was the internal attribute that we injected with our mock.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Mocking a method means replacing a call to a real method with one to a mock object.</p><p>Stubbing a method means replacing a call to a real method with one to a stub.</p>
</div>
</div>
<h3>There's more...</h3>
<p>Because this test case focuses on storing and retrieving from the cart's perspective, we didn't have to define the real <kbd>DataAccess</kbd> calls. That is why we simply put <kbd>pass</kbd> in their method definitions.</p>
<p>This conveniently lets us work on the behavior of persistence without forcing us to choose whether the cart would be stored in a relational database, a NoSQL database, a flat file, or any other file format. This shows that our shopping cart and data persistence are nicely decoupled.</p>
<h3>Tell me more about the spec nose plugin!</h3>
<p>We quickly skimmed over the useful <kbd>spec</kbd> plugin for <kbd>nose</kbd>. It provides the same essential functionality that we coded by hand in the <em>Naming</em> <em>tests</em> <em>so</em> <em>they</em> <em>sound</em> <em>like</em> <em>sentences</em> <em>and</em> <em>stories</em> section. It converts test case names and test method names into readable results. It gives us a runnable <kbd>spec</kbd>. This plugin works with unittest and doesn't care whether or not we were using Voidspace Mock.</p>
<h3>Why didn't we reuse the plugin from the recipe "Naming tests so they sound like sentences and stories"?</h3>
<p>Another way to phrase this question is&nbsp;<em>Why did we write that recipe's plugin in the first place?</em> An important point of using test tools is to understand how they work, and how to write our own extensions. The <em>Naming</em> <em>tests</em> <em>so</em> <em>they</em> <em>sound</em> <em>like</em> <em>sentences</em> <em>and</em> <em>stories</em> section not only discussed the philosophy of naming tests, but also explored ways to write <kbd>nose</kbd> plugins to support this need. In this recipe, our focus was on using Voidspace Mock to verify certain behaviors, and not on coding <kbd>nose</kbd> plugins. Producing a nice BDD report was easily served by the existing <kbd>spec</kbd> plugin.</p>
<h3>See also</h3>
<p><em>Writing a testable story with mockito and nose.</em></p>
<h2>Writing a testable story with mockito and nose</h2>
<p>When our code interacts with other classes through methods and attributes, these are referred to as collaborators. Mocking out collaborators using <kbd>mockito</kbd> (<a href="http://code.google.com/p/mockito">http://code.google.com/p/mockito</a> and <a href="http://code.google.com/p/mockito-python">http://code.google.com/p/mockito-python</a>) provides a key tool for BDD. Mocks provide canned behavior, whereas stubs,&nbsp; provide canned states. While mocks by themselves don't define BDD, their usage keenly overlaps the ideas of BDD.</p>
<p>To further demonstrate the behavioral nature of the tests, we will also use the <kbd>spec</kbd>&nbsp;plugin found in the <kbd>pinocchio</kbd> project (<a href="http://darcs.idyll.org/~t/projects/">http://darcs.idyll.org/~t/projects/ pinocchio/doc</a>).</p>
<h3>Getting ready</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter with some slight modifications:</p>
<ol>
<li>Create a new file called <kbd>recipe31_cart.py</kbd> and copy all the code from <kbd>cart.py</kbd> created in the introduction of this chapter.</li>
</ol>
<p>&nbsp;</p>
<ol start="2">
<li>Alter <kbd>__init__</kbd> to add an extra <kbd>storer</kbd> attribute used for persistence:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
    def __init__(self, storer=None):
    self.items = []
    self.storer = storer</code></pre>
<ol start="3">
<li>Add a <kbd>store</kbd> method that uses the <kbd>storer</kbd> to save the cart:</li>
</ol>
<pre><code class="lang-python">   def store(self):
       return self.storer.store_cart(self)</code></pre>
<ol start="4">
<li>Add a <kbd>retrieve</kbd> method that updates the internal <kbd>items</kbd> by using the <kbd>storer</kbd>:</li>
</ol>
<pre><code class="lang-python">  def restore(self, id):
      self.items = self.storer.retrieve_cart(id).items
      return self</code></pre>
<p>The specifics of the API of the storer will be given further down in this recipe.</p>
<p>We need to activate our virtual environment and then install <kbd>mockito</kbd> for this recipe:</p>
<ol>
<li>Create a virtual environment, activate it, and verify the tools are working:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000068.png" class="lazyload" /></p>
<ol start="2">
<li>Install <kbd>mockito</kbd> by typing <kbd>pip install mockito</kbd>.</li>
</ol>
<p>Install <kbd>pinocchio</kbd> and <kbd>figleaf</kbd> using the same steps from the <em>Writing a testable story with Voidspace Mock and nose</em> recipe.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will explore how to use mocking to write a testable story:</p>
<ol>
<li>In <kbd>recipe31_cart.py</kbd>, create a <kbd>DataAccess</kbd> class with empty methods for storing and retrieving shopping carts:</li>
</ol>
<pre><code class="lang-python">class DataAccess(object):
     def store_cart(self, cart):
         pass
     def retrieve_cart(self, id):
         pass</code></pre>
<ol start="2">
<li>Create a new file called <kbd>recipe31.py</kbd> for writing the test code.</li>
<li>Create an automated unit test that exercises the cart by mocking out the methods of <kbd>DataAccess</kbd>:</li>
</ol>
<pre><code class="lang-python">import unittest
from copy import deepcopy
from recipe31_cart import *
from mockito import *
class CartThatWeWillSaveAndRestoreUsingMockito(unittest.TestCase):
      def test_fill_up_a_cart_then_save_it_and_restore_it(self):
          # Create an empty shopping cart
          cart = ShoppingCart(DataAccess())
          # Add a couple of items
          cart.add("carton of milk", 2.50)
          cart.add("frozen pizza", 3.00)
          self.assertEquals(2, len(cart))
         # Create a clone of the cart for mocking
         # purposes.
         original_cart = deepcopy(cart)
         # Save the cart at this point in time into a database
         # using a mock
         cart.storer = mock()
         when(cart.storer).store_cart(cart).thenReturn(1)
         when(cart.storer).retrieve_cart(1). \   
                             thenReturn(original_cart)
         id = cart.store()
         self.assertEquals(1, id)
         # Add more items to cart
         cart.add("cookie dough", 1.75)
         cart.add("ginger ale", 3.25)
         self.assertEquals(4, len(cart))
         # Restore the cart to the last point in time
         cart.restore(id)
         self.assertEquals(2, len(cart))
         verify(cart.storer).store_cart(cart)
         verify(cart.storer).retrieve_cart(1)

</code></pre>
<ol start="4">
<li>Run the test using <kbd>nosetests</kbd> with the <kbd>spec</kbd> plugin:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000041.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This recipe is very similar to the earlier recipe,&nbsp;<em>Writing</em> <em>a</em> <em>testable</em> <em>story</em> <em>with</em>&nbsp;<em>Voidspace</em>&nbsp;M<em>ock</em> <em>and</em> <em>nose</em>. For details about mocking and the benefits with regards to BDD, it is very useful to read that recipe.</p>
<p>Let's compare the syntax of Voidspace Mock with <kbd>mockito</kbd> to get a feel for the differences. Look at the following Voidspace Mock block of code:</p>
<pre><code class="lang-python">         cart.storer.store_cart = Mock()
         cart.storer.store_cart.return_value = 1
         cart.storer.retrieve_cart = Mock()
         cart.storer.retrieve_cart.return_value = original_cart</code></pre>
<p>It shows the function <kbd>store_cart</kbd> being mocked:</p>
<pre><code class="lang-python">         cart.storer = mock()
         when(cart.storer).store_cart(cart).thenReturn(1)
         when(cart.storer).retrieve_cart(1).thenReturn(original_cart)</code></pre>
<p><kbd>mockito</kbd> approaches this by mocking out the entire <kbd>storer</kbd> object. <kbd>mockito</kbd> originated as a Java mocking tool, which explains its Java-ish APIs like <kbd>thenReturn</kbd>, compared to Voidspace Mock's Pythonic style of <kbd>return_value</kbd>.</p>
<p>Some find this influence from Java on Python's implementation of <kbd>mockito</kbd> distasteful. Frankly, I believe that is insufficient reason to discard a library. In the previous example, <kbd>mockito</kbd> records the desired behavior in a more succinct fashion, something that would definitely offset the Java-like API.</p>
<h3>See also</h3>
<p><em>Writing a testable story with Voidspace Mock and nose.</em></p>
<h2>Writing a testable story with Lettuce</h2>
<p><strong>Lettuce</strong> (<a href="http://lettuce.it">http://lettuce.it</a>) is a Cucumber-like BDD tool built for Python.</p>
<p>Cucumber (<a href="http://cukes.info">http://cukes.info</a>) was developed by the Ruby community and provides a way to write scenarios in a textual style. By letting our stakeholders read the stories, they can easily discern what the software is expected to do.</p>
<p>This recipe shows how to install Lettuce, write a test story, and then wire it into our shopping cart application to exercise our code.</p>
<h3>Getting ready...</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter. We also need to install Lettuce and its dependencies.</p>
<p>Install Lettuce by typing <kbd>pip install lettuce</kbd>.</p>
<h3>How to do it...</h3>
<p>In the following steps, we will explore creating some testable stories with Lettuce, and wiring them to runnable Python code:</p>
<ol>
<li>Create a new folder called <kbd>recipe32</kbd> to contain all the files in this recipe.</li>
<li>Create a file named <kbd>recipe32.feature</kbd> to capture our story. Write the top-level description of our new feature, based on our shopping cart:</li>
</ol>
<pre><code class="lang-python">Feature: Shopping cart As a shopper
   I want to load up items in my cart
   So that I can check out and pay for them</code></pre>
<ol start="3">
<li>Let's first create a scenario that captures the behavior of the cart when it's empty:</li>
</ol>
<pre><code class="lang-python">       Scenario: Empty cart
            Given an empty cart
            Then looking up the fifth item causes an error
            And looking up a negative price causes an error
            And the price with no taxes is $0.00
            And the price with taxes is $0.00</code></pre>
<ol start="4">
<li>Add another scenario that shows what happens when we add cartons of milk:</li>
</ol>
<pre><code class="lang-python">       Scenario: Cart getting loaded with multiple of the same 
            Given an empty cart
            When I add a carton of milk for $2.50
            And I add another carton of milk for $2.50 
            Then the first item is a carton of milk
            And the price is $5.00 And the cart has 2 items
            And the total cost with 10% taxes is $5.50</code></pre>
<ol start="5">
<li>Add a third scenario that shows what happens when we combine a carton of milk and a frozen pizza:</li>
</ol>
<pre><code class="lang-python">    Scenario: Cart getting loaded with different items 
            Given an empty cart
            When I add a carton of milk
            And I add a frozen pizza
            Then the first item is a carton of milk
            And the second item is a frozen pizza
            And the first price is $2.50
            And the second price is $3.00
            And the total cost with no taxes is $5.50
            And the total cost with 10% taes is $6.05</code></pre>
<ol start="6">
<li>Let's run the story through Lettuce to see what the outcome is, considering we haven't linked this story to any Python code. In the following screenshot, it's impossible to discern the color of the outputs. The feature and scenario declarations are white. <kbd>Given</kbd>, <kbd>When</kbd>, and <kbd>Then</kbd> are undefined and colored yellow. This shows that we haven't tied the steps to any code yet:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000149.png" class="lazyload" /></p>
<ol start="7">
<li>Create a new file in <kbd>recipe32</kbd> called <kbd>steps.py</kbd> to implement the steps needed to support <kbd>Given</kbd>.</li>
<li>Add some code to <kbd>steps.py</kbd> to implement the first <kbd>Given</kbd>:</li>
</ol>
<pre><code class="lang-python">from lettuce import *
from cart import *
@step("an empty cart")
def an_empty_cart(step):
   world.cart = ShoppingCart()</code></pre>
<ol start="9">
<li>To run the steps, we need to make sure the current path that contains the <kbd>cart.py</kbd> module is part of our <kbd>PYTHONPATH</kbd>.</li>
</ol>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>For Linux and Mac OSX systems, type <kbd>export PYTHONPATH=/path/to/ cart.py</kbd>. For Windows, go to Control Panel | System | Advanced, click Environment Variables, and either edit the existing <kbd>PYTHONPATH</kbd> variable or add a new one, pointing to the folder that contains <kbd>cart.py</kbd>.</p>
</div>
</div>
<ol start="10">
<li>Run the stories again. It's hard to see in the following screenshot, but <kbd>Given an empty cart</kbd> is now green:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000097.png" class="lazyload" /></p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>While this screenshot only focuses on the first scenario, all three scenarios have the same <kbd>Given</kbd>. The code we wrote satisfied all three <kbd>Given.</kbd></p>
</div>
</div>
<ol start="11">
<li>Add code to <kbd>steps.py</kbd> that implements support for the first scenario's <kbd>Then</kbd>:</li>
</ol>
<pre><code class="lang-python">@step("looking up the fifth item causes an error") 
def looking_up_fifth_item(step):
    try:
      world.cart.item(5)
      raise AssertionError("Expected IndexError") 
    except IndexError, e:
      pass
@step("looking up a negative price causes an error")
    def looking_up_negative_price(step):
        try:
          world.cart.price(-2)
             raise AssertionError("Expected IndexError")
        except IndexError, e:
          pass
@step("the price with no taxes is (.*)")
    def price_with_no_taxes(step, total):
       assert world.cart.total(0.0) == float(total)
@step("the price with taxes is (.*)")
    def price_with_taxes(step, total):
        assert world.cart.total(10.0) == float(total)

</code></pre>
<ol start="12">
<li>Run the stories again and notice how the first scenario is completely passing, as shown in the following screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000032.png" class="lazyload" /></p>
<ol start="13">
<li>Now add code to <kbd>steps.py</kbd> to implement the steps needed for the second scenario:</li>
</ol>
<pre><code class="lang-python">@step("I add a carton of milk for (.*)")
def add_a_carton_of_milk(step, price):
    world.cart.add("carton of milk", float(price))
@step("I add another carton of milk for (.*)")
def add_another_carton_of_milk(step, price):
    world.cart.add("carton of milk", float(price))
@step("the first item is a carton of milk")
def check_first_item(step):
    assert world.cart.item(1) == "carton of milk"
@step("the price is (.*)")
def check_first_price(step, price):
    assert world.cart.price(1) == float(price)
@step("the cart has (.*) items")
def check_size_of_cart(step, num_items): 
    assert len(world.cart) == float(num_items)
@step("the total cost with (.*)% taxes is (.*)")
def check_total_cost(step, tax_rate, total):
    assert world.cart.total(float(tax_rate))==float(total)</code></pre>
<ol start="14">
<li>Finally, add code to <kbd>steps.py</kbd> to implement the steps needed for the last scenario:</li>
</ol>
<pre><code class="lang-python">@step("I add a carton of milk")
def add_a_carton_of_milk(step):
    world.cart.add("carton of milk", 2.50)
@step("I add a frozen pizza")
def add_a_frozen_pizza(step):
    world.cart.add("frozen pizza", 3.00)
@step("the second item is a frozen pizza")
def check_the_second_item(step):
    assert world.cart.item(2) == "frozen pizza"
@step("the first price is (.*)")
def check_the_first_price(step, price):
   assert world.cart.price(1) == float(price)
@step("the second price is (.*)")
def check_the_second_price(step, price): 
    assert world.cart.price(2) == float(price)
@step("the total cost with no taxes is (.*)")
def check_total_cost_with_no_taxes(step, total):
    assert world.cart.total(0.0) == float(total)
@step("the total cost with (.*)% taxes is (.*)")
def check_total_cost_with_taxes(step, tax_rate, total):
    assert round(world.cart.total(float(tax_rate)),2) == float(total)</code></pre>
<ol start="15">
<li>Run the story by typing <kbd>lettuce recipe32</kbd> and see how they are all now passing. In the next screenshot, we have all the tests passing and everything is green:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000003.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Lettuce uses the popular <kbd>Given</kbd>/<kbd>When</kbd>/<kbd>Then</kbd> style of BDD story telling.</p>
<ul>
<li><strong>Givens</strong>: This involves setting up a scenario. This often includes creating objects. For each of our scenarios, we created an instance of the <kbd>ShoppingCart</kbd>. This is very similar to unittest's setup method.</li>
<li><strong>Thens</strong>: This acts on <kbd>Given</kbd>. These are the operations we want to exercise in a scenario. We can exercise more than one <kbd>Then</kbd>.</li>
<li><strong>Whens</strong>: This involves testing the final results of <kbd>Then</kbd>. In our code, we mostly used Python asserts. In a couple of cases, where we needed to detect an exception, we wrapped the call with a <kbd>try-catch</kbd> block with a <kbd>throw</kbd> if the expected exception didn't occur.</li>
</ul>
<p>It doesn't matter in what order we put the <kbd>Given</kbd>/<kbd>Then</kbd>/<kbd>When</kbd>. Lettuce will record everything so that all the Givens are listed first, followed by all the <kbd>When</kbd> conditions, and then all the <kbd>Then</kbd> conditions. Lettuce puts on the final polish by translating successive <kbd>Given</kbd>/<kbd>When</kbd>/<kbd>Then</kbd> conditions into <kbd>And</kbd> for better readability.</p>
<h3>There's more...</h3>
<p>If you look closely at some of the steps, you will notice some wildcards:</p>
<pre><code class="lang-python">@step("the total cost with (.*)% taxes is (.*)")
def check_total_cost(step, tax_rate, total):
   assert world.cart.total(float(tax_rate)) == float(total)

</code></pre>
<p>The <kbd>@step</kbd> string lets us dynamically grab parts of the string as variables by using pattern matchers:</p>
<ul>
<li>The first <kbd>(.*)</kbd> is a pattern to capture&nbsp;<kbd>tax_rate</kbd></li>
<li>The second <kbd>(.*)</kbd> is a pattern to capture&nbsp;<kbd>total</kbd></li>
</ul>
<p>The method definition shows these two extra variables added in. We can name them anything we want. This gives us the ability to actually drive the tests, data and all, from <kbd>recipe32.feature</kbd> and only use <kbd>steps.py</kbd> to link things together in a generalized way.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It's important to point out that actual values stored in <kbd>tax_rate</kbd> and <kbd>total</kbd> are Unicode strings. Because the test involves floating point numbers, we have to convert the variables or the <kbd>assert</kbd> fails.</p>
</div>
</div>
<h3>How complex should a story be?</h3>
<p>In this recipe, we fit everything into a single story. Our story involved all the various shopping cart operations. As we write more scenarios, we may expand this into multiple stories. This goes back to the concept discussed in the <em>Breaking</em> <em>down</em> <em>obscure</em> <em>tests</em> <em>into</em> <em>simple</em> <em>ones</em> section of Chapter 1,&nbsp;<em>Using Unittest to Develop Basic Tests</em>. If we overload a single scenario with too many steps, it may get too complex. It is better if we can visualize a single thread of execution that is easy to verify at the end.</p>
<h4>Don't mix wiring code with application code</h4>
<p>The project's website shows a sample building a factorial function. It has both the factorial function as well as the wiring in a single file. For demo purposes this is alright. But for actual production use, it is best to decouple the application from the Lettuce wiring. This encourages a clean interface and demonstrates usability.</p>
<h4>Lettuce works great using folders</h4>
<p>Lettuce, by default, will look for a <kbd>features</kbd> folder wherever we run it, and discover any files ending in <kbd>.feature</kbd>. That way it can automatically find all of our stories and run them.</p>
<p>It is possible to override the features directory with <kbd>-s</kbd> or <kbd>&ndash;-scenarios</kbd>.</p>
<h3>See also</h3>
<p>The <em>Breaking</em> <em>down</em> <em>obscure</em> <em>tests</em> <em>into</em> <em>simple</em> <em>ones</em> section from Chapter 1,&nbsp;<em>Using Unittest to Develop Basic Tests</em>.</p>
<h2>Using Should DSL to write succinct assertions with Lettuce</h2>
<p>Lettuce (<a href="http://lettuce.it">http://lettuce.it</a>) is a BDD tool built for Python.</p>
<p><strong>Should DSL</strong> (<a href="http://www.should-dsl.info">http://www.should-dsl.info</a>) provides a simpler way to write assertions for <kbd>Then</kbd> conditions.</p>
<p>This recipe shows how to install Lettuce and Should DSL. Then, we will write a test story. Finally, we will wire it into our shopping cart application using Should DSL to exercise our code.</p>
<h3>Getting ready</h3>
<p>For this recipe, we will be using the shopping cart application shown at the beginning of this chapter. We also need to install Lettuce and its dependencies by doing the following:</p>
<ul>
<li>Install Lettuce by typing <kbd>pip install lettuce</kbd></li>
<li>Install Should DSL by typing <kbd>pip install should_dsl</kbd></li>
</ul>
<h3>How to do it...</h3>
<p>With the following steps, we will use Should DSL to write more succinct assertions in our test stories:</p>
<ol>
<li>Create a new directory called <kbd>recipe33</kbd> to contain all the files for this recipe.</li>
<li>Create a new file in&nbsp;<kbd>recipe33</kbd> called <kbd>recipe33.feature</kbd> to contain our test scenarios.</li>
<li>Create a story in&nbsp;<kbd>recipe33.feature</kbd> with several scenarios to exercise our shopping cart, as follows:</li>
</ol>
<pre><code class="lang-python">Feature: Shopping cart
  As a shopper
  I want to load up items in my cart
  So that I can check out and pay for them
     Scenario: Empty cart
        Given an empty cart
        Then looking up the fifth item causes an error
        And looking up a negative price causes an error
        And the price with no taxes is 0.0
        And the price with taxes is 0.0
     Scenario: Cart getting loaded with multiple of the same
        Given an empty cart
        When I add a carton of milk for 2.50
        And I add another carton of milk for 2.50
        Then the first item is a carton of milk
        And the price is 5.00
        And the cart has 2 items
        And the total cost with 10% taxes is 5.50
     Scenario: Cart getting loaded with different items
        Given an empty cart
        When I add a carton of milk
        And I add a frozen pizza
        Then the first item is a carton of milk
        And the second item is a frozen pizza 
        And the first price is 2.50
        And the second price is 3.00
        And the total cost with no taxes is 5.50
        And the total cost with 10% taxes is 6.05</code></pre>
<ol start="4">
<li>Write a set of assertions using Should DSL, as follows:</li>
</ol>
<pre><code class="lang-python">from lettuce import *
from should_dsl import should, should_not
from cart import *
@step("an empty cart")
def an_empty_cart(step):
    world.cart = ShoppingCart()
@step("looking up the fifth item causes an error")
def looking_up_fifth_item(step):
   (world.cart.item, 5) |should| throw(IndexError)
@step("looking up a negative price causes an error")
def looking_up_negative_price(step):
   (world.cart.price, -2) |should| throw(IndexError)
@step("the price with no taxes is (.*)")
def price_with_no_taxes(step, total):
   world.cart.total(0.0) |should| equal_to(float(total))
@step("the price with taxes is (.*)")
def price_with_taxes(step, total):
   world.cart.total(10.0) |should| equal_to(float(total))
@step("I add a carton of milk for 2.50")
def add_a_carton_of_milk(step):
   world.cart.add("carton of milk", 2.50)
@step("I add another carton of milk for 2.50")
def add_another_carton_of_milk(step):
   world.cart.add("carton of milk", 2.50)
@step("the first item is a carton of milk")
def check_first_item(step):
   world.cart.item(1) |should| equal_to("carton of milk")
@step("the price is 5.00")
def check_first_price(step):
   world.cart.price(1) |should| equal_to(5.0)
@step("the cart has 2 items")
def check_size_of_cart(step):
   len(world.cart) |should| equal_to(2)
@step("the total cost with 10% taxes is 5.50")
def check_total_cost(step):
   world.cart.total(10.0) |should| equal_to(5.5)
@step("I add a carton of milk")
def add_a_carton_of_milk(step):
   world.cart.add("carton of milk", 2.50)
@step("I add a frozen pizza")
def add_a_frozen_pizza(step):
   world.cart.add("frozen pizza", 3.00)
@step("the second item is a frozen pizza")
def check_the_second_item(step):
   world.cart.item(2) |should| equal_to("frozen pizza")
@step("the first price is 2.50")
def check_the_first_price(step):
   world.cart.price(1) |should| equal_to(2.5)
@step("the second price is 3.00")
def check_the_second_price(step):
   world.cart.price(2) |should| equal_to(3.0)
@step("the total cost with no taxes is 5.50")
def check_total_cost_with_no_taxes(step):
   world.cart.total(0.0) |should| equal_to(5.5)
@step("the total cost with 10% taxes is (.*)")
def check_total_cost_with_taxes(step, total):
   world.cart.total(10.0) |should| close_to(float(total),\
delta=0.1)</code></pre>
<ol start="5">
<li>Run the story:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000011.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The previous recipe (<em>Writing</em> <em>a</em> <em>testable</em> <em>story</em> <em>with</em> <em>Lettuce</em>) shows more details on how Lettuce works. This recipe demonstrates how to use Should DSL to make useful assertions.</p>
<p>Why do we need Should DSL? The simplest checks we write involve testing values to confirm the behavior of the shopping cart application. In the previous recipe, we mostly used Python assertions such as:</p>
<pre><code class="lang-python">assert len(context.cart) == 2</code></pre>
<p>This is pretty easy to understand. Should DSL offers a simple alternative, which is this:</p>
<pre><code class="lang-python">len(context.cart) |should| equal_to(2)</code></pre>
<p>Does this look like much of a difference? Some say yes, others say no. It is wordier, and for some this is easier to read. For others, it isn't.</p>
<p>So why are we visiting this? Because Should DSL has more than just <kbd>equal_to</kbd>. There are many more commands, such as these:</p>
<ul>
<li><kbd>be</kbd>: Checks identity</li>
<li><kbd>contain, include, be_into</kbd>: Verifies whether an object is contained or contains another</li>
<li><kbd>be_kind_of</kbd>: Checks types</li>
<li><kbd>be_like</kbd>: Checks using a regular expression</li>
<li><kbd>be_thrown_by,throws</kbd>: Checks that an exception is thrown</li>
<li><kbd>close_to</kbd>: Checks whether a&nbsp;value is close, given a delta</li>
<li><kbd>end_with</kbd>: Checks whether a string ends with a given suffix</li>
<li><kbd>equal_to</kbd>: Checks value equality</li>
<li><kbd>respond_to</kbd>: Checks whether an object has a given attribute or method</li>
<li><kbd>start_with</kbd>: Checks whether a string starts with a given prefix</li>
</ul>
<p>There are other alternatives as well, but this provides a diverse set of comparisons. If we imagine the code needed to write assertions that check the same things, then things get more complex.</p>
<p>For example, let's think about confirming expected exceptions. In the previous recipe, we needed to confirm that an <kbd>IndexError</kbd> is thrown when accessing an item outside the boundaries of our cart. A simple Python <kbd>assert</kbd> didn't work, so instead we coded this pattern:</p>
<pre><code class="lang-python">try:
  world.cart.price(-2)
  raise AssertionError("Expected an IndexError") 
except IndexError, e:
   pass</code></pre>
<p>This is clunky and ugly. Now, imagine a more complex, more realistic system, and the idea of having to use this pattern for lots of test situations where we want to verify that a proper exception is thrown. This can quickly become an expensive coding task.</p>
<p>Thankfully, Should DSL turns this pattern of exception assertion into a one-liner:</p>
<pre><code class="lang-python">(world.cart.price, -2) |should| throw(IndexError)</code></pre>
<p>This is clear and concise. We can instantly understand that invoking this method with these arguments should throw a certain exception. If no exception is raised, or a different one is raised, it will fail and give us clear feedback.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>If you notice, Should DSL requires the method call to be split up into a tuple, with the first element of the tuple being the method handle, and the rest being the arguments for the method.</p>
</div>
</div>
<h3>There's more...</h3>
<p>In the sample code listed in this chapter, we used <kbd>|should|</kbd>. But Should DSL also comes with <kbd>|should_not|</kbd>. Sometimes, the condition we want to express is best captured with a <kbd>|should_not|</kbd>. Combined with all the matchers listed earlier, we have a plethora of opportunities to test things, positive or negative.</p>
<p>But, don't forget, we can still use Python's plain old <kbd>assert</kbd> if it is easier to read. The idea is to have plenty of ways to express the same verification of behavior.</p>
<h3>See also</h3>
<ul>
<li><em>Writing a testable story with Lettuce.</em></li>
</ul>
<h2>Updating the project-level script to run this chapter's BDD tests</h2>
<p>In this chapter, we have developed several tactics to write and exercise BDD tests. This should help us in developing new projects. An invaluable tool for any project is having a top-level script used to manage things such as packaging, bundling, and testing.</p>
<p>This recipe shows how to create a command-line project script that will run all the tests we created in this chapter using the various runners.</p>
<h3>Getting ready</h3>
<p>For this recipe, we need to have coded all the recipes from this chapter.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will create a project-level script that will run all the test recipes from this chapter:</p>
<ol>
<li>Create a new file called <kbd>recipe34.py</kbd>.</li>
<li>Add code that uses the <kbd>getopt</kbd> library for parsing command-line arguments, as shown here:</li>
</ol>
<pre><code class="lang-python">import getopt
import logging 
import nose 
import os 
import os.path 
import re 
import sys 
import lettuce 
import doctest
from glob import glob
def usage(): 
    print()
    print("Usage: python recipe34.py [command]" 
    print()
    print "\t--help" 
    print "\t--test" 
    print "\t--package" 
    print "\t--publish" 
    print "\t--register" 
    print()
    try:
      optlist, args = getopt.getopt(sys.argv[1:], 
               "h",
              ["help", "test", "package", "publish", "register"]) 
   except getopt.GetoptError:
       # print help information and exit:
       print "Invalid command found in %s" % sys.argv 
       usage()
       sys.exit(2)</code></pre>
<ol start="3">
<li>Add a test function that uses our custom <kbd>nose</kbd> plugin,&nbsp;<kbd>BddPrinter</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">def test_with_bdd():
    from recipe26_plugin import BddPrinter
    suite = ["recipe26", "recipe30", "recipe31"] 
    print("Running suite %s" % suite)
    args = [""] 
    args.extend(suite) 
    args.extend(["--with-bdd"])
    nose.run(argv=args, plugins=[BddPrinter()])</code></pre>
<ol start="4">
<li>Add a test function that exercises file-based <kbd>doctest</kbd>:</li>
</ol>
<pre><code class="lang-python">def test_plain_old_doctest():
   for extension in ["doctest", "txt"]:
       for doc in glob("recipe27*.%s" % extension): 
           print("Testing %s" % doc) 
           doctest.testfile(doc)</code></pre>
<ol start="5">
<li>Add a test function that exercises multiple <kbd>doctest</kbd> using a customized <kbd>doctest</kbd> runner:</li>
</ol>
<pre><code class="lang-python">def test_customized_doctests():
    def test_customized_doctests():
    from recipe28 import BddDocTestRunner
    old_doctest_runner = doctest.DocTestRunner 
    doctest.DocTestRunner = BddDocTestRunner
    for recipe in ["recipe28", "recipe29"]:
        for file in glob("%s*.doctest" % recipe): 
            given = file[len("%s_" % recipe):] 
            given = given[:-len(".doctest")] 
            given = " ".join(given.split("_"))
            print("===================================") 
            print("%s: Given a %s..." % (recipe, given)) 
            print( "===================================") 
            doctest.testfile(file)
            print()
    doctest.DocTestRunner = old_doctest_runner</code></pre>
<ol start="6">
<li>Add a test function that exercises Lettuce tests:</li>
</ol>
<pre><code class="lang-python">def test_lettuce_scenarios():
    print("Running suite recipe32")
    lettuce.Runner(os.path.abspath("recipe32"), verbosity=3).run()
    print()
    print("Running suite recipe33") 
    lettuce.Runner(os.path.abspath("recipe33"), verbosity=3).run() 
    print()</code></pre>
<ol start="7">
<li>Add a top-level test function that runs all of our test functions and can be wired to the command-line option:</li>
</ol>
<pre><code class="lang-python">def test():
    def test(): 
        test_with_bdd()
        test_plain_old_doctest() 
        test_customized_doctests() 
        test_lettuce_scenarios()</code></pre>
<ol start="8">
<li>Add some extra stub functions that represent packaging, publishing, and registration options:</li>
</ol>
<pre><code class="lang-python">def package():
    print "This is where we can plug in code to run " + \ 
          "setup.py to generate a bundle."
def publish():
    print "This is where we can plug in code to upload " + \ 
          "our tarball to S3 or some other download site."
def register():
    print "setup.py has a built in function to " + \ 
          "'register' a release to PyPI. It's " + \ 
          "convenient to put a hook in here."
    # os.system("%s setup.py register" % sys.executable)</code></pre>
<ol start="9">
<li>Add code to parse the command-line options:</li>
</ol>
<pre><code class="lang-python">if len(optlist) == 0:
   usage()
   sys.exit(1)
# Check for help requests, which cause all other
# options to be ignored.
for option in optlist:
   if option[0] in ("--help", "-h"):
      usage()
      sys.exit(1)
# Parse the arguments, in order
for option in optlist:
   if option[0] in ("--test"):
      test()
   if option[0] in ("--package"):
      package()
   if option[0] in ("--publish"):
      publish()
   if option[0] in ("--register"):
      registe</code></pre>
<ol start="10">
<li>Run the script with no options:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000064.png" class="lazyload" /></p>
<ol start="11">
<li>Run the script with <kbd>&ndash;test</kbd>:</li>
</ol>
<pre><code class="lang-python">(ptc)gturnquist-mbp:04 gturnquist$ python recipe34.py --test Running suite ['recipe26', 'recipe30', 'recipe31']
...
  Scenario: Cart getting loaded with different items        #
recipe33/recipe33.feature:22
     Given an empty cart                                    #
recipe33/steps.py:6
     When I add a carton of milk                            #
recipe33/steps.py:50
     And I add a frozen pizza                               #
recipe33/steps.py:54
     Then the first item is a carton of milk                #
recipe33/steps.py:34
     And the second item is a frozen pizza                  #
recipe33/steps.py:58
     And the first price is 2.50                            #
recipe32/steps.py:69
     And the second price is 3.00                           #
recipe33/steps.py:66
     And the total cost with no taxes is 5.50               #
recipe33/steps.py:70
     And the total cost with 10% taxes is 6.05              #
recipe33/steps.py:74
1 feature (1 passed)
3 scenarios (3 passed)
21 steps (21 passed)
</code></pre>
<ol start="12">
<li>Run the script using <kbd>--package --publish --register</kbd>. Take a look at this screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000053.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This script uses Python's <kbd>getopt</kbd> library.</p>
<h3>See also</h3>
<p>For more details about how and why to use <kbd>getopt</kbd>, reasons to write a project-level script, and why we are using <kbd>getopt</kbd> instead of <kbd>optparse</kbd>.</p>

</div>


<!--Chapter 5-->


<div class="chapter" data-chapter-number="5">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 5 </span></div>
<h1 class="chaptertitle">High-Level Customer Scenarios with Acceptance Testing</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>

<p>In this chapter, we will cover the following:</p>
<ul>
<li>Installing Pyccuracy</li>
<li>Testing the basics with Pyccuracy</li>
<li>Using Pyccuracy to verify web app security</li>
<li>Installing Robot Framework</li>
<li>Creating a data-driven test suite with Robot Framework</li>
<li>Writing a testable story using Robot Framework</li>
<li>Tagging Robot Framework tests and running a subset</li>
<li>Testing web basics with Robot Framework</li>
<li>Using Robot Framework to verify web app security</li>
<li>Creating a project-level script to run this chapter's acceptance tests</li>
</ul>
<h2>Introduction</h2>
<p>Acceptance testing involves writing tests to prove that our code is, well, acceptable! However, what does this mean? The context implies acceptable from a customer's perspective. Customers are usually more interested in what the software does, not how it does it. This means that tests are aimed at inputs and outputs and tend to be at a higher level than unit testing. This has sometimes been called black box testing and is usually more system oriented. At the end of the day, it is often associated with testing that asserts whether or not the customer will accept the software.</p>
<p>There is an assumption among some developers that acceptance testing involves verifying the frontend of web applications. In fact, several testing tools, including Pyccuracy, are built on the sole premise of testing web applications. When viewed from the perspective of whether or not a customer will accept the software, this will quite literally fit into acceptable from a customer's perspective.</p>
<p>However, web testing isn't the only form of acceptance testing. Not all systems are web-based. If a subsystem is to be built by one team and handed off to another team that plans to build another layer on top of it, an acceptance test may be required before the second team will accept it.</p>
<p>For this chapter, we will dig into some recipes that involve both web and non-web application acceptance testing.</p>
<p>To create an e-store web application for testing, follow these steps:</p>
<ol>
<li>Ensure that you have <kbd>mercurial</kbd> installed on your system:
<ul>
<li>For macOS, use either MacPorts or Homebrew</li>
<li>For Ubuntu/Debian, use <kbd>sudo apt-get install mercurial</kbd></li>
<li>For other systems, you will need to do extra research in installing <kbd>mercurial</kbd></li>
</ul>
</li>
<li>This also requires having compilable tools installed, such as&nbsp;<kbd>gcc</kbd>:
<ul>
<li>For Ubuntu, use <kbd>sudo apt-get install build-essential</kbd></li>
<li>For other systems, you will need to do extra research in installing <kbd>gcc</kbd></li>
</ul>
</li>
<li>If you have other issues installing Satchmo in the following steps, visit the project site at <a href="http://www.satchmoproject.com">http://www.satchmoproject.com</a> and possibly their support group at <a href="http://groups.google.com/group/satchmo-users">http://groups.google.com/group/satchmo-users</a>.</li>
<li>Install Satchmo, an e-commerce website builder, by typing the following commands:</li>
</ol>
<pre><code class="lang-python">pip install -r http://bitbucket.org/gturnquist/satchmo/raw/tip/scripts/requirements.txt
pip install -e hg+http://bitbucket.org/gturnquist/satchmo/#egg=satchmo</code></pre>
<ol start="5">
<li>Install Python's <kbd>PIL</kbd> library for image processing with <kbd>pip install PIL</kbd>.</li>
</ol>
<ol start="6">
<li>Edit <kbd>&lt;virtualenv root&gt;/lib/python2.6/site-packages/django/contrib/admin/templates/admin/login.html</kbd> to add <kbd>id="login"</kbd> to the <kbd>Log in&lt;input&gt;</kbd> tag. This allows Pyccuracy to grab the <kbd>Log in</kbd> button and click it.</li>
<li>Run the Satchmo script to create store application: <kbd>clonesatchmo.py</kbd>.</li>
<li>When prompted about creating a super-user, say <kbd>yes</kbd>.</li>
<li>When prompted, enter a <kbd>username</kbd>.</li>
<li>&nbsp;When prompted, enter an <kbd>e-mail address</kbd>.</li>
<li>When prompted, enter a <kbd>password</kbd>.</li>
<li>Go into store directory: <kbd>cd store</kbd>.</li>
<li>Start up the store app: <kbd>python manage.py runserver</kbd>.</li>
</ol>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>If you have issues installing Satchmo with these steps, visit the project site at <a href="http://www.satchmoproject.com">http://www.satchmoproject.com</a> and possibly their support group at <a href="https://groups.google.com/forum/#!forum/satchmo-users">https://groups.google.com/forum/#!forum/satchmo-users</a>.</p>
</div>
</div>
<p>To create a non-web shopping cart application for testing, create <kbd>cart.py</kbd> with the following code:</p>
<pre><code class="lang-python">class ShoppingCart(object): 
    def __init__(self): 
        self.items = [] 
 
    def add(self, item, price): 
        for cart_item in self.items: 
            # Since we found the item, we increment 
            # instead of append 
            if cart_item.item == item: 
                cart_item.q += 1 
                return self 
 
        # If we didn't find, then we append 
        self.items.append(Item(item, price)) 
        return self 
 
    def item(self, index): 
        return self.items[index-1].item 
 
    def price(self, index): 
        return self.items[index-1].price * self.items[index-1].q 
 
    def total(self, sales_tax): 
        sum_price = sum([item.price*item.q for item in self.items]) 
        return sum_price*(1.0 + sales_tax/100.0) 
 
    def __len__(self): 
        return sum([item.q for item in self.items]) 
 
class Item(object): 
    def __init__(self, item, price, q=1): 
        self.item = item 
        self.price = price 
        self.q = q </code></pre>
<p>This shopping cart has the following characteristics:</p>
<ul>
<li>Is 1-based, meaning that the first item and price are at <kbd>[1]</kbd> not <kbd>[0]</kbd></li>
<li>Includes the ability to have multiples of the same item</li>
<li>Will calculate total price and then add taxes</li>
</ul>
<p>This application isn't complex. Maybe it doesn't look exactly at a system level, but it does provide an easy application to write acceptance tests against.</p>
<h2>Installing Pyccuracy</h2>
<p>Pyccuracy is a useful tool for writing web acceptance tests using a BDD-style language. This recipe shows all the steps needed to install it and set it up for later recipes.</p>
<h3>How to do it...</h3>
<p>With these steps, we will install Pyccuracy and all the tools needed to run the scenarios later in this chapter:</p>
<ol>
<li>Install <kbd>Pyccuracy</kbd> by typing <kbd>pip install pyccuracy</kbd>.</li>
<li>Download <kbd>selenium-server.jar</kbd> from <a href="http://github.com/heynemann/pyccuracy/raw/master/lib/selenium-server.jar">http://github.com/heynemann/pyccuracy/raw/master/lib/selenium-server.jar</a>.</li>
<li>Start it up by typing <kbd>java -jar selenium-server.jar</kbd>. Note that if you don't have Java installed, you definitely need to download and install it as well.</li>
<li>Install <kbd>lxml</kbd> by typing <kbd>pip install lxml</kbd>.</li>
</ol>
<ol start="5">
<li>Create a simple test file called <kbd>recipe35.acc</kbd> and enter the following code:</li>
</ol>
<pre><code class="lang-python">As a Yahoo User
I want to search Yahoo
So that I can test my installation of Pyccuracy

Scenario 1 - Searching for Python Testing Cookbook
Given
    I go to "http://yahoo.com"
When
    I fill "p" textbox with "Python Testing Cookbook"
    And I click "search-submit" button and wait
Then
    I see "Python Testing Cookbook - Yahoo! Search Results" title</code></pre>
<ol start="6">
<li>Run it by typing <kbd>pyccuracy_console -p test.acc</kbd>. The following screenshot shows it being run with Firefox (default for this system):</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000043.png" class="lazyload" /></p>
<ol start="7">
<li>Run it again, using a different web browser such as Safari, by typing&nbsp;<kbd>pyccuracy_console -p test.acc -b safari</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000033.png" class="lazyload" /></p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>At the time of writing,&nbsp;Selenium&nbsp;supports Firefox, Safari, Opera, and IE 7+, but not Chrome.</p>
</div>
</div>
<ol start="8">
<li>In the folder where we ran the test, there should now be a <kbd>report.html</kbd> file. Open it up using a browser to view the results. Then, click on Expand All:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000022.png" class="lazyload" /></p>
<p>&nbsp;</p>
<h3>How it works...</h3>
<p>Pyccuracy uses Selenium, a popular browser-driving application tester to run its scenarios. Pyccuracy provides an out-of-the-box <strong>Domain Specific Language</strong> (<strong>DSL</strong>) to write tests. The DSL provides the means to send commands to a test browser and also check the results, verifying web application behavior.</p>
<p>Later on in this chapter, there are several recipes that show more details of Pyccuracy.</p>
<h3>See also</h3>
<ul>
<li><em>Testing the basics with Pyccuracy</em></li>
<li><em>Using Pyccuracy to verify web app security</em></li>
</ul>
<h2>Testing the basics with Pyccuracy</h2>
<p>Pyccuracy provides an easy-to-read set of operations to drive the frontend of a web application. This recipe shows how to use it to drive a shopping cart application and verify application functionality.</p>
<h3>Getting ready</h3>
<ol>
<li>If it isn't already running, start up the Selenium server in another shell or window by typing <kbd>java -jar selenium-server.jar</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000012.png" class="lazyload" /></p>
<ol start="2">
<li>If the Satchmo store application isn't already running, start it up in another shell or window by typing python manage.py runserver.</li>
</ol>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This must run inside the <kbd>virtualenv</kbd> environment.</p>
</div>
</div>
<h3>How to do it...</h3>
<p>With these steps, we will explore the basics of writing a Pyccuracy test:</p>
<ol>
<li>Create a new file called <kbd>recipe36.acc</kbd>.</li>
<li>Create a story for loading items into the shopping cart:</li>
</ol>
<pre><code class="lang-python">As a store customer
I want to put things into my cart
So that I can verify the store's functionality.</code></pre>
<ol start="3">
<li>Add a scenario where the empty cart is looked at in detail, with a confirmed balance of <kbd>$0.00</kbd>:</li>
</ol>
<pre><code class="lang-python">Scenario 1 - Inspect empty cart in detail
Given
I go to "http://localhost:8000"
When
I click "Cart" link and wait
Then
I see that current page contains "Your cart is empty"
And I see that current page contains "0 - $0.00"</code></pre>
<ol start="4">
<li>Add another scenario where a book is selected and two of them are added to the cart:</li>
</ol>
<pre><code class="lang-python">Scenario 2 - Load up a cart with 2 of the same
Given
I go to "http://localhost:8000"
When
I click "Science Fiction" link
And I click "Robots Attack!" link and wait
And I fill "quantity" textbox with "2"
And I click "addcart" button and wait
And I click "Cart" link and wait
Then
I see that current page contains "Robots Attack!"
And I see "quantity" textbox contains "2"
And I see that current page contains "&lt;td align="center"&gt;$7.99&lt;/td&gt;"
And I see that current page contains "&lt;td align="center"&gt;$15.98&lt;/td&gt;"
And I see that current page contains "&lt;td&gt;$15.98&lt;/td&gt;"</code></pre>
<ol start="5">
<li>Run the story by typing <kbd>pyccuracy_console -p recipe36.acc</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000159.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Pyccuracy has a lot of built-in actions based on driving the browser or reading the page. These actions are patterns used to parse the story file and generate commands sent to the Selenium server, which in turn drives the browser and then reads the results of the page.</p>
<p>The key is picking the right text to identify the element being actioned or read.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Web apps that are missing ID tags are much hard to read.</p>
</div>
</div>
<h3>There's more...</h3>
<p>The key is picking the right identifier and element type. With good identifiers, it is easy to do things like&mdash;<em>I&nbsp;</em><em>click&nbsp;</em><em>on&nbsp;</em><em>Cart&nbsp;</em><em>link</em>. Did you note the issue we had with drilling into the shopping cart table? The HTML <kbd>&lt;table&gt;</kbd> tag had no identifier, which made it impossible for us to pick. Instead, we had to look at the whole page and do a global search for some markup.</p>
<p>This makes it more difficult to read the test. A good solution is to alter the web app to include an ID in the <kbd>&lt;table&gt;</kbd> tag. Then, we narrow down our acceptance criteria to just the table. With this application it was okay, but with complex web applications, it will surely be much more difficult to find the exact bit of text we are looking for without good IDs.</p>
<p>This raises an interesting question&mdash;should&nbsp;an&nbsp;application&nbsp;be&nbsp;amended&nbsp;to&nbsp;support&nbsp;a&nbsp;test? Simply put, yes. It isn't a major upheaval to add some good identifiers to key HTML elements to support testing. It didn't involve major design changes to the application. The net result was easier to read test cases and better-automated testing.</p>
<p>This begs another question&mdash;what if making the application more testable did involve major design changes?&nbsp;This can be viewed as a major interruption in work. Alternatively, maybe it's a strong hint that our design has components that are too tightly coupled or not cohesive enough.</p>
<p>In software development, <strong>coupling</strong> and <strong>cohesiveness</strong> are subjective terms that aren't very measurable. What can be said is that applications that don't lend themselves to testing are often monolithic, hard to maintain, and probably have circular dependencies, which implies that it will be much harder for us to make changes (as developers) to meet needs without impacting the entire system.</p>
<p>Of course, all of this would be a big leap from our recipe's situation, where we simply lack an identifier for an HTML table. However, it's important to ask this question&mdash;what&nbsp;if&nbsp;we&nbsp;need&nbsp;more&nbsp;changes&nbsp;than&nbsp;something&nbsp;so&nbsp;small?</p>
<h3>See also</h3>
<p><em>Installing Pyccuracy</em></p>
<h2>Using Pyccuracy to verify web app security</h2>
<p>Applications often have login screens. Testing a secured web application requires us to capture the login process as a custom action. That way, we can reuse it repeatedly for as many scenarios as we need.</p>
<h3>Getting ready</h3>
<ol>
<li>If it isn't already running, start up the Selenium server in another shell or window by typing <kbd>java -jar selenium-server.jar</kbd>.</li>
<li>If the Satchmo store application isn't already running, start it up in another shell or window by typing python manage.py runserver.</li>
</ol>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This must run inside the <kbd>virtualenv</kbd> environment.</p>
</div>
</div>
<h3>How to do it...</h3>
<p>With the following steps, we will exercise a web application's security and then see how to extend Pyccuracy by creating a custom action that does so:</p>
<ol>
<li>Create a new file called <kbd>recipe37.acc</kbd> to put this recipe's scenario into.</li>
<li>Create a story for exercising Django's admin application:</li>
</ol>
<pre><code class="lang-python">As a system administrator, 
I want to log in to Django's admin page 
so that I can check the product catalog.</code></pre>
<ol start="3">
<li>Add a scenario that logs in to the admin application:</li>
</ol>
<pre><code class="lang-python">Scenario 1 - Logging in to the admin page
Given
    I go to "http://localhost:8000/admin"
When
    I fill "username" textbox with "gturnquist"
    And I fill "password" textbox with "password"
    And I click "login" button and wait
Then
    I see that current page contains 
    "&lt;ahref="product/product/"&gt;Products&lt;/a&gt;"</code></pre>
<ol start="4">
<li>Add a scenario that inspects the product catalog, using the custom login action:</li>
</ol>
<pre><code class="lang-python">Scenario 2 - Check product catalog
Given
    I am logged in with username "gturnquist" and password "password"
When
    I click "Products" link and wait
Then
    I see that current page contains "robot-attack"</code></pre>
<ol start="5">
<li>Create a matching file called <kbd>recipe37.py</kbd> containing a custom-defined action.</li>
<li>Code the custom action of logging in to the admin action:</li>
</ol>
<pre><code class="lang-python">from pyccuracy.actions import ActionBase
from pyccuracy.errors import *

class LoggedInAction(ActionBase):
    regex = r'(And )?I am logged in with username ["] (?P&lt;username&gt;.+)["] and password ["](?P&lt;password&gt;.+)["]$'
    def execute(self, context, username, password):
        self.execute_action(u'I go to "http://localhost:8000/
admin"', context)
    logged_in = False
    try:
        self.execute_action(
          u'And I see that current page contains "id_username"', context)
        except ActionFailedError:
            logged_in = True
        if not logged_in:
            self.execute_action(u'And I fill "username" textbox with "%s"' % username, context)
            self.execute_action(u'And I fill "password" textbox with "%s"' % password, context)
            self.execute_action(u'And I click "login" button', context)</code></pre>
<ol start="7">
<li>Run the story by typing <kbd>pyccuracy_console -p recipe37.acc</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000150.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The first scenario shows the simple steps needed to exercise the login screen. After having proven that the login screen works, it becomes cumbersome to repeat this procedure for more scenarios.</p>
<p>To handle this, we create a custom action in Python by extending <kbd>ActionBase</kbd>. Custom actions require a regular expression to define the DSL text. Next, we define an <kbd>execute</kbd> method to include a combination of application logic and Pyccuracy steps to execute. Essentially, we can define a set of steps to automatically execute actions and dynamically handle different situations.</p>
<p>In our situation, we coded it to handle whether or not the user was already logged in. With this custom action, we built the second scenario and handled logging in with a single statement, allowing us to move on and test the core part of our scenario.</p>
<h3>See also</h3>
<p><em>Installing Pyccuracy</em></p>
<h2>Installing Robot Framework</h2>
<p>Robot Framework is a useful framework for writing acceptance tests using the <strong>keyword</strong> approach. Keywords are shorthand commands that are provided by various libraries and can also be user defined. This easily supports BDD-style <kbd>Given</kbd>-<kbd>When</kbd>-<kbd>Then</kbd> keywords. It also opens the door to third-party libraries defining custom keywords to integrate with other test tools, such as Selenium. It also means that acceptance tests written using Robot Framework aren't confined to web applications.</p>
<p>This recipe shows all the steps needed to install Robot Framework as well as the third-party Robot Framework Selenium library for use in later recipes.</p>
<h3>How to do it...</h3>
<ol>
<li>Ensure that you activate your <kbd>virtualenv</kbd> sandbox.</li>
<li>Install by typing <kbd>easy_install robotframework</kbd>.</li>
</ol>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>At the time of writing, Robot Framework was not able to be installed using <kbd>pip</kbd>.</p>
</div>
</div>
<ol start="3">
<li>Using any type of window navigator, go to <kbd>&lt;virtualenvroot&gt;/build/robotframework/doc/quickstart</kbd> and open <kbd>quickstart.html</kbd> with your favorite browser. This is not only a guide but also a runnable test suite.</li>
<li>Switch to your virtualenv's build directory for Robot Framework: <kbd>cd&lt;virtualenvroot&gt;/build/robotframework/doc/quickstart</kbd>.</li>
<li>Run the Quick Start manual through <kbd>pybot</kbd> to verify installation with <kbd>pybot quickstart.html</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000138.png" class="lazyload" /></p>
<ol start="6">
<li>Inspect the <kbd>report.html</kbd>, <kbd>log.html</kbd>, and <kbd>output.xml</kbd> files generated by the test run.</li>
<li>Install the Robot Framework Selenium library to allow integration with Selenium by first downloading <a href="http://robotframework-seleniumlibrary.googlecode.com/files/robotframework-seleniumlibrary-2.5.tar.gz">http://robotframework-seleniumlibrary.googlecode.com/files/robotframework-seleniumlibrary-2.5.tar.gz</a>.</li>
</ol>
<ol start="8">
<li>Unpack the tarball.</li>
<li>Switch to the directory with <kbd>cd robotframework-seleniumlibrary-2.5</kbd>.</li>
<li>Install the package with <kbd>python setup.py install</kbd>.</li>
<li>Switch to the demo directory with <kbd>cd demo</kbd>.</li>
<li>Start up the demo web app with <kbd>python run demo.py demoapp start</kbd>.</li>
<li>Start up the Selenium server with <kbd>python run demo.py selenium start</kbd>.</li>
<li>Run the demo tests with <kbd>pybot login_tests</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000129.png" class="lazyload" /></p>
<ol start="15">
<li>Shut down the demo web app with <kbd>python run demo.py demoapp stop</kbd>.</li>
<li>Shut down the Selenium server with <kbd>python run demo.py selenium stop</kbd>.</li>
<li>Inspect the <kbd>report.html</kbd>, <kbd>log.html</kbd>, <kbd>output.xml</kbd>, and <kbd>selenium_log.txt</kbd> files generated by the test run.</li>
</ol>
<h3>There's more...</h3>
<p>With this recipe, we have installed Robot Framework and one third-party library that integrates Robot Framework with Selenium.</p>
<p>There are many more third-party libraries that provide enhanced functionality to Robot Framework. The options have enough potential to fill an entire book. So, we must narrow our focus to some of the core features provided by Robot Framework, including both web and non-web testing.</p>
<h2>Creating a data-driven test suite with Robot Framework</h2>
<p>Robot Framework uses <strong>keywords</strong> to define tests, test steps, variables, and other testing components. Keywords are shorthand commands that are provided by various libraries and can also be custom-defined. This allows many different ways of writing and organizing tests.</p>
<p>In this recipe, we'll explore how to run the same test procedure with varying inputs and outputs. These can be described as data-driven tests.</p>
<h3>Getting ready</h3>
<ol>
<li>We first need to activate our <kbd>virtualenv</kbd> setup</li>
<li>For this recipe, we will use the shopping cart application</li>
<li>Next, we need to install Robot Framework, as shown in the previous recipe</li>
</ol>
<h3>How to do it...</h3>
<p>The following steps will show us how to write a simple acceptance test using HTML tables:</p>
<ol>
<li>Create a new file called <kbd>recipe39.html</kbd> to capture the tests and configurations.</li>
<li>Add an HTML paragraph and table that contains a set of data-driven test cases, as shown in the following browser screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000120.png" class="lazyload" /></p>
<ol start="3">
<li>Add another HTML paragraph and table defining the custom keywords Adding&nbsp;items&nbsp;to&nbsp;cart and Add&nbsp;item:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000109.png" class="lazyload" /></p>
<ol start="4">
<li>Create a new file called <kbd>recipe39.py</kbd> to contain Python code that is wired into our custom keywords.</li>
<li>Create an old-style Python class that implements the custom keywords needed for the scenarios:</li>
</ol>
<pre><code class="lang-python">from cart import *

class recipe39:
    def __init__(self):
        self.cart = ShoppingCart()
    def add_item_to_cart(self, description, price):
        self.cart.add(description, float(price))
    def get_total(self, tax):
        return format(self.cart.total(float(tax)), ".2f")</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It's important to define the class <em>old-</em><em>style</em>. If we define it as <em>new-</em><em>style</em> by subclassing <kbd>object</kbd>, Robot Framework's runner, <kbd>pybot</kbd>, won't find the methods and associate them with our HTML keywords.</p>
</div>
</div>
<ol start="6">
<li>Add a third HTML paragraph and table that loads our Python code to implement Add&nbsp;item&nbsp;to&nbsp;cart and Get&nbsp;total:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000100.png" class="lazyload" /></p>
<ol start="7">
<li>View the HTML file in your favorite browser:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000089.png" class="lazyload" /></p>
<ol start="8">
<li>Run the HTML file through <kbd>pybot</kbd> to exercise the tests by typing <kbd>pybot recipe39.html</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000081.png" class="lazyload" /></p>
<ol start="9">
<li>You can inspect <kbd>report.html</kbd> and <kbd>log.html</kbd> using your favorite browser for more details about the results.</li>
</ol>
<h3>How it works...</h3>
<p>Robot Framework uses HTML tables to define test components. The header row of the table identifies what type of component the table defines.</p>
<p>The first table we created was a set of test cases. Robot Framework spots this by seeing <kbd>Test Case</kbd> in the first cell of the header row. The rest of the header cells aren't parsed, which leaves us free to put in descriptive text. In this recipe, each of our test cases is defined with one-line. The second column has <kbd>Adding items to cart</kbd> on every row, which is a custom keyword defined in the second table. The rest of the columns are arguments for this custom keywords.</p>
<p>The second table we wrote is used to define custom keywords. Robot Framework figures this out by seeing <kbd>Keyword</kbd> in the first cell of the header row. Our table defines two keywords:</p>
<ul>
<li><kbd>Adding items to cart</kbd>:
<ul>
<li>The first line defines the arguments by starting with <kbd>[Arguments]</kbd> and six input variables: <kbd>${item1}</kbd>, <kbd>${price1}</kbd>, <kbd>${item2}</kbd>, <kbd>${price2}</kbd>, <kbd>${tax}</kbd>, and <kbd>${total}</kbd></li>
<li>The next set of lines are actions</li>
<li>Lines two and three use another custom keyword: <kbd>Add&nbsp;item</kbd> with two arguments</li>
<li>Line four defines a new variable, <kbd>${calculated total}</kbd>, which is assigned the results of another keyword, <kbd>Get total</kbd>&nbsp;with one argument, <kbd>${tax},</kbd> that is defined in our Python module</li>
<li>The last line uses a built-in keyword, <kbd>Should Be Equal</kbd>, to confirm that the output of <kbd>Get total</kbd> matches the original <kbd>${total}</kbd></li>
</ul>
</li>
<li><kbd>Add item</kbd>:
<ul>
<li>The first line defines arguments by starting with <kbd>[Arguments]</kbd> and two input variables: <kbd>${description}</kbd> and <kbd>${price}</kbd>.</li>
<li>The second line uses another keyword, <kbd>Add item to cart</kbd>, that is defined in our Python module, with two named arguments: <kbd>${description}</kbd> and <kbd>${price}</kbd>.</li>
</ul>
</li>
</ul>
<p>The third table we made contains settings. This is identified by seeing <kbd>Setting</kbd> in the first cell of the header row. This table is used to import Python code that contains the final keywords using the built-in keyword <kbd>Library</kbd>.</p>
<h3>There's more...</h3>
<p>Robot Framework maps our keywords to our Python code by a very simple convention:</p>
<ul>
<li><kbd>Get total ${tax}</kbd> maps to <kbd>get_total(self,tax)</kbd></li>
<li><kbd>Add item to cart ${description} ${price}</kbd> maps to <kbd>add_item_to_cart(self, description, price)</kbd></li>
</ul>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The reason we need <kbd>add_item_to_cart</kbd>&nbsp;and couldn't have just written <kbd>add_item</kbd> to tie in to the&nbsp;<kbd>Add item</kbd>&nbsp;keyword is because Robot Framework uses named arguments when connecting to Python code. Since each usage of <kbd>Add item</kbd> in our tables had a different variable name, we needed a separate keyword with distinct arguments.</p>
</div>
</div>
<h4>Do I have to write HTML tables?</h4>
<p>Robot Framework is driven by HTML tables, but it doesn't matter how the tables are generated. Many projects use tools like <strong>reStructuredText</strong> (<a href="http://docutils.sourceforge.net/rst.html">http://docutils.sourceforge.net/rst.html</a>) to write tables in a less verbose way, and then have a parser that converts it into HTML. A useful tool for converting <kbd>.rst</kbd> to HTML is <strong>docutils</strong> (<a href="http://docutils.sourceforge.net/">http://docutils.sourceforge.net/</a>). It provides a convenient <kbd>rst2html.py</kbd> script that will convert all the <kbd>.rst</kbd> tables into HTML.</p>
<p>Unfortunately, the format of this book makes it hard to present <kbd>.rst</kbd> as either code or with a screenshot.</p>
<h4>What are the best ways to write the code that implements our custom keywords?</h4>
<p>We wrote a chunk of Python code to tie in our custom keywords with the <kbd>ShoppingCart</kbd> application. It is important to make this as light as possible. <em>Why?</em> It's because when we deploy the actual application, this bridge shouldn't be a part of it. It may be tempting to use this bridge as an opportunity to bundle things up or to transform things, but this should be avoided.</p>
<p>Instead, it is better to include these functions in the software application itself. Then, this extra functionality becomes a part of the tested, deployed software functionality.</p>
<p>If we don't invest too heavily in the bridging code, it helps us avoid making the software dependent on the test framework. For some reason, if we ever decided to switch to something other than Robot Framework, we wouldn't be tied into that particular tool due to having too much invested in the bridging code.</p>
<h4>Robot Framework variables are Unicode</h4>
<p>Another critical factor in making our Python code work is recognizing that the input values are Unicode strings. Since <kbd>ShoppingCart</kbd> is based on floating point values, we had to use Python's <kbd>float(input)</kbd> function to convert inputs, and <kbd>format(output, ".2f")</kbd> to convert outputs.</p>
<p>Does&nbsp;this&nbsp;contradict&nbsp;the&nbsp;previous&nbsp;section&nbsp;where&nbsp;we&nbsp;discussed&nbsp;keeping&nbsp;this&nbsp;bridge&nbsp;as&nbsp;light&nbsp;as&nbsp;possible? It doesn't. By using pure, built-in Python functions that have no side effects, we aren't getting in deep, and instead are only messaging the formats to line things up. If we started manipulating containers, or converting strings to lists, and vice versa, or even defining new classes, then that would definitely be getting too heavy for this bridge.</p>
<h3>See also</h3>
<p><em>Installing Robot Framework</em></p>
<h2>Writing a testable story with Robot Framework</h2>
<p>As discussed earlier in this chapter, Robot Framework lets us use custom-defined keywords.</p>
<p>This gives us the ability to structure keywords in any style. In this recipe, we will define custom keywords that implement the BDD-<kbd>Given</kbd>-<kbd>When</kbd>-<kbd>Then</kbd>-style of specification.</p>
<h3>Getting ready</h3>
<ol>
<li>We first need to activate our <kbd>virtualenv</kbd> setup.</li>
<li>For this recipe, we will use the shopping cart application.</li>
<li>Next, we need to install Robot Framework, as shown in the previous sections of this chapter.</li>
</ol>
<h3>How to do it...</h3>
<p>The following steps will explore how to write a BDD-<kbd>Given-When-Then</kbd>-style acceptance test:</p>
<ol>
<li>Create a new file called <kbd>recipe40.html</kbd> to put our HTML tables.</li>
<li>Create a story file in HTML with an opening statement:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000069.png" class="lazyload" /></p>
<ol start="3">
<li>Add a table with several scenarios used to exercise the shopping cart application with a series of <kbd>Given</kbd>-<kbd>When</kbd>-<kbd>Then</kbd> keywords:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000088.png" class="lazyload" /></p>
<ol start="4">
<li>Add a second table that defines all of our custom <kbd>Given-When-Then</kbd> custom keywords:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000057.png" class="lazyload" /></p>
<ol start="5">
<li>Create a new file called <kbd>recipe40.py</kbd> to put Python code that links the custom keywords to the <kbd>ShoppingCart</kbd> application:</li>
</ol>
<pre><code class="lang-python">from cart import *
class recipe40:
def __init__(self):
self.cart = None
def create_empty_cart(self):
self.cart = ShoppingCart()
def lookup_item(self, index):
try:
return self.cart.item(int(index))
except IndexError:
return "ERROR"
def lookup_price(self, index):
try:
return format(self.cart.price(int(index)), ".2f")
except IndexError:
return "ERROR"
def add_item(self, description, price):
self.cart.add(description, float(price))
def size_of_cart(self):
return len(self.cart)
def total(self, tax):
return format(self.cart.total(float(tax)), ".2f")</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It is critical that this class is implemented old-style. If implemented new-style by extending <kbd>object</kbd>, Robot Framework will not link the keywords.</p>
</div>
</div>
<ol start="6">
<li>Add a third table to our <kbd>recipe40.html</kbd> file to import our Python module:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000046.png" class="lazyload" /></p>
<ol start="7">
<li>Run the story by typing <kbd>pybot recipe40.html</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000035.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Robot Framework uses HTML tables to define test components. The header row of the table identifies what type of component the table defines.</p>
<p>The first table we created was a set of test cases. Robot Framework spots this by seeing <kbd>Test Case</kbd> in the first cell of the header row. The rest of the header cells aren't parsed, which leaves us free to put in descriptive text.</p>
<p>In this recipe, each of our test cases comprised several custom keywords using the <kbd>Given-When-Then</kbd>-style familiar to BDD testers. Many of these keywords have one or more arguments.</p>
<p>The second table we wrote was used to define our custom <kbd>Given-When-Then</kbd>-keywords. Robot Framework figures this out by seeing <kbd>Keyword</kbd> in the first cell of the header row.</p>
<p>The third table we made contains settings. This is identified by seeing <kbd>Setting</kbd> in the first cell of the header row. This table is used to import Python code that contains the final keywords using the built-in keyword <kbd>Library</kbd>.</p>
<p>An important aspect of our custom keywords, in this recipe, is that we wrote them in a natural, flowing language:</p>
<pre><code class="lang-python">When I add a carton of milk for 2.50 </code></pre>
<p>This is broken up into four HTML cells in order to parameterize the inputs and make the keywords reusable for several test steps:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000025.png" class="lazyload" /></p>
<p>Robot Framework sees this as a custom keyword, <kbd>When</kbd><kbd>I</kbd><kbd>add</kbd><kbd>a</kbd>, with three arguments: <kbd>carton of milk</kbd>, <kbd>for</kbd>, and <kbd>2.50</kbd>.</p>
<p>Later on, we fill in the actual steps involved with this keyword. In doing so, we are really only concerned with using <kbd>carton of milk</kbd> and <kbd>2.50</kbd>, but we still have to treat <kbd>for</kbd> like an input variable. We do this using a place holder variable, <kbd>${noop}</kbd>, which we will simply not use in any following keyword steps.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>In this recipe, we call the throwaway variable, <kbd>${noop}</kbd>. We could have called it anything. We can also reuse it if we have more than one throwaway argument in the same keyword. This is because Robot Framework doesn't engage in strong type checks.</p>
</div>
</div>
<h3>There's more...</h3>
<p>This entire chunk of HTML that we had to write starts to feel a bit heavy. As mentioned in the <em>Creating&nbsp;</em><em>a&nbsp;</em><em>data</em>-<em>driven&nbsp;</em><em>test&nbsp;</em><em>suite&nbsp;</em><em>with&nbsp;</em><em>Robot Framework</em> recipe, <kbd>.rst</kbd> is a great alternative. Unfortunately, writing this recipe using <kbd>.rst</kbd> is too wide for the format of this book. Refer to that recipe for more details about writing <kbd>.rst</kbd> and getting the tools to convert <kbd>.rst</kbd> to HTML.</p>
<h4>Given-When-Then results in duplicate rules</h4>
<p>It's true that we had to define both <kbd>Then item</kbd> and <kbd>Add item</kbd>, which are basically the same, in order to support two different test scenarios. In other BDD tools, these would have been automatically spotted as the same clause. Robot Framework doesn't directly provide a BDD domain specific language, so we had to fill this in for ourselves.</p>
<p>The most efficient way to handle this was to define <kbd>Then item</kbd> in detail with all the steps needed, and then code <kbd>And item</kbd> to just call <kbd>Then item</kbd>.</p>
<p>In contrast, <kbd>When I add a</kbd> and <kbd>And I add a</kbd> were implemented by calling <kbd>add item</kbd>. Since this clause was a simpler pass-through to our Python module, it wasn't necessary to chain them together like the previous example.</p>
<p>Another option would be to investigate coding our own BDD plugin library to simplify all of this.</p>
<h4>Do the try-except blocks violate the idea of keeping things light?</h4>
<p>In the <em>Creating&nbsp;</em><em>a&nbsp;</em><em>data</em>-<em>driven&nbsp;</em><em>test&nbsp;</em><em>suite&nbsp;</em><em>with&nbsp;Robot</em>&nbsp;<em>Framework</em> recipe, I mentioned that the code that bridges the HTML tables with the <kbd>ShoppingCart</kbd> application should be kept as light as possible and avoid transformations and other manipulations.</p>
<p>It is quite possible to view trapping of an expected exception and returning a string as crossing this line. In our case, the solution was to define a single clause that could handle errors and legitimate values. The clause takes whatever is returned and verifies it using the built-in <kbd>Should Be Equal</kbd> keyword.</p>
<p>If this wasn't the case, it may have been smoother to not have the try-expect block, and instead use the built-in <kbd>Run Keyword And Expect Error</kbd> keyword linked to another custom Python keyword. However, in this situation, I think the goal of keeping things light was satisfied.</p>
<h3>See also</h3>
<ul>
<li><em>Installing the Robot Framework</em></li>
<li><em>Creating a data-driven test suite with Robot Framework</em></li>
</ul>
<h2>Tagging Robot Framework tests and running a subset</h2>
<p>Robot Framework provides a comprehensive way to capture test scenarios using table-driven structures. This includes the ability to add metadata in the form of tagging as well as documentation.</p>
<p>Tagging allows including or excluding tags for testing. Documentation appears on the command line and also in the outcome reports. This recipe will demonstrate both of these keen features.</p>
<p>Finally, HTML tables aren't the only way to define data tables with Robot Framework. In this recipe, we will explore using double-space-separated entries. While this isn't the only non-HTML way to write stories, it is the easiest non-HTML way to demonstrate that still fits within the font size limits of this book in printed form.</p>
<h3>Getting ready</h3>
<ol>
<li>We first need to activate our <kbd>virtualenv</kbd> setup.</li>
<li>Create a new file called <kbd>cart41.py</kbd> to put an alternate version of the shopping cart application.</li>
</ol>
<ol start="3">
<li>Type in the following code that stores the cart to a database:</li>
</ol>
<pre><code class="lang-python">class ShoppingCart(object):
    def __init__(self):
        self.items = []
    def add(self, item, price):
        for cart_item in self.items:
            # Since we found the item, we increment
            # instead of append
            if cart_item.item == item:
                cart_item.q += 1
                return self
        # If we didn't find, then we append
        self.items.append(Item(item, price))
        return self
    def item(self, index):
        return self.items[index-1].item
    def price(self, index):
        return self.items[index-1].price * self.items[index-1].q
    def total(self, sales_tax):
        sum_price = sum([item.price*item.q for item in self.items])
        return sum_price*(1.0 + sales_tax/100.0)
    def store(self):
        # This simulates a DB being created.
        f = open("cart.db", "w")
        f.close()
    def retrieve(self, id):
        # This simulates a DB being read.
        f = open("cart.db")
        f.close()
    def __len__(self):
        return sum([item.q for item in self.items])
class Item(object):
    def __init__(self, item, price, q=1):
        self.item = item
        self.price = price
        self.q = q</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This version of the shopping cart has two extra methods: <kbd>store</kbd> and <kbd>retrieve</kbd>. They don't actually talk to a database, but instead create an empty&nbsp;<kbd>cart.db</kbd>&nbsp;file.&nbsp;<em>Why?</em> The purpose is to simulate interaction with a database. Later in the recipe, we will show how to tag test cases that involve this operation and easily exclude them from test runs.</p>
</div>
</div>
<ol start="4">
<li>Next, we need to install Robot Framework, as shown in the earlier sections of this chapter.</li>
</ol>
<h3>How to do it...</h3>
<p>The following steps will show how to write scenarios in a format other than HTML tables and also how to tag tests to allow picking and choosing which tests are run on the command line:</p>
<ol>
<li>Create a new file called <kbd>recipe41.txt</kbd> using plain text and space-separated entries that has a couple of test cases: a simple one and another a more complex one with documentation and tags:</li>
</ol>
<pre><code class="lang-python">***Test Cases***
Simple check of adding one item
    Given an empty cart
    When I add a carton of milk for 2.50
    Then the total with 0 % tax is 2.50
    And the total with 10 % tax is 2.75

More complex by storing cart to database
    [Documentation] This test case has special tagging, so it can be
excluded. This is in case the developer doesn't have the right database
system installed to interact properly.cart.db
    [Tags] database
    Given an empty cart
    When I add a carton of milk for 2.50
    And I add a frozen pizza for 3.50
    And I store the cart
    And I retrieve the cart
    Then there are 2 items</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It's important to note that two spaces at the minimum are required to identify breaks between one cell and the next. The line with <kbd>When I add a carton of milk for 2.50</kbd> actually has four cells of information: <kbd>| When I add a | carton of milk | for | 2.50 |</kbd>. There is actually a fifth, empty cell that prefixes this row indicated by the two-space indentation. It is necessary to mark this row as a step in test case <kbd>Simple check of adding one item</kbd> rather than another test case.</p>
</div>
</div>
<ol start="2">
<li>Add a table for custom keyword definitions using plain text and space-separated values:</li>
</ol>
<pre><code class="lang-python">***Keywords***
Given an empty cart
    create empty cart
When I add a
    [Arguments] ${description} ${noop} ${price}
    add item ${description} ${price}
And I add a
    [Arguments] ${description} ${noop} ${price}
    add item ${description} ${price}
Then the total with
    [Arguments] ${tax} ${noop} ${total}
    ${calc total}= total ${tax}
    Should Be Equal ${calc total} ${total}
And the total with
    [Arguments] ${tax} ${noop} ${total}
    Then the total with ${tax} ${noop} ${total}
And I store the cart
    Set Test Variable ${cart id} store cart
And I retrieve the cart
    retrieve cart ${cart id}
Then there are
    [Arguments] ${size} ${noop}
    ${calc size}= Size of cart
    Should Be Equal As Numbers ${calc size} ${size}</code></pre>
<ol start="3">
<li>Create a new file called <kbd>recipe41.py</kbd> that contains Python code that bridges some of the keywords with the shopping cart application:</li>
</ol>
<pre><code class="lang-python">from cart41 import *

class recipe41:
    def __init__(self):
        self.cart = None
    def create_empty_cart(self):
        self.cart = ShoppingCart()
    def lookup_item(self, index):
        try:
            return self.cart.item(int(index))
        except IndexError:
            return "ERROR"
    def lookup_price(self, index):
        try:
            return format(self.cart.price(int(index)), ".2f")
        except IndexError:
            return "ERROR"
    def add_item(self, description, price):
        self.cart.add(description, float(price))
    def size_of_cart(self):
        return len(self.cart)
    def total(self, tax):
        return format(self.cart.total(float(tax)), ".2f")
    def store_cart(self):
        return self.cart.store()
    def retrieve_cart(self, id):
        self.cart.retrieve(id)
    def size_of_cart(self):
        return len(self.cart)</code></pre>
<ol start="4">
<li>Add a last table to <kbd>recipe41.txt</kbd> that imports our Python code as a library to provide the last set of needed keywords:</li>
</ol>
<pre><code class="lang-python">***Settings****
Library recipe41.py</code></pre>
<ol start="5">
<li>Run the test scenario as if we are on a machine that has database support by typing <kbd>pybot recipe41.txt</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000005.png" class="lazyload" /></p>
<ol start="6">
<li>Run the test scenario, excluding tests that were tagged <kbd>database</kbd>, by typing <kbd>pybot -exclude database recipe41.txt</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000004.png" class="lazyload" /></p>
<ol start="7">
<li>Run the test scenario, including tests that were tagged <kbd>database</kbd>, by typing <kbd>pybot -include database recipe41.txt</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000152.png" class="lazyload" /></p>
<ol start="8">
<li>Look at <kbd>report.html</kbd>, and observe where the extra <kbd>[Documentation]</kbd> text appears as well as our <kbd>database</kbd> tag:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000141.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In this recipe, we added an extra section to the second test case, including both documentation and a tag:</p>
<pre><code class="lang-python">More complex by storing cart to database 
  [Documentation]  This test case has special tagging, so it can be excluded. This is in case the developer doesn't have the right database system installed to interact properly.cart.db 
  [Tags]  database 
  Given an empty cart 
  When I add a  carton of milk  for  2.50 
  And I add a   frozen pizza    for  3.50 
  And I store the cart 
  And I retrieve the cart 
  Then there are  2  items </code></pre>
<p>Tags are usable on the command line, as shown in the previous example. It provides a useful way to organize test cases. Test cases can have as many tags as needed.</p>
<p>We showed earlier that this provides a convenient command-line option to include or exclude based on tags. Tags also provide useful documentation, and the previous screenshot of <kbd>report.html</kbd> shows that test results are also subtotaled by tag:</p>
<ul>
<li>Tags can be used to identify different layers of testing, such as smoke, integration, and customer-facing layers</li>
<li>Tags can also be used to mark subsystems such as databases, invoicing, customer-service, and billing</li>
</ul>
<h3>There's more...</h3>
<p>This recipe demonstrates plain text formatting. Triple asterisks are used to surround header cells, and two spaces are used to designate a break between two cells.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It is debatable as to whether this is more difficult to read than HTML. It may not be as crisp as reading the HTML markup, but I personally preferred this to angle tax of reading HTML. It's possible to add more spaces so that the table's cells are clearer, but I didn't, because the font sizes of this book don't work very well with it.</p>
</div>
</div>
<h4>What about documentation?</h4>
<p>We also added a little bit of documentation for demonstration purposes. A piece of the text appears when <kbd>pybot</kbd> runs, and it also appears in the resulting artifacts.</p>
<h3>See also</h3>
<ul>
<li><em>Installing Robot Framework</em></li>
<li><em>Creating a data-driven test suite with Robot Framework</em></li>
<li><em>Writing a testable story using Robot Framework</em></li>
</ul>
<h2>Testing web basics with Robot Framework</h2>
<p>Web testing is a common style of acceptance testing, because the customer wants to know whether the system is acceptable, and this is a perfect way to demonstrate it.</p>
<p>In the previous recipes, we have explored writing tests against non-web applications. In this recipe, let's see how to use a third-party Robot Framework plugin to use Selenium to test a shopping cart web application.</p>
<h3>Getting ready...</h3>
<ol>
<li>We first need to activate our <kbd>virtualenv</kbd> setup.</li>
<li>For this recipe, we are using the Satchmo shopping cart web application. To start it, switch to the store directory and type python manage.py runserver. You can explore it by visiting http://localhost:8000.</li>
<li>Next, install Robot Framework and the third-party Selenium plugin, as shown in the <em>Installing&nbsp;</em><em>Robot&nbsp;</em><em>Framework</em> recipe.</li>
</ol>
<h3>How to do it...</h3>
<p>With the following steps, we will see how to get going with using some of the basic Robot commands for driving a web application:</p>
<ol>
<li>Create a plain text story file called <kbd>recipe42.txt</kbd>, with an opening description of the story:</li>
</ol>
<pre><code class="lang-python">As a store customer
I want to put things into my cart
So that I can verify the store's functionality.</code></pre>
<ol start="2">
<li>Create a section for test cases, and add a scenario that verifies that there is an empty shopping cart and captures a screenshot:</li>
</ol>
<pre><code class="lang-python">***Test Cases***
Inspect empty cart in detail
  Click link Cart
  Page Should Contain Your cart is empty
  Page Should Contain 0 - $0.00
  Capture Page Screenshot recipe42-scenario1-1.png</code></pre>
<ol start="3">
<li>Add another scenario that picks a book, adds two copies of the cart, and confirms the total cart value:</li>
</ol>
<pre><code class="lang-python">Load up a cart with 2 of the same
  Click link Science Fiction don't wait
  Capture Page Screenshot recipe42-scenario2-1.png
  Click link Robots Attack!
  Capture Page Screenshot recipe42-scenario2-2.png
  Input text quantity 2
  Capture Page Screenshot recipe42-scenario2-3.png
  Click button Add to cart
  Click link Cart
  Capture Page Screenshot recipe42-scenario2-4.png
  Textfield Value Should Be quantity 2
  Page Should Contain Robots Attack! (Hard cover)
  Html Should Contain &lt;td align="center"&gt;$7.99&lt;/td&gt;
  Html Should Contain &lt;td align="center"&gt;$15.98&lt;/td&gt;
  Html Should Contain &lt;td&gt;$15.98&lt;/td&gt;</code></pre>
<ol start="4">
<li>Add a section of keywords and define a keyword for inspecting the raw HTML of the page:</li>
</ol>
<pre><code class="lang-python">***Keywords***
Html Should Contain
    [Arguments]     ${expected}
    ${html}= Get Source
    Should Contain ${html} ${expected}
Startup
    Start Selenium Server
    Sleep 3s</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p><kbd>Get Source</kbd> is a Selenium Library keyword that fetches the raw HTML of the entire page. <kbd>Start Selenium Server</kbd> is another keyword to launch the Selenium server. A built-in <kbd>Sleep</kbd> call is included to avoid startup/shutdown timing issues, if this test happens before or after another one Selenium-based test suite.</p>
</div>
</div>
<ol start="5">
<li>Add a section that imports the Selenium library and also defines a setup and teardown process for launching and shutting down the browser for each test case:</li>
</ol>
<pre><code class="lang-python">***Settings***
Library         SeleniumLibrary
Test Setup      Open Browser http://localhost:8000
Test Teardown   Close All Browsers
Suite Setup     Startup
Suite Teardown  Stop Selenium Server</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p><kbd>Test Setup</kbd> is a built-in keyword that defines steps executed before each test case. In this case, it uses the Selenium library keyword <kbd>Open Browser</kbd> to launch a browser pointed at the Satchmo application. Test Teardown is a built-in keyword that executes at the end of each test and closes the browsers launched by this test.&nbsp;Suite Setup is a built-in keyword that is only run before any tests are executed, and Suite Teardown is only run after all the tests in this suite. In this case, we use it to start and stop the Selenium library.</p>
</div>
</div>
<ol start="6">
<li>Run the test suite by typing <kbd>pybot recipe42.txt</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000130.png" class="lazyload" /></p>
<ol start="7">
<li>Open <kbd>log.html</kbd> and observe the details, including the captured screenshots in each scenario. The following screenshot is just one of the many captured screenshots. Feel free to inspect the rest of the screenshots as well as the logs:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000121.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Robot Framework provides a powerful environment to define tests through keywords. The Selenium plugin interfaces with Selenium and provides a whole set of keywords that are focused on manipulating web applications and reading and confirming their outputs.</p>
<p>An important part of web application testing is getting a hold of an element to manipulate it or test values. The most common way of doing this is by checking key attributes of the element, such as <kbd>id</kbd>, <kbd>name</kbd>, or <kbd>href</kbd>. For example, in our scenario, there is a button we need to click on to add the book to the cart. It can be identified by either the ID,&nbsp;<kbd>addcart</kbd>, or the displayed text,&nbsp;<kbd>Add to cart</kbd>.</p>
<h3>There's more...</h3>
<p>While Robot Framework is free compared to other commercial frontend test solutions, it is important to realize that the effort in writing automated tests isn't free and effortless. It takes effort to make this an active part of frontend design.</p>
<p>Incorporating tools like Robot and Selenium library early in the process of screen design will encourage good practices like tagging frames and elements so that they'll be testable early on. This is no different from attempting to write automated tests for a backend server system after it's already built. Both situations are much more costly, if they are introduced later. Making automated testing a part of backend systems early on encourages similar coding to support testability.</p>
<p>If we are looking at embracing acceptance testing late in our development cycle, or perhaps trying to test a system we inherited from another team, we need to include time to make changes to the web interface in order to add tags and identifiers to support writing the tests.</p>
<h4>Learn about timing configurations&nbsp;&ndash; they may be important!</h4>
<p>While the Satchmo shopping cart application didn't have any significant delays in the tests we wrote, it doesn't mean other applications won't. If your web application has certain parts that are noticeably slower, it is valuable to read the online documentation&nbsp;about configuring how long Selenium should wait for a response from your application.</p>
<h3>See also</h3>
<ul>
<li><em>Installing Robot Framework</em></li>
<li><em>Creating a data-driven test suite with Robot Framework</em></li>
<li><em>Writing a testable story using Robot Framework</em></li>
</ul>
<h2>Using Robot Framework to verify web app security</h2>
<p>Web applications often have some sort of security in place. This is often in the form of a login page. A well written test case should start a new browser session at the beginning and close it at the end. This results in the user logging in repeatedly for every test case.</p>
<p>In this recipe, we will explore writing code to log in to Satchmo's admin page, as provided by Django. Then, we will show how to capture this entire login procedure into a single keyword, allowing us to smoothly write a test that visits the product catalog without getting encumbered by logging in.</p>
<h3>Getting ready</h3>
<ol>
<li>We first need to activate our <kbd>virtualenv</kbd> setup.</li>
<li>For this recipe, we are using the Satchmo shopping cart web application. To start it, switch to the store directory and type <kbd>python manage.py runserver</kbd>. You can explore it by visiting <kbd>http://localhost:8000</kbd>.</li>
<li>Next, install Robot Framework and the third-party Selenium plugin, as shown in the <em>Installing&nbsp;</em><em>Robot&nbsp;</em><em>Framework</em> recipe.</li>
</ol>
<h3>How to do it...</h3>
<p>The following steps will highlight how to capture login steps and then encapsulate them in a single custom keyword:</p>
<ol>
<li>Create a new file called <kbd>recipe43.txt</kbd>, and write a test story for exercising Django's admin interface:</li>
</ol>
<pre><code class="lang-python">As a system administrator
I want to login to Django's admin page
So that I can check the product catalog.</code></pre>
<ol start="2">
<li>Add a section for test cases, and write a test case that exercises the login page:</li>
</ol>
<pre><code class="lang-python">***Test Cases***
Logging in to the admin page
  Open Browser http://localhost:8000/admin
  Input text username gturnquist
  Input text password password
  Submit form
  Page Should Contain Link Products
  Close All Browsers</code></pre>
<ol start="3">
<li>Add another test case that inspects the product catalog and verifies a particular row of the table:</li>
</ol>
<pre><code class="lang-python">Check product catalog
  Given that I am logged in
  Click link Products
  Capture Page Screenshot recipe43-scenario2-1.png
  Table Should Contain result_list Robots Attack!
  Table Row Should Contain result_list 4 Robots Attack!
  Table Row Should Contain result_list 4 7.99
  Close All Browsers</code></pre>
<ol start="4">
<li>Create a keyword section that captures the login procedure as a single keyword:</li>
</ol>
<pre><code class="lang-python">***Keywords***
Given that I am logged in
  Open Browser http://localhost:8000/admin/
  Input text username gturnquist
  Input text password password
  Submit form

Startup
  Start Selenium Server
  Sleep 3s</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>For your own testing, put in the username and password you used when installing Satchmo. The&nbsp;<kbd>Start Selenium Server</kbd>&nbsp;keyword is another keyword to launch the Selenium server. A built-in Sleep call is included to avoid startup/shutdown timing issues if this test happens before or after another one Selenium-based test suite.</p>
</div>
</div>
<ol start="5">
<li>Finally, add a settings section that imports Selenium library and also starts and stops the Selenium server at the beginning and end of the test suite:</li>
</ol>
<pre><code class="lang-python">***Settings***
Library SeleniumLibrary
Suite Setup Startup
Suite Teardown Stop Selenium Server</code></pre>
<ol start="6">
<li>Run the test suite by typing <kbd>pybot recipe43.txt</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000114.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The first test case shows how we input username and password data and then submit the form. SeleniumLibrary allows us to pick a form by name, but in the event we don't identify it, it picks the first HTML form it finds. Since there is only one form on the login page, this works fine for us.</p>
<p>With the second test case, we want to navigate to the product catalog. Since it runs with a clean browser session, we are forced to deal with the login screen again. This means we need to include the same steps to log in again. For more comprehensive testing, we would probably write lots of test cases. <em>Why</em> <em>should</em> <em>we</em> <em>avoid</em> <em>copying</em> <em>and</em> pasting <em>the</em> <em>same</em> <em>login</em> <em>steps</em> <em>for</em> <em>every</em> <em>test</em> <em>case?</em> That's because it violates the <strong>Don't Repeat Yourself</strong> (<strong>DRY</strong>) principle. If the login page is modified, we might have to alter every instance.</p>
<p>Instead, we captured the login steps with <kbd>Given that I am logged in</kbd>&nbsp;keyword. This gives us a useful clause for many test cases, and lets us focus on the admin page.</p>
<h3>There's more...</h3>
<p>In this recipe, we used some of Selenium library's table testing operations. We verified that a particular book exists both at the table level as well as the row level. We also verified the price of the book in that row.</p>
<p>Finally, we captured a screenshot of the product catalog. This screenshot gives us a quick, visual glance we can use to either manually confirm the product catalog, or use to plan our next test step.</p>
<h4>Why not use a "remember me" option?</h4>
<p>Lots of websites include a "remember me" checkbox in order to save login credentials in a client-side cookie. The Django admin page doesn't have one, so why is this relevant<em>?</em> It's because many websites do, and we may be tempted to incorporate it into our tests to avoid logging in every time. Even if this option existed for the web app we want to test, it is not a good idea to use it. It creates a persistent state that can propagate from one test to the next. Different user accounts may have different roles, impacting what is visible. We may not know in what order test cases run and therefore have to add extra code to sniff what user we are logged in as.</p>
<p>Instead, it is much easier and cleaner to <em>not</em> persist this information. Instead, explicitly logging in through a single keyword provides a clearer intent. This doesn't mean we shouldn't test and confirm the remember checkbox of our particular web application. On the contrary, we should actually test both good and bad accounts to ensure that the login screen works as expected. However, beyond that, it is best to not confuse future test cases with persisted results of the current test case.</p>
<h4>Shouldn't we refactor the first test scenario to use the keyword?</h4>
<p>To uphold the DRY principle, we should have the login procedure in only one place inside our test story. However, for demonstration purposes, we coded it at the top, and then later copied the same code into a keyword. The best solution would be to encapsulate it into a single keyword that can be reused in either a test case or to define other custom keywords like <kbd>Given I am logged in</kbd>.</p>
<h4>Would arguments make the login keyword more flexible?</h4>
<p>Absolutely&mdash;in this test story, we hardcoded the username as well as the password. However, good testing of the login page would involve a data-driven table with lots of combinations of good and bad accounts along with valid and invalid passwords. This drives the need for some sort of login keyword that would accept username and password as arguments.</p>
<h3>See also</h3>
<ul>
<li><em>Installing Robot Framework</em></li>
<li><em>Using Pyccuracy to verify web app security</em></li>
<li><em>Creating a data-driven test suite with Robot Framework</em></li>
</ul>
<h2>Creating a project-level script to verify this chapter's acceptance tests</h2>
<p>We have used <kbd>pyccuracy_console</kbd> and <kbd>pybot</kbd> to run various test recipes. However, management of a Python project involves more than just running tests. Things like packaging, registering with the Python Project Index, and pushing to deployment sites are important procedures to manage.</p>
<p>Building a command-line script to encapsulate all this is very convenient. With this recipe, we will run a script that runs all the tests covered in this chapter.</p>
<h3>Getting ready</h3>
<ol>
<li>We first need to activate our <kbd>virtualenv</kbd> setup.</li>
<li>For this recipe, we are using the Satchmo shopping cart web application. To start it, switch to the store directory and type <kbd>python manage.py runserver</kbd>. You can explore it by visiting <kbd>http://localhost:8000</kbd>.</li>
<li>Next, install Robot Framework and the third-party Selenium plugin, as shown in the <em>Installing&nbsp;</em><em>Robot&nbsp;</em><em>Framework</em> recipe.</li>
<li>This recipe assumes that all the various recipes from this chapter have been coded.</li>
</ol>
<h3>How to do it...</h3>
<p>With these steps, we will see how to programmatically run all the tests in this chapter:</p>
<ol>
<li>Create a new file called <kbd>recipe44.py</kbd> to contain the code for this recipe.</li>
<li>Create a command-line script that defines several options:</li>
</ol>
<pre><code class="lang-python">import getopt
import logging
import os
import os.path
import re
import sys
from glob import glob

def usage():
    print
    print "Usage: python recipe44.py [command]"
    print
    print "t--help"
    print "t--test"
    print "t--package"
    print "t--publish"
    print "t--register"
    print
try:
    optlist, args = getopt.getopt(sys.argv[1:],
            "h",
            ["help", "test", "package", "publish", "register"])
except getopt.GetoptError:
    # print help information and exit:
    print "Invalid command found in %s" % sys.argv
    usage()
    sys.exit(2)</code></pre>
<ol start="3">
<li>Add a method that starts Selenium, runs the Pyccuracy-based tests, and then shuts down Selenium:</li>
</ol>
<pre><code class="lang-python">def test_with_pyccuracy():
    from SeleniumLibrary import start_selenium_server
    from SeleniumLibrary import shut_down_selenium_server
    from time import sleep

    f = open("recipe44_selenium_log.txt", "w")
    start_selenium_server(logfile=f)
    sleep(10)

    import subprocess
    subprocess.call(["pyccuracy_console"])
    shut_down_selenium_server()
    sleep(5)
    f.close()</code></pre>
<ol start="4">
<li>Add a method that runs the Robot Framework tests:</li>
</ol>
<pre><code class="lang-python">def test_with_robot():
    from robot import run
    run(".")</code></pre>
<ol start="5">
<li>Add a method to run both of these test methods:</li>
</ol>
<pre><code class="lang-python">def test():
    test_with_pyccuracy()
    test_with_robot()</code></pre>
<ol start="6">
<li>Add some stubbed out methods for the other project functions:</li>
</ol>
<pre><code class="lang-python">def package():
    print "This is where we can plug in code to run " +
        "setup.py to generate a bundle."
def publish():
    print "This is where we can plug in code to upload " +
        "our tarball to S3 or some other download site."
def register():
    print "setup.py has a built in function to " +
        "'register' a release to PyPI. It's " +
        "convenient to put a hook in here."
    # os.system("%s setup.py register" % sys.executable)</code></pre>
<ol start="7">
<li>Add some code that parses the options:</li>
</ol>
<pre><code class="lang-python">if len(optlist) == 0:
    usage()
    sys.exit(1)
# Check for help requests, which cause all other
# options to be ignored.
for option in optlist:
    if option[0] in ("--help", "-h"):
        usage()
        sys.exit(1)

# Parse the arguments, in order
for option in optlist:
    if option[0] in ("--test"):
        test()
    if option[0] in ("--package"):
        package()
    if option[0] in ("--publish"):
        publish()
    if option[0] in ("--register"):
        register()</code></pre>
<ol start="8">
<li>Run the script with the testing flag by typing <kbd>python</kbd><kbd>recipe44 -test</kbd>. In the following screenshot, we can see that all the Pyccuracy tests passed:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000102.png" class="lazyload" /></p>
<p>In the next screenshot, we can see that the Robot Framework tests passed as well:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000090.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We use Python's <kbd>getopt</kbd> module to define command-line options:</p>
<pre><code class="lang-python">    optlist, args = getopt.getopt(sys.argv[1:], 
            "h", 
           ["help", "test", "package", "publish", "register"]) </code></pre>
<p>This maps the following:</p>
<ul>
<li><kbd>"h"</kbd>: <kbd>-h</kbd></li>
<li><kbd>"help"</kbd>: <kbd>--help</kbd></li>
<li><kbd>"test"</kbd>: <kbd>--test</kbd></li>
<li><kbd>"package"</kbd>: <kbd>--package</kbd></li>
<li><kbd>"publish"</kbd>: <kbd>--publish</kbd></li>
<li><kbd>"register"</kbd>: <kbd>--register</kbd></li>
</ul>
<p>We scan the list of received arguments and call the appropriate functions. For our test functions, we used Python's <kbd>subprocess</kbd> module to call <kbd>pyccuracy_console</kbd>. We could have done the same to call <kbd>pybot</kbd>, but Robot Framework provides a convenient API to call it directly:</p>
<pre><code class="lang-python">    from robot import run 
    run(".") </code></pre>
<p>This lets us use it inside our code.</p>
<h3>There's more</h3>
<p>To run these tests, we need Selenium running. Our Robot Framework tests are built to run Selenium on their own. Pyccuracy doesn't have such a feature, so it needed another means. In those recipes, we used <kbd>java -jar selenium-server.jar</kbd>. We could try to manage this, but it is easier to use Selenium library's API to start and stop Selenium.</p>
<p>This is where writing code in pure Python gives us the most options. We are able to empower Pyccuracy with parts of another library that was never intended to work with it.</p>
<h4>Can we only use getopt?</h4>
<p>Python 2.7 introduces <kbd>argparse</kbd> as an alternative. Current documentation has no indication that <kbd>getopt</kbd> is deprecated, so it's safe to use it as we have just done. The <kbd>getopt</kbd> module is a nice, easy-to-use command-line parser.</p>
<h4>What's wrong with using the various command-line tools?</h4>
<p>There is nothing wrong with using tools like <kbd>pyccuracy_console</kbd>, <kbd>pybot</kbd>, <kbd>nosetests</kbd>, and many other tools that come with the Python libraries. The purpose of this recipe is to offer a convenient, alternative approach that brings all these tools into one central script. By investing a little bit of time in this script, we don't have to remember how to use all these features; instead, we can develop our script to support the development workflow of our project.</p>

</div>


<!--Chapter 6-->


<div class="chapter" data-chapter-number="6">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 6 </span></div>
<h1 class="chaptertitle">Integrating Automated Tests with Continuous Integration</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>


<p>In this chapter, we will cover:</p>
<ul>
<li>Generating a continuous integration report for Jenkins with NoseXUnit</li>
<li>Configuring Jenkins to run Python tests upon commit</li>
<li>Configuring Jenkins to run Python tests when scheduled</li>
<li>Generating a continuous integration report for TeamCity using teamcity-nose</li>
<li>Configuring TeamCity to run Python tests upon commit</li>
<li>Configuring TeamCity to run Python tests when scheduled</li>
</ul>
<h2>Introduction</h2>
<p>The classic software development process known as the waterfall model involves the following stages:</p>
<ol>
<li>Requirements are collected and defined</li>
<li>Designs are drafted to satisfy the requirements</li>
<li>An implementation strategy is written to meet the design</li>
<li>Coding is done</li>
<li>The coded implementation is tested</li>
<li>The system is integrated with other systems as well as future versions of that system</li>
</ol>
<p>In the waterfall model, these steps are often spread across several months of work. What this means is that the final step of integration with external systems is done after several months and often takes a lot of effort.&nbsp;<strong>Continuous integration (CI)</strong> remedies the deficiencies of the waterfall model by introducing the concept of writing tests that exercise these points of integration and has them run automatically whenever the code is checked into the system. Teams that adopt CI often adopt a corresponding policy of immediately fixing the baseline if the test suite fails. This forces the team to keep their code working and integrated continuously, thus making this final step relatively cost-free. Teams that adopt a more agile approach work in much shorter cycles. Teams may work in coding sprints that may vary anywhere from the weekly to the monthly. Again, by having integrating test suites run with every check-in, the baseline is always kept functional; thus, it is ready for delivery at any time. This prevents the system from being in a nonworking state that is only brought into working state at the end of a sprint or at the end of a waterfall cycle. It opens the door to more code demonstrations for either the customer or management, in which feedback can be garnered and fed more proactively into development. This chapter is more focused on integrating automated tests with CI systems than with writing the tests. For that reason, we will reuse the following Shopping Cart application. Create a new file called <kbd>cart.py</kbd> and enter the following code into it:</p>
<pre><code class="lang-python">class ShoppingCart(object): 
    def __init__(self): 
        self.items = [] 
 
    def add(self, item, price): 
        for cart_item in self.items: 
            # Since we found the item, we increment 
            # instead of append 
            if cart_item.item == item: 
                cart_item.q += 1 
                return self 
 
        # If we didn't find, then we append 
        self.items.append(Item(item, price)) 
        return self 
 
    def item(self, index): 
        return self.items[index-1].item 
 
    def price(self, index): 
        return self.items[index-1].price * self.items[index-1].q 
 
    def total(self, sales_tax): 
        sum_price = sum([item.price*item.q for item in self.items]) 
        return sum_price*(1.0 + sales_tax/100.0) 
 
    def __len__(self): 
        return sum([item.q for item in self.items]) 
 
class Item(object): 
    def __init__(self, item, price, q=1): 
        self.item = item 
        self.price = price 
        self.q = q </code></pre>
<p>To exercise this simple application, the following simple set of unit tests will be used by various recipes in this chapter to demonstrate CI. Create another file called <kbd>tests.py</kbd> and enter the following test code into it:</p>
<pre><code class="lang-python">from cart import * 
import unittest 
 
class ShoppingCartTest(unittest.TestCase): 
    def setUp(self): 
        self.cart = ShoppingCart().add("tuna sandwich", 15.00) 
 
    def test_length(self): 
        self.assertEquals(1, len(self.cart)) 
 
    def test_item(self): 
        self.assertEquals("tuna sandwich", self.cart.item(1)) 
 
    def test_price(self): 
        self.assertEquals(15.00, self.cart.price(1)) 
 
    def test_total_with_sales_tax(self): 
        self.assertAlmostEquals(16.39,  
                                self.cart.total(9.25), 2) </code></pre>
<p>This simple set of tests doesn't look very impressive, does it? In fact, it isn't really integration testing like we were talking about earlier, but instead it appears to be basic unit testing, right? Absolutely! This chapter isn't focusing on writing test code. So, if this book is about code recipes, why are we focusing on tools? Because there is more to making automated testing work with your team than writing tests. It's important to become aware of the tools that take the concepts of automating tests and leverage them into our development cycles. CI products are a valuable tool, and we need to see how to link them with our test code, in turn allowing the whole team to come on board and make testing a first-class citizen of our development process. This chapter explores two powerful CI products: Jenkins and TeamCity.&nbsp;</p>
<p><strong>Jenkins</strong> (<a href="http://jenkins-ci.org/">http://jenkins-ci.org/</a>) is an open source product whose creation was led by a developer originally from Sun Microsystems, who left after Sun's acquisition by Oracle. It has a strong developer community with many people providing patches, plugins, and improvements. It was originally called <strong>Hudson</strong>, but the development community voted to rename it to avoid legal entanglements. There is more history to the entire Hudson/Jenkins naming that can be read online, but it's not relevant to the recipes in this book.&nbsp;<strong>TeamCity</strong> (<a href="http://www.jetbrains.com/teamcity/">http://www.jetbrains.com/teamcity/</a>) is a product created by JetBrains, the same company that produces commercial products such as IntelliJ IDE, ReSharper, and the PyCharm IDE. The Professional Edition is a free version that will be used in this chapter to show another CI system. It has an enterprise version, a commercial upgrade, which you can evaluate for yourself.</p>
<h2>Generating a CI report for Jenkins using NoseXUnit</h2>
<p><strong>JUnit</strong> (<a href="http://junit.org">http://junit.org</a>) is a software industry leader in automated testing. It provides the ability to generate XML report files that are consumable by many tools. This extends to continuous tools like Jenkins.&nbsp;<strong>NoseXUnit</strong> (<a href="http://nosexunit.sourceforge.net/">http://nosexunit.sourceforge.net/</a>) is a <kbd>nose</kbd> plugin that generates XML reports with Python test results in the same format. It works like JUnit with XML reporting but for PyUnit. Even though we aren't building Java code, there is no requirement that states our CI server can't be a Java-based system. As long as we can generate the right reports, those tools are candidates for usage. Considering that one of the most popular and well-supported CI systems is Jenkins, this type of plugin is very useful. With this recipe, we will explore generating consumable reports from simple Python testing.</p>
<h3>Getting ready</h3>
<p>The following steps are needed to have all the components installed for this chapter:</p>
<ol>
<li>Install <kbd>nose</kbd>.</li>
<li>Install NoseXUnit (<a href="http://nosexunit.sourceforge.net/">http://nosexunit.sourceforge.net/</a>) by typing <kbd>pip install nosexunit</kbd></li>
</ol>
<h3>How to do it...</h3>
<p>The following steps will show how to use the NoseXUnit plugin to generate an XML report in a Jenkins-compatible format:</p>
<ol>
<li>Test the shopping cart application using <kbd>nosetests</kbd> and the NoseXUnit plugin by typing <kbd>nosetests tests.py --with-nosexunit</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000083.png" class="lazyload" /></p>
<ol start="2">
<li>Open the report found in <kbd>target/NoseXUnit/core/TEST-tests.xml</kbd> using an XML or text editor. The following screenshot shows the report displayed in Spring Tool Suite (<a href="http://www.springsource.com/developer/sts">http://www.springsource.com/developer/sts</a>), an Eclipse derivative (this is by no means a recommendation). Many modern IDEs have built-in XML support as do other editors like Emacs, TextPad, and so on:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000071.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>NoseXUnit collects the outcome of each test and generates an XML report that has the same format as JUnit. The XML file isn't designed to be human-consumable, but it's not too hard to discern the results. When we ran <kbd>nosetests</kbd> earlier, how many test cases passed? What were the test method names? In this XML file, we can see the names of the four test cases. In fact, if this file is opened inside certain tools such as STS, it displays itself as a test outcome:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000065.png" class="lazyload" /></p>
<p>We don't have to use STS to do any of this. In fact, STS is a bit heavyweight for this simple task. Your favorite XML or text editor is fine to inspect the report. I just wanted to demonstrate how the output of this plugin works neatly with existing tools. By typing <kbd>nosetests help</kbd>, we can see all the options that <kbd>nose</kbd> has from all the installed plugins. This includes:</p>
<ul>
<li><kbd>--core-target=CORE_TARGET</kbd>: Output folder for test reports (defaults to target/NoseXUnit/core)</li>
<li><kbd>--with-nosexunit</kbd>: Runs it through the plugin</li>
</ul>
<h2>Configuring Jenkins to run Python tests upon commit</h2>
<p>Jenkins can be configured to invoke our test suite upon commit. This is very useful, because we can gear it to track our changes. Teams that use CI systems usually adopt an attitude of addressing CI failures immediately in order to keep the baseline functional. Jenkins offers an almost unlimited number of features, such as retrieving the latest source from version control, packaging a release, running tests, and even analyzing source code. This recipe shows how to configure Jenkins to run our test suite against our Shopping Cart application.</p>
<h3>Getting ready</h3>
<ol>
<li>Download Jenkins from <a href="http://mirrors.jenkins-ci.org/war/latest/jenkins.war">http://mirrors.jenkins-ci.org/war/latest/jenkins.war</a>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000058.png" class="lazyload" /></p>
<ol start="2">
<li>Start it up by running <kbd>java -jar jenkins.war</kbd>. It's important that no other applications are listening on port <kbd>8080</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000048.png" class="lazyload" /></p>
<ol start="3">
<li>Open the console to confirm Jenkins is working:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000036.png" class="lazyload" /></p>
<ol start="4">
<li>Click on Manage Jenkins.</li>
<li>Click on Manage Plugins.</li>
<li>Click on the Available tab.</li>
<li>Find the Git Plugin and click the checkbox next to it.</li>
<li>At the bottom of the page, click on the Install button. Verify that the plugin has successfully installed.</li>
<li>Navigate back to the dashboard screen.</li>
<li>Shut down Jenkins and start it back up again.</li>
<li>Install Git source code control on your machine. You can visit <a href="http://git-scm.com/">http://git-scm.com/</a> to find downloadable packages. It is also possible that your system may include package installation options like MacPorts or Homebrew for Macs, <kbd>yum</kbd> for Red Hat-based Linux distributions, and <kbd>apt-get</kbd> for Debian/Ubuntu systems.</li>
</ol>
<ol start="12">
<li>Create an empty folder for this recipe:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ mkdir /tmp/recipe46
  </code></pre>
<ol start="13">
<li>Initialize the folder for source code maintenance:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ git init /tmp/recipe46
    Initialized empty Git repository in /private/tmp/recipe46/.git/
  </code></pre>
<ol start="14">
<li>Copy the Shopping Cart application into the folder, add it, and commit the changes:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp cart.py /tmp/recipe46/
    gturnquist$ cd /tmp/recipe46/
    gturnquist$ git add cart.py
    gturnquist$ git commit -m "Added shopping cart application to setup this recipe."
    [master (root-commit) 057d936] Added shopping cart application to setup this recipe.
     1 files changed, 35 insertions(+), 0 deletions(-)
     create mode 100644 cart.py
  </code></pre>
<h3>How to do it...</h3>
<p>The following steps will show how to put our code under control and then run the test suite when we make any changes and commit them:</p>
<ol>
<li>Open the Jenkins console.</li>
<li>Click on New Job.</li>
<li>Enter <kbd>recipe46</kbd> as the Job name and pick build a free-style software project.</li>
<li>Click on a.</li>
<li>In the Source Code Management section, pick Git. For the&nbsp;URL, enter <kbd>/tmp/recipe46/</kbd>.</li>
<li>In the Build Triggers section, pick Poll SCM and enter <kbd>* * * * *</kbd> into the schedule box, to trigger a poll once per minute.</li>
<li>In the Build section, select Execute shell and enter the following ad hoc script, which loads the virtualenv and runs the test suite:</li>
</ol>
<pre><code class="lang-python">. /Users/gturnquist/ptc/bin/activate 
nosetests tests.py -with-nosexunit </code></pre>
<p>You need to substitute the command to activate your own virtualenv, whether this is on Windows, Linux, or macOS, and then follow it with the command used to run the tests just like we did earlier in this chapter.</p>
<ol start="8">
<li>In the Post-build Actions section, pick Publish JUnit test result report and enter <kbd>target/NoseXUnit/core/*.xml</kbd>, so that the test results are collected by Jenkins.</li>
<li>Click on Save to store all the job settings.</li>
<li>Click on Enable Auto Refresh. We should expect the first run to fail, because we haven't added any tests yet:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000027.png" class="lazyload" /></p>
<ol start="11">
<li>Copy the test suite into the controlled source folder, add it, and commit it:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp tests.py /tmp/recipe46/
    gturnquist$ cd /tmp/recipe46/
    gturnquist$ git add tests.py
    gturnquist$ git commit -m "Added tests for the recipe."
    [master 0f6ef56] Added tests for the recipe.
     1 files changed, 20 insertions(+), 0 deletions(-)
     create mode 100644 tests.py
  </code></pre>
<ol start="12">
<li>Watch to verify whether Jenkins launches a successful test run:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000018.png" class="lazyload" /></p>
<ol start="13">
<li>Navigate to the test results page, where we can see that four of our tests were run.</li>
</ol>
<h3>How it works...</h3>
<p>Jenkins provides a powerful, flexible way to configure CI jobs. In this recipe, we configured it to poll our software confirmation management system once a minute. When it detects a change, it pulls a fresh copy of the software and runs our test script. By using the NoseXUnit plugin, we generated an artifact that was easy to harvest with Jenkins. With a handful of steps, we were able to configure a web page that monitors our source code.</p>
<h3>There's more...</h3>
<p>Jenkins has lots of options. If you examine the web interface, you can drill into output logs to see what actually happened. It also collects trends showing how long we have had success, when the last build failed, and more.</p>
<h4>Do I have to use git for source code management?</h4>
<p>The answer is no. We used it in this recipe to show quickly how to install a Jenkins plugin from inside the web interface. To apply the plugin, we had to restart Jenkins. Subversion and CVS are supported out of the box. Jenkins also has plugins that support every major source code control system out there, so it should be easy to meet your needs. In fact, there is support for social coding sites like GitHub and BitKeeper. Instead of using the Git plugin, we could configure our Jenkins installation to watch a certain GitHub account for updates.</p>
<h4>What is the format of polling?</h4>
<p>We configured the polling with <kbd>* * * * *</kbd>, which means once a minute. This is based on the format used to configure crontab files. The columns from left to right are:</p>
<ul>
<li><strong>MINUTE</strong>:&nbsp;Minutes within the hour (0-59)</li>
<li><strong>HOUR</strong>:&nbsp;The hour of the day (0-23)</li>
<li><strong>DOM</strong>:&nbsp;The day of the month (1-31)</li>
<li><strong>MONTH</strong>:&nbsp;The month (1-12)</li>
<li><strong>DOW:</strong>&nbsp;The day of the week (0-7) where 0 and 7 are Sunday</li>
</ul>
<h3>See also</h3>
<p>Generating a CI report for Jenkins using NoseXUnit</p>
<h2>Configuring Jenkins to run Python tests when scheduled</h2>
<p>We just explored how to configure Jenkins to run our test suite when we commit the code changes. Jenkins can also be configured to invoke our test suite at scheduled intervals. This is very useful, because we can gear it to make scheduled releases. Daily or weekly releases can provide potential customers with a nice release cadence. CI releases are usually understood to not necessarily be final, but instead provide bleeding-edge support in case new features need to be investigated early and integrated by the customer.</p>
<h3>Getting ready</h3>
<p>The following steps are used to set up Jenkins as well as a copy of our tests, so we can poll it at a scheduled interval:</p>
<ol>
<li>Set up Jenkins as shown in the earlier recipe,&nbsp;<em>Configuring Jenkins to run Python tests upon commit</em>. This should include having set up the Git plugin.</li>
<li>Create an empty folder for this recipe:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ mkdir /tmp/recipe47
  </code></pre>
<ol start="3">
<li>Initialize the folder for source code maintenance:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ git init /tmp/recipe47
    Initialized empty Git repository in /private/tmp/recipe47/.git/
  </code></pre>
<ol start="4">
<li>Copy the Shopping Cart application into the folder, add it, and commit the changes:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp cart.py /tmp/recipe47/
    gturnquist$ cd /tmp/recipe47/
    gturnquist$ git add cart.py
    gturnquist$ git commit -m "Added shopping cart application to setup this recipe."
    [master (root-commit) 057d936] Added shopping cart application to setup this recipe.
     1 files changed, 35 insertions(+), 0 deletions(-)
     create mode 100644 cart.py
  </code></pre>
<h3>How to do it...</h3>
<p>The following steps will let us explore creating a Jenkins job to run our automated test suite periodically:</p>
<ol>
<li>Open the Jenkins console.</li>
<li>Click on New Job.</li>
<li>Enter <kbd>recipe47</kbd> as the Job name and pick Build a free-style software project.</li>
<li>Click on Ok.</li>
<li>In the Source Code Management section, pick Git. For the&nbsp;URL, enter <kbd>/tmp/recipe47/</kbd>.</li>
<li>In the Build Triggers section, pick Build periodically and enter some time in the future. While writing this recipe for the book, the job was created around 6:10 P.M., so entering <kbd>15 18 * * *</kbd> into the schedule box schedules the job five minutes into the future at 6:15 P.M.</li>
<li>In the Build section, select Execute shell and enter the following ad hoc script, which loads the virtualenv and runs the test suite:</li>
</ol>
<pre><code class="lang-python">. /Users/gturnquist/ptc/bin/activatenosetests tests.py -with-nosexunit</code></pre>
<p>You need to replace this with the command used to activate your virtualenv, followed by the step to run the tests.</p>
<ol start="8">
<li>In the Post-build Actions section, pick Publish JUnit test result report and enter <kbd>target/NoseXUnit/core/*.xml</kbd>, so that test results are collected by Jenkins.</li>
<li>Click on Save to store all the job settings.</li>
<li>Click on Enable Auto Refresh.</li>
<li>Copy the test suite into the controlled source folder, add it, and commit it:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp tests.py /tmp/recipe47/
    gturnquist$ cd /tmp/recipe47/
    gturnquist$ git add tests.py
    gturnquist$ git commit -m "Added tests for the recipe."
    [master 0f6ef56] Added tests for the recipe.
     1 files changed, 20 insertions(+), 0 deletions(-)
     create mode 100644 tests.py
  </code></pre>
<ol start="12">
<li>Watch to verify whether Jenkins launches a successful test run:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000007.png" class="lazyload" /></p>
<ol start="13">
<li>Navigate to the test results, and we can see that four of our tests were run.</li>
</ol>
<h3>How it works...</h3>
<p>This is very similar to the previous recipe, only this time we configured a polling interval for running our test suite instead of polling the version control system. It is useful to run a build once a day to make sure things are stable and working.</p>
<h3>There's more...</h3>
<p>Jenkins has lots of options. If you examine the web interface, you can drill into output logs to see what actually happened. It also collects trends showing how long we have had success, when the last build failed, and more. To be honest, Jenkins has so many plugins and options that an entire book could be devoted to exploring its features. This half of the chapter is merely an introduction to using it with some common jobs that are test-oriented.</p>
<h4>Jenkins versus TeamCity</h4>
<p>So far, we have explored using Jenkins. Later in this chapter, we will visit TeamCity. What are the differences? Why should we pick one or the other? Feature-wise, they both offer powerful choices. That is why they are both covered in this book. The key thing both provide is setting up jobs to run tests, as well as other things like packaging. A key difference is that Jenkins is an open source product and TeamCity is commercial. You or your company may prefer to have a paid company associated with the product (<a href="http://www.jetbrains.com/">http://www.jetbrains.com/</a>), which is what TeamCity offers. This doesn't make the decision crystal clear because the principal developer of Jenkins currently works for CloudBees (<a href="http://www.cloudbees.com/">http://www.cloudbees.com/</a>), which invests effort in Jenkins as well as products surrounding it. If commercial support isn't imperative, you may find the pace of development of Jenkins is faster and the number of plugins more diverse. The bottom line is that choosing the product that meets your CI needs requires a detailed analysis and simply can't be answered here.</p>
<h3>See also</h3>
<p>Generating a CI report for Jenkins using NoseXUnit</p>
<h2>Generating a CI report for TeamCity using teamcity-nose</h2>
<p>There is a <kbd>nose</kbd> plugin that automatically detects when tests are being run from inside TeamCity. This conveniently captures test results and communicates them with TeamCity. With this recipe, we will explore how to setup a CI job inside TeamCity that runs our tests and then manually invokes that job.</p>
<h3>Getting ready</h3>
<p>The following steps are needed to get us prepared to run a TeamCity CI job:</p>
<ol>
<li>Install <kbd>nosetests</kbd>.</li>
<li>Install <kbd>teamcity-nose</kbd> by typing <kbd>pip install teamcity-nose</kbd>.</li>
</ol>
<ol start="3">
<li>Download TeamCity using Wget (<a href="http://download.jetbrains.com/teamcity/TeamCity-6.0.tar.gz">http://download.jetbrains.com/teamcity/TeamCity-6.0.tar.gz</a>).</li>
<li>Unpack the download.</li>
<li>Switch to the&nbsp;<kbd>TeamCity/bin</kbd> directory.</li>
<li>Start it up: <kbd>./runAll.sh start</kbd>.</li>
<li>Open a browser to <kbd>http://localhost:8111</kbd>.</li>
<li>If this is the first time you are starting TeamCity, accept the license agreement.</li>
<li>Create an administrator account by picking a username and password.</li>
<li>Install Git source code control on your machine.</li>
<li>Create an empty folder for this recipe:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ mkdir /tmp/recipe48
  </code></pre>
<ol start="12">
<li>Initialize the folder for source code maintenance:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ git init /tmp/recipe48
    Initialized empty Git repository in /private/tmp/recipe48/.git/
  </code></pre>
<ol start="13">
<li>Copy the shopping cart application and tests into the folder, add it, and commit the changes:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp cart.py /tmp/recipe48/
    gturnquist$ cp tests.py /tmp/recipe48/
    gturnquist$ cd /tmp/recipe48/
    gturnquist$ git add cart.py tests.py
    gturnquist$ git commit -m "Added shopping cart and tests to setup this recipe."
    [master (root-commit) ccc7155] Added shopping cart and tests to setup this recipe.
     2 files changed, 55 insertions(+), 0 deletions(-)
     create mode 100644 cart.py
     create mode 100644 tests.py
  </code></pre>
<h3>How to do it...</h3>
<p>The following steps show how to configure a CI job in TeamCity:</p>
<ol>
<li>Log in to the TeamCity console.</li>
<li>Underneath the&nbsp;Projects tab, click Create project.</li>
<li>Type in <kbd>recipe48</kbd>, and then click Create.</li>
</ol>
<ol start="4">
<li>Click Add a build configuration for this project.</li>
<li>Enter <kbd>nose testing</kbd> for the name and then click VCS settings.</li>
<li>Click on Create and attach new VCS root.</li>
<li>Enter <kbd>recipe48</kbd> in VCS root name.</li>
<li>Select Git as the Type of VCS.</li>
<li>Enter <kbd>/tmp/recipe48</kbd> as the Fetch URL.</li>
<li>Click on Test Connection to confirm the settings and then click Save.</li>
<li>Click on Add Build Step.</li>
<li>Select Command Line for Runner type.</li>
<li>Select Custom script for Run type and enter the following script:</li>
</ol>
<pre><code class="lang-python">. /Users/gturnquist/ptc/bin/activatenosetests tests.py</code></pre>
<p>You need to customize this with the command needed to activate your virtualenv.</p>
<ol start="14">
<li>Click on Save.</li>
<li>Go back to the project, and run it manually:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000154.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This plugin is designed not to be used in the classic style of being invoked by a command-line argument. Instead, it is manually run automatically whenever <kbd>nosetests</kbd> is executed, and it checks if there is a TeamCity-specific environment variable set. If so, it kicks in by printing out viewable results as well as sending back useful information to TeamCity:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000143.png" class="lazyload" /></p>
<p>Otherwise, the plugin lets itself be bypassed and does nothing. If the plugin was NOT installed, the following screenshot would be the output:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000131.png" class="lazyload" /></p>
<p>In turn, drilling into the details shows the following output with little detail. There are four periods, one for each test method, but we don't know much more than that:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000125.png" class="lazyload" /></p>
<p>This means no extra arguments are needed to use the TeamCity plugin, but running it from the command line, outside of TeamCity, causes no changes.</p>
<h2>Configuring TeamCity to run Python tests upon commit</h2>
<p>TeamCity can be configured to invoke your test suite upon commit.</p>
<h3>Getting ready</h3>
<p>The following steps will help us prep TeamCity to run our test suite when the code changes are committed:</p>
<ol>
<li>Set up TeamCity like in the previous recipe, and have it started up. You also need to have <kbd>git</kbd> installed, as mentioned earlier in this chapter.</li>
<li>Create an empty folder for this recipe:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ mkdir /tmp/recipe49
  </code></pre>
<ol start="3">
<li>Initialize the folder for source code maintenance:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ git init /tmp/recipe49
    Initialized empty Git repository in /private/tmp/recipe49/.git/
  </code></pre>
<ol start="4">
<li>Copy the Shopping Cart application into the folder, add it, and commit the changes:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp cart.py /tmp/recipe49/
    gturnquist$ cd /tmp/recipe49/
    gturnquist$ git add cart.py
    gturnquist$ git commit -m "Added shopping cart application to setup this recipe."
    [master (root-commit) 057d936] Added shopping cart application to setup this recipe.
     1 files changed, 35 insertions(+), 0 deletions(-)
     create mode 100644 cart.py
  </code></pre>
<h3>How to do it...</h3>
<p>These steps will show us how to create a TeamCity job that polls version control to detect a change and then run a test suite:</p>
<ol>
<li>Log in to the TeamCity console.</li>
<li>Underneath the&nbsp;Projects tab, click Create project.</li>
<li>Type in <kbd>recipe49</kbd>, and then click Create.</li>
<li>Click Add a build configuration for this project.</li>
<li>Enter <kbd>nose testing</kbd> for the name and then click VCS settings.</li>
<li>Click on Create and attach new VCS root.</li>
<li>Enter <kbd>recipe49</kbd> in VCS root name.</li>
<li>Select Git as the Type of VCS.</li>
<li>Enter <kbd><strong>/</strong>tmp/recipe49</kbd> as the Fetch URL.</li>
<li>Click on Test Connection to confirm settings and then click Save.</li>
<li>Click on Add Build Step.</li>
<li>Select Command Line for Runner type.</li>
<li>Select Custom script for Run type and enter the following script:</li>
</ol>
<pre><code class="lang-python">. /Users/gturnquist/ptc/bin/activatenosetests tests.py</code></pre>
<p>You must replace this with the command to activate your own virtualenv and invoke <kbd>nosetests</kbd>.</p>
<ol start="14">
<li>Click on Save.</li>
<li>Click on Build Triggering.</li>
<li>Click on Add new Trigger.</li>
<li>Pick VCS Trigger from Trigger Type.</li>
<li>At the top, it should display VCS Trigger will add build to the queue if VCS check-in is detected. Click Save.</li>
<li>Navigate back to Projects. There should be no jobs scheduled or results displayed.</li>
</ol>
<ol start="20">
<li>Click on Run. It should fail, because we haven't added the tests to the repository:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000115.png" class="lazyload" /></p>
<ol start="21">
<li>From the command line, copy the test file into the repository. Then add it and commit it:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp tests.py /tmp/recipe49/
    gturnquist$ cd /tmp/recipe49/
    gturnquist$ git add tests.py
    gturnquist$ git commit -m "Adding tests."
    [master 4c3c418] Adding tests.
     1 files changed, 20 insertions(+), 0 deletions(-)
     create mode 100644 tests.py
  </code></pre>
<ol start="22">
<li>Go back to the browser. It may take a minute for TeamCity to detect the change in the code and start another build job. It should automatically update the screen:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000104.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In this recipe, we configured TeamCity to do a job for us tied to a specific trigger. The trigger is activated whenever a check-in is done to the software baseline. We had to take several steps to configure this, but it demonstrates the flexible power TeamCity offers. We also installed the <kbd>teamcity-nose</kbd> plugin, which gave us more details about the results.</p>
<h3>There's more...</h3>
<p>TeamCity calls our <kbd>nose testing</kbd> job a build job. That is because running tests isn't the only thing TeamCity is used for. Instead, it's geared to build packages, deploy to sites, and any other action we may want it to do anytime a commit happens. This is why CI servers are sometimes called <strong>build servers</strong>. But if we start with simple jobs like testing the baseline, we are well on our way to discovering the other useful features TeamCity has to offer.</p>
<h4>What did teamcity-nose give us?</h4>
<p>This is a <kbd>nose</kbd> plugin that provided us with a more detailed output. We didn't go into much detail in this recipe.</p>
<h3>See also</h3>
<ul>
<li>Generating a CI report for TeamCity using <kbd>teamcity-nose</kbd></li>
<li>Configuring Jenkins to run Python tests upon commit</li>
</ul>
<h2>Configuring TeamCity to run Python tests when scheduled</h2>
<p>TeamCity can be configured to invoke our test suite and collect results at a scheduled interval.</p>
<h3>Getting ready</h3>
<p>These steps will prepare us for this recipe by starting up TeamCity and having some code ready for testing:</p>
<ol>
<li>Set up TeamCity like we did earlier in this chapter and have it up and running.</li>
<li>Create an empty folder for this recipe:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ mkdir /tmp/recipe50</code></pre>
<ol start="3">
<li>Initialize the folder for source code maintenance:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ git init /tmp/recipe50
    Initialized empty Git repository in /private/tmp/recipe50/.git/</code></pre>
<ol start="4">
<li>Copy the shopping cart application into the folder, add it, and commit the changes:</li>
</ol>
<pre><code class="lang-python">    gturnquist$ cp cart.py /tmp/recipe50/
    gturnquist$ cp tests.py /tmp/recipe50/
    gturnquist$ cd /tmp/recipe50/
    gturnquist$ git add cart.py tests.py
    gturnquist$ git commit -m "Adding shopping cart and tests for this recipe."
    [master (root-commit) 01cd72a] Adding shopping cart and tests for this recipe.
     2 files changed, 55 insertions(+), 0 deletions(-)
     create mode 100644 cart.py
     create mode 100644 tests.py  </code></pre>
<h3>How to do it...</h3>
<p>These steps show the details for configuring TeamCity to run our test suite on a scheduled basis:</p>
<ol>
<li>Log in to the TeamCity console.</li>
<li>Underneath the&nbsp;Projects tab, click Create project.</li>
<li>Type in <kbd>recipe50</kbd>, and then click Create.</li>
<li>Click Add a build configuration for this project.</li>
<li>Enter <kbd>nose testing</kbd> for the name and then click VCS settings.</li>
<li>Click on Create and attach new VCS root.</li>
<li>Enter <kbd>recipe50</kbd> in VCS root name.</li>
<li>Select Git as the Type of VCS.</li>
<li>Enter <kbd>/tmp/recipe50</kbd> as the Fetch URL.</li>
<li>Click on Test Connection to confirm settings and then click Save.</li>
<li>Click on Add Build Step.</li>
<li>Select Command Line for Runner type.</li>
</ol>
<ol start="13">
<li>Select Custom script for Run type and enter the following script:</li>
</ol>
<pre><code class="lang-python">. /Users/gturnquist/ptc/bin/activatenosetests tests.py</code></pre>
<p>Replace this with your own steps to activate your virtualenv and then run the tests using <kbd>nosetests</kbd>.</p>
<ol start="14">
<li>Click on Save.</li>
<li>Click on Build Triggering.</li>
<li>Click on Add new Trigger.</li>
<li>Select Schedule Trigger from Trigger Type.</li>
<li>Pick daily for frequency, and pick a time of about five minutes into the future.</li>
<li>Deselect the option to Trigger build only if there are pending changes.</li>
<li>Click Save.</li>
<li>Navigate back to Projects. There should be no jobs scheduled or results displayed.</li>
<li>Wait for the scheduled time to occur. The following screenshot shows the job when it is activated:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000092.png" class="lazyload" /></p>
<p>The following screenshot shows the results summarized with our tests having passed:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000084.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Doesn't this look suspiciously similar to the previous recipe? Of course! We varied it a bit by creating a time-based trigger instead of a source-based trigger. The time trigger we picked is a daily, scheduled build at a set time. The point is to show a commonly used trigger rule. By seeing what is the same and what's different, we can start to see how to bend TeamCity to serve our needs. TeamCity has other triggers that are very useful, like triggering one job when another one completes. This lets us build lots of small, simple jobs, and chaining them together. We also installed the <kbd>teamcity-nose</kbd> plugin, which gave us more details in the results.</p>
<h3>See also</h3>
<ul>
<li>Generating a CI report for TeamCity using <kbd>teamcity-nose</kbd></li>
<li>Configuring Jenkins to run Python tests when scheduled</li>
</ul>

</div>



<!--Chapter 7-->


<div class="chapter" data-chapter-number="7">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 7 </span></div>
<h1 class="chaptertitle">Measuring Your Success with Test Coverage</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>


<p>In this chapter, we will cover:</p>
<ul>
<li>Building a network management application</li>
<li>Installing and running coverage on your test suite</li>
<li>Generating an HTML report using coverage</li>
<li>Generating an XML report using coverage</li>
<li>Getting nosy with coverage</li>
<li>Filtering out test noise from coverage</li>
<li>Letting Jenkins get nosy with coverage</li>
<li>Updating the project-level script to provide coverage reports</li>
</ul>
<h2>Introduction</h2>

<p><strong>Coverage</strong> <strong>analysis</strong> is measuring which lines in a program are run and which lines aren't. This type of analysis is also known as <strong>code coverage</strong>, or more simply <strong>coverage</strong>.</p>
<p>A coverage analyzer can be used while running a system in production, but what are the pros and cons of this? What about using a coverage analyzer when running test suites? What benefits would this approach provide compared to checking systems in production?</p>

<p>Coverage helps us to see whether we are adequately testing our system. But it must be performed with a certain amount of skepticism. This is because, even if we achieve 100% coverage, meaning every line of our system was run, in no way does this guarantee us having no bugs. Testing can only uncover the presence of bugs. A quick example involves code we have written, and what it processes is the return value from a system call. What if there are three possible values, but we only handle two of them? We may write two test cases covering our handling of it, and this could certainly achieve 100% statement coverage. However, it doesn't mean we have handled the third possible return value, thus leaving us with a potentially undiscovered bug. 100% code coverage can also be obtained by condition coverage, but may not be achieved with statement coverage. The kind of coverage we are planning to target should be clear.</p>
<p>Another key point is that not all testing is aimed at&nbsp;fixing&nbsp;bugs. Another key purpose is to make sure that the application meets our customer's needs. This means that, even if we have 100% code coverage, we can't guarantee that we are covering all the scenarios expected by our users. This is the difference between <em>building it right</em> and <em>building the right thing</em>.</p>
<p>In this chapter, we will explore various recipes to build a network management application, run coverage tools, and harvest the results. We will discuss how coverage can introduce noise and show us more than we need to know, and we'll&nbsp;introduce performance issues when it implements our code. We will also see how to trim out information we don't need to get a concise, targeted view of things.</p>
<p>This chapter uses several third-party tools in many recipes:</p>
<ul>
<li><strong>Spring Python</strong> (<a href="http://springpython.webfactional.com">http://springpython.webfactional.com</a>) contains many useful abstractions. The one used in this chapter is its <kbd>DatabaseTemplate</kbd>, which provides easy ways to write SQL queries and updates without having to deal with Python's verbose API. Install it by typing <kbd>pip install springpython</kbd>.</li>
<li>Install the coverage tool by typing <kbd>pip install coverage</kbd>. This may fail because other plugins may install an older version of coverage. If so, uninstall coverage by typing <kbd>pip uninstall coverage</kbd>, and then install it again with <kbd>pip install coverage</kbd>.</li>
<li>Nose, a useful test runner, is covered in Chapter 2, <em>Running Automated Test Suites with Nose</em>. Refer to that chapter to see how to install nose.</li>
</ul>
<h2>Building a network management application</h2>
<p>For this chapter, we will build a very simple network management application, write different types of test, and check their coverage. This network management application is focused on digesting alarms, also referred to as <strong>network</strong> <strong>events</strong>. This is different from certain other network management tools that focus on gathering SNMP alarms from devices.</p>
<p>For reasons of simplicity, this correlation engine doesn't contain complex rules, but instead contains a simple mapping of network events onto equipment and customer service inventory. We'll explore this in the next few paragraphs as we dig through the code.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will build a simple network management application:</p>
<ol>
<li>Create a file called <kbd>network.py</kbd> to store the network application.</li>
<li>Create a class definition to represent a network event:</li>
</ol>
<pre><code class="lang-python">class Event(object):
   def init  (self, hostname, condition, severity, event_time):
       self.hostname = hostname
       self.condition = condition
       self.severity = severity
       self.id = -1
   def str(self):
       return "(ID:%s) %s:%s - %s" % (self.id, self.hostname,\ 
               self.condition,self.severity)</code></pre>
<p>Let's have a look at few properties of <kbd>self</kbd>:</p>
<ul>
<li><kbd>hostname</kbd>: It is assumed that all network alarms originate from pieces of equipment that have a hostname.</li>
<li><kbd>condition</kbd>: This indicates the type of alarm being generated. Two different alarm conditions can come from the same device.</li>
<li><kbd>severity</kbd>: <kbd>1</kbd> indicates a clear, green status, and <kbd>5</kbd> indicates a faulty, red status.</li>
<li><kbd>id</kbd>: This is the primary key value used when the event is stored in a database.</li>
</ul>
<ol start="3">
<li>Create a new file called <kbd>network.sql</kbd> to contain the SQL code.</li>
</ol>
<ol start="4">
<li>Create a SQL script that sets up the database and adds the definition for storing network events:</li>
</ol>
<pre><code class="lang-python">   CREATE TABLE EVENTS(
       ID INTEGER PRIMARY KEY,
       HOST_NAME TEXT,
       SEVERITY INTEGER,
       EVENT_CONDITION TEXT
       );  </code></pre>
<ol start="5">
<li>Code a high-level algorithm where events are assessed for their impact on equipment and customer services, and add it to <kbd>network.py</kbd>:</li>
</ol>
<pre><code class="lang-python">from springpython.database.core import*

class EventCorrelator(object):
   def init(self, factory):
      self.dt = DatabaseTemplate(factory)
   def del(self):
      del(self.dt)
   def process(self, event):
     stored_event, is_active = self.store_event(event)
     affected_services, affected_equip = self.impact(event)
     updated_services = [
          self.update_service(service, event) 
          for service in affected_services]
     updated_equipment = [
          self.update_equipment(equip, event)
          for equip in affected_equip]
     return (stored_event, is_active,
         updated_services,updated_equipment)</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The&nbsp;<kbd>init</kbd> method contains some setup code for creating a <kbd>DatabaseTemplate</kbd>. This is a Spring Python utility class used for database operations. See&nbsp;<a href="https://docs.spring.io/spring-python/1.2.x/sphinx/html/dao.html">https://docs.spring.io/spring-python/1.2.x/sphinx/html/dao.html</a> for more details. We are also using <em>SQLite3</em> as our database engine, since it is a standard part of Python.&nbsp;</p>
</div>
</div>
<p>The process method contains some simple steps to process an incoming event:</p>
<ul>
<li>
<ul>
<li>We first need to store the event in the <kbd>EVENTS</kbd> table. This includes evaluating whether or not it is an active event, meaning that it is actively impacting a piece of equipment.</li>
<li>Then we determine what equipment and what services the event impacts.</li>
<li>Next, we update the affected service by determining whether it causes any service outages or restorations.</li>
<li>Then we update the affected equipment by determining whether it fails or clears a device.</li>
<li>Finally, we return a tuple containing all the affected assets to support any screen interfaces that could be developed on top of this.</li>
</ul>
</li>
</ul>
<ol start="6">
<li>Implement the <kbd>store_event</kbd> algorithm:</li>
</ol>
<pre><code class="lang-python">def store_event(self,event):
   try:
     max_id = self.dt.query_for_init("""select max(ID) 
        from EVENTS""")
   except DataAccessException, e:
     max_id=0
   event.id = max_id+1
   self.dt.update("""insert into EVENTS
                     (ID, HOST_NAME, SEVERITY, EVENT_CONDITION)
                        values(?,?,?,?)""",
                     (event.id, event.hostname, event.severity,
                        event.condition))
   is active = self.add_or_remove_from_active_events(event)
                     </code></pre>
<p>This method stores every event that is processed. This supports many things, including data mining and the post-mortem analysis of outages. It is also the authoritative place where other event-related data can point back using a foreign key:</p>
<ul>
<li>
<ul>
<li>The <kbd>store_event</kbd> method looks up the maximum primary key value from the <kbd>EVENTS</kbd> table.</li>
<li>It increments it by <kbd>1</kbd>.</li>
<li>It assigns it to <kbd>event.id</kbd>.</li>
<li>It then inserts it into the <kbd>EVENTS</kbd> table.</li>
<li>Next, it calls a method to evaluate whether or not the event should be added to the list of active events, or if it clears out existing active events. Active events are events that are actively causing a piece of equipment to be unclear.</li>
<li>Finally, it returns a tuple containing the event and whether or not it was classified as an active event.</li>
</ul>
</li>
</ul>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>For a more sophisticated system, some sort of partitioning solution needs to be implemented. Querying against a table containing millions of rows is very inefficient. However, this is for demonstration purposes only, so we will skip scaling as well as performance and security.</p>
</div>
</div>
<ol start="7">
<li>Implement a method to evaluate whether to add or remove active events:</li>
</ol>
<pre><code class="lang-python">def add_or_remove_from_active_events(self,event):
    """Active events are current ones that cause equipment
       and\or services to be down."""
    if event.severity == 1:
       self.dt.update ("""DELETE FROM ACTIVE_EVENTS
                          WHERE EVENT_FK in (
                          SELECT ID FROM EVENTS
                          WHERE HOST_NAME=?
                          AND EVENT_CONDITION=?)""",
                       (event.hostname,event.condition))
      return False
    else:
      self.dt.execute ("""INSERT INTO ACTIVE_EVENTS (EVENT_FK) values (?) """, event.id,))
      return True</code></pre>
<p>When a device fails, it sends a <kbd>severity 5</kbd> event. This is an active event, and in this method a row is inserted into the <kbd>ACTIVE_EVENTS</kbd> table with a foreign key pointing back to the <kbd>EVENTS</kbd> table. Then we return&nbsp;<kbd>True</kbd>, indicating that this is an active event.</p>
<ol start="8">
<li>Add the table definition for <kbd>ACTIVE_EVENTS</kbd> to the SQL script:</li>
</ol>
<pre><code class="lang-python">CREATE TABLE ACTIVE_EVENTS(ID INTEGER PRIMARY KEY, EVENT_FK,
    FOREIGN KEY(EVENT_FK) REFERENCES EVENTS(ID)
    );</code></pre>
<p>This table makes it easy to query what events are currently causing equipment failures.</p>
<p>Later, when the failing condition on the device clears, it sends a <kbd>severity 1</kbd> event. This means that <kbd>severity 1</kbd> events are never active, since they aren't contributing &nbsp;to a piece of equipment being down. In our previous method, we search for any active events that have the same hostname and condition, and delete them. Then we return <kbd>False</kbd>, indicating this is not an active event.</p>
<ol start="9">
<li>
<p>Write a method that evaluates the services and pieces of equipment that are affected by the network event:</p>
</li>
</ol>
<pre><code class="lang-python">def impact(self, event):
   """Look up this event has impact on either equipment 
      or services."""
   affected_equipment = self.dt.query(\
               """select * from EQUIPMENT 
                  where HOST_NAME = ?""",
               (event.hostname,), 
               rowhandler=DictionaryRowMapper())
   affected_services = self.dt.query(\
               """select SERVICE.* from   SERVICE
                  join SERVICE_MAPPING SM
                  on (SERVICE.ID = SM.SERVICE_FK)
                  join EQUIPMENT
                  on (SM.EQUIPMENT_FK = EQUIPMENT.ID) where
                  EQUIPMENT.HOST_NAME = ?""",
                  (event.hostname,),                   
                  rowhandler=DictionaryRowMapper())
   return (affected_services, affected_equipment)</code></pre>
<ul>
<li>
<ul>
<li>We first query the <kbd>EQUIPMENT</kbd> table to see if <kbd>event.hostname</kbd> matches anything.</li>
<li>Next, we join the <kbd>SERVICE</kbd> table to the <kbd>EQUIPMENT</kbd> table through a many-to-many relationship tracked by the <kbd>SERVICE_MAPPING</kbd> table. Any service that is related to the equipment that the event was reported on is captured.</li>
<li>Finally, we return a tuple containing both the list of equipment and list of services that are potentially impacted.</li>
</ul>
</li>
</ul>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Spring Python provides a convenient query operation that returns a list of objects mapped to every row of the query. It also provides an out-of-the-box <kbd>DictionaryRowMapper</kbd> that converts each row into a Python dictionary, with the keys matching the column names.</p>
</div>
</div>
<ol start="10">
<li>
<p>Add the table definitions to the SQL script for <kbd>EQUIPMENT</kbd>, <kbd>SERVICE</kbd>, and&nbsp;<kbd>SERVICE_MAPPING</kbd>:</p>
</li>
</ol>
<pre><code class="lang-python">CREATE TABLE EQUIPMENT(
      ID INTEGER PRIMARY KEY, 
      HOST_NAME TEXT UNIQUE,
      STATUS INTEGER );
CREATE TABLE SERVICE (
      ID INTEGER PRIMARY KEY, 
      NAME TEXT UNIQUE, 
      STATUS TEXT );
CREATE TABLE SERVICE_MAPPING (
      ID INTEGER PRIMARY KEY, 
      SERVICE_FK,EQUIPMENT_FK,
      FOREIGN KEY(SERVICE_FK) REFERENCES SERVICE(ID),
      FOREIGN KEY(EQUIPMENT_FK) REFERENCES EQUIPMENT(ID));</code></pre>
<ol start="11">
<li>Write the <kbd>update_service</kbd> method, which stores or clears service-related events and then updates the service's status based on the remaining active events:</li>
</ol>
<pre><code class="lang-python">def update_service(self, service, event):
    if event.severity == 1:
        self.dt.update("""delete from SERVICE_EVENTS
                          where EVENT_FK in (select ID from EVENTS
                          where HOST_NAME = ?
                          and EVENT_CONDITION = ?)""",
                          (event.hostname,event.condition))
    else:
      self.dt.execute("""insert into SERVICE_EVENTS 
                         (EVENT_FK, SERVICE_FK) values (?,?)""",
                         (event.id,service["ID"]))
    try:
      max = self.dt.query_for_int(\
                      """select max(EVENTS.SEVERITY)   
                         from SERVICE_EVENTS SE join EVENTS
                         on (EVENTS.ID = SE.EVENT_FK) join SERVICE
                         on (SERVICE.ID = SE.SERVICE_FK)
                         where SERVICE.NAME = ?""", 
                         (service["NAME"],))
    except DataAccessException, e:
           max=1
    if max &gt; 1 and service["STATUS"] == "Operational":
       service["STATUS"] = "Outage"
       self.dt.update("""update SERVICE
                         set STATUS = ? 
                         where ID = ?""",
                     (service["STATUS"], service["ID"]))

    if max == 1 and service["STATUS"] == "Outage":
       service["STATUS"] = "Operational"
       self.dt.update("""update SERVICE set STATUS = ?
                         where ID = ?""",
                     (service["STATUS"], service["ID"]))
    if event.severity == 1:
       return {"service":service, "is_active":False}
    else:
       return {"service":service, "is_active":True}</code></pre>
<p>Service-related events are active events related to a service. A single event can be related to many services. For example, what if we were monitoring a wireless router that provided internet service to a lot of users, and it reported a critical error? This one event would be mapped as an impact to all the end users. When a new active event is processed, it is stored in <kbd>SERVICE_EVENTS</kbd> for each related service.</p>
<p>Then, when a clearing event is processed, the previous service event must be deleted from the <kbd>SERVICE_EVENTS</kbd> table.</p>
<ol start="12">
<li>Add the table definition for <kbd>SERVICE_EVENTS</kbd> to the SQL script:</li>
</ol>
<pre><code class="lang-python">CREATE TABLE SERVICE_EVENTS ( 
    ID INTEGER PRIMARY KEY, 
    SERVICE_FK,
    EVENT_FK,FOREIGN KEY(SERVICE_FK) REFERENCES SERVICE(ID),
    FOREIGN KEY(EVENT_FK) REFERENCES EVENTS(ID));</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It is important to recognize that deleting an entry from <kbd>SERVICE_EVENTS</kbd> doesn't mean that we delete the original event from the <kbd>EVENTS</kbd> table. Instead, we are merely indicating that the original active event is no longer active and it does not impact the related service.</p>
</div>
</div>
<ol start="13">
<li>Prepend the entire SQL script with <kbd>DROP</kbd> statements, making it possible to run the script for several recipes:</li>
</ol>
<pre><code class="lang-python">DROP TABLE IF EXISTS SERVICE_MAPPING;
DROP TABLE IF EXISTS SERVICE_EVENTS;
DROP TABLE IF EXISTS ACTIVE_EVENTS;
DROP TABLE IF EXISTS EQUIPMENT;
DROP TABLE IF EXISTS SERVICE;
DROP TABLE IF EXISTS EVENTS;</code></pre>
<ol start="14">
<li>Append the SQL script used to set up the database with inserts to pre-load some equipment and services:</li>
</ol>
<pre><code class="lang-python">INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (1,'pyhost1', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (2,'pyhost2', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (3,'pyhost3', 1);
INSERT into SERVICE (ID, NAME, STATUS) values (1, 'service-abc', 'Operational');
INSERT into SERVICE (ID, NAME, STATUS) values (2, 'service-xyz', 'Outage');
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,1);
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,2);
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (2,1);
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (2,3);</code></pre>
<ol start="15">
<li>Finally, write a method that updates the equipment status based on the currently active events:</li>
</ol>
<pre><code class="lang-python">def update_equipment(self,equip,event):
    try:
      max = self.dt.query_for_int(\
                  """select max(EVENTS.SEVERITY) 
                     from ACTIVE_EVENTS AE
                     join EVENTS on (EVENTS.ID = AE.EVENT_FK) 
                     where EVENTS.HOST_NAME = ?""",
                  (event.hostname,))
    except DataAccessException:
        max = 1
    if max != equip["STATUS"]:
         equip["STATUS"] = max 
         self.dt.update("""update EQUIPMENT
                           set STATUS = ?""",
                        (equip["STATUS"],))
    return equip</code></pre>
<p>Here, we need to find the maximum severity from the list of active events for a given host name. If there are no active events, then Spring Python raises a DataAccessException and we translate that to a severity of 1.</p>
<p>We check whether this is different from the existing device's status. If so, we issue a SQL update. Finally, we return the record for the device, with its status updated appropriately.</p>
<h3>How it works...</h3>
<p>This application uses a database-backed mechanism to process incoming network events, and checks them against the inventory of equipment and services to evaluate failures and restorations. Our application doesn't handle specialized devices or unusual types of service. This real-world complexity has been traded in for a relatively simple application that can be used to write various test recipes.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Events typically map to a single piece of equipment and to zero or more services. A service can be thought of as a string of equipment used to provide a type of service to the customer. New failing events are considered active until a clearing event arrives. Active events, when aggregated against a piece of equipment, define its current status. Active events, when aggregated against a service, define the service's current status.</p>
</div>
</div>
<h2>Installing and running coverage on your test suite</h2>
<p>Install the coverage tool and run it against your test suite. Then you can view a report showing what lines were covered by the test suite.</p>
<h3>How to do it...</h3>
<p>With the following steps, we will build some unittests and then run them through the coverage tool:</p>
<ol>
<li>Create a new file called <kbd>recipe52.py</kbd> to contain our test code for this recipe.</li>
<li>Write a simple unit test that injects a single, alarming event into the system:</li>
</ol>
<pre><code class="lang-python">from network import * 
import unittest
from springpython.database.factory import *
from springpython.database.core import *
class EventCorrelationTest(unittest.TestCase):
      def setUp(self):
          db_name = "recipe52.db"
          factory = Sqlite3ConnectionFactory(db_name)
          self.correlator = EventCorrelator(factory)
          dt = DatabaseTemplate(factory)
          sql = open("network.sql").read().split(";")
          for statement in sql:
              dt.execute(statement + ";")
      def test_process_events(self):
          evt1 = Event("pyhost1", "serverRestart", 5)
          stored_event, is_active, \
                updated_services, updated_equipment = \
                self.correlator.process(evt1)
          print "Stored event: %s" % stored_event
          if is_active:
             print "This event was an active event."
             print "Updated services: %s" % updated_services
             print "Updated equipment: %s" % updated_equipment
             print "---------------------------------"
if __name__ == "main":
     unittest.main()          </code></pre>
<ol start="3">
<li>Clear out any existing coverage report data using <kbd>coverage -e</kbd>.</li>
<li>Run the test suite using the coverage tool:</li>
</ol>
<pre><code class="lang-python">gturnquist$ coverage -x recipe52.py
Stored event: (ID:1) pyhost1:serverRestart - 5 This event was an active event.
Updated services: [{'is_active': True, 'service': {'STATUS': 'Outage', 'ID': 1, 'NAME': u'service-abc'}}, {'is_active': True, 'service': {'STATUS': u'Outage', 'ID': 2, 'NAME': u'service- xyz'}}] 
Updated equipment: [{'STATUS': 5, 'ID': 1, 'HOST_NAME': u'pyhost1'}]
---------------------------------

.

------------------------------------------------------------------
----
Ran 1 test in 0.211s OK
</code></pre>
<ol start="5">
<li>Print out the report captured by the previous command by typing <kbd>coverage -r</kbd>. If the report shows several other modules listed from Python's standard libraries, it's a hint that you have an older version of the coverage tool installed. If so, uninstall the old version by typing <kbd>pip uninstall coverage</kbd> followed by reinstalling with <kbd>pip install coverage</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000076.png" class="lazyload" /></p>
<ol start="5">
<li>
<p>Create another file called <kbd>recipe52b.py</kbd> to contain a different test suite.</p>
</li>
<li>
<p>Write another test suite that generates two faults and then clears them out:</p>
</li>
</ol>
<pre><code class="lang-python">from network import*
import unittest
from springpython.database.factory import*
from springpython.database.core import*
class EventCorrelationTest(unittest.TestCase):
      def setUp(self):
          db_name = "recipe52b.db"
          factory = Sqlite3ConnectionFactory(db=db_name)
          self.correlator = EventCorrelator(factory)
          dt = DatabaseTemplate(factory)
          sql = open("network.sql").read().split(";")
          for statement in sql:
             dt.execute(statement + ";")
      def test_process_events(self):
          evt1 = Event("pyhost1", "serverRestart", 5)
          evt2 = Event("pyhost2", "lineStatus", 5)
          evt3 = Event("pyhost2", "lineStatus", 1)
          evt4 = Event("pyhost1", "serverRestart", 1)
          for event in [evt1, evt2, evt3, evt4]:
              stored_event, is_active, \ 
                 updated_services, updated_equipment = \
                  self.correlator.process(event)
              print "Stored event: %s" % stored_event
              if is_active:
                print "This event was an active event."
                print "Updated services: %s" % updated_services
                print "Updated equipment: %s" % updated_equipment
                print "---------------------------------"
  if __name__ == "__main__": 
     unittest.main()</code></pre>
<ol start="8">
<li>Run this test suite through the coverage tool using <kbd>coverage -x recipe52b.py</kbd>.</li>
<li>Print out the report by typing <kbd>coverage -r</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000067.png" class="lazyload" /></p>
<p>The first test suite only injects a single alarm. We expect it to cause a service outage and take its related piece of equipment down. Since this would <em>not</em> exercise any event clearing logic, we certainly don't expect 100% code coverage.</p>
<p>In the report, we can see that it says&nbsp;<kbd>network.py</kbd>&nbsp;had 65 statements, and has executed 55 of them, resulting in 85% coverage. We also see that <kbd>recipe52.py</kbd> had 23 statements and executed all of them. This means all of our test code ran.</p>
<p>At this point, we realize that we are only testing the alarming part of the event correlator. To make this more effective, we should inject another alarm, followed by a couple of clears to make sure that everything clears out and the services return to operational status. This should result in 100% coverage in our simple application.</p>
<p>The second screenshot indeed shows that we have reached full coverage of <kbd>network.py</kbd>.</p>
<h3>There's more...</h3>
<p>We also see Spring Python reported as well. If we had used any other third-party libraries, then they would also appear. Is this right? It depends. The previous comments seem to indicate that we don't really care about coverage of Spring Python, but in other situations, we might be very interested. And how can the coverage tool know where to draw the line?</p>
<p>In later recipes, we will look into how to be more selective of what to measure so we can filter out the noise.</p>
<h4>Why are there no asserts in unittest?</h4>

<p>It is true that unittest isn't adequate with regard to testing outcomes. To draw up this recipe, I visually inspected the output to see whether the network management application was performing as expected. But this is incomplete. A real production grade unit test needs to finish this with a set of assertions so that visual scanning is not needed.</p>

<p>So why didn't we code any? Because the focus of this recipe was on how to generate a coverage report and then use that information to enhance the testing. We covered both of these things. By thinking about what was and wasn't tested, we wrote a comprehensive test that shows services going into outage and back to operational status. We just didn't just confirm this automatically.</p>

<h2>Generating an HTML report using coverage</h2>
<p>Using the coverage tool, generate an HTML visual coverage report. This is useful because we can drill into the source code and see what lines did not run in the test procedures.</p>
<p>Reading a coverage report without reading the source code is not very useful. It may be tempting to compare two different projects based on the coverage percentages. But unless the actual code is analyzed, this type of comparison can lead to faulty conclusions about the quality of the software.</p>
<h3>How to do it...</h3>
<p>With these steps, we will explore creating a nicely viewable HTML coverage report:</p>
<ol>
<li>Generate coverage metrics by following the steps in the&nbsp;<em>Installing and running coverage</em> <em>on your test suite</em> recipe and only running the first test suite (which resulted in less than 100% coverage).</li>
<li>Generate an HTML report by typing&nbsp;<kbd>coverage.html</kbd>.</li>
</ol>
<ol start="3">
<li>Open <kbd>htmlcov/index.html</kbd> using your favorite browser and inspect the overall report:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000060.png" class="lazyload" /></p>
<ol start="3">
<li>Click on&nbsp;network, and scroll down to see where the event clearing logic didn't run due to no clearing events being processed:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000050.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The coverage tool has a built-in feature to generate an HTML report. This provides a powerful way to visually inspect the source code and see which lines were not executed.</p>
<p>By looking at this report, we can clearly see that the lines that were not executed relate to the lack of clearing network events that are being processed. This tips us off about another test case, which involves clearing events that need to be drafted.</p>
<h2>Generating an XML report using coverage</h2>
<p>The coverage tool can generate an XML coverage report in Cobertura format (<a href="http://cobertura.sourceforge.net/">http://cobertura.sourceforge.net/</a>). This is useful if we want to process the coverage information in another tool. In this recipe, we will see how to use the coverage command-line tool, and then view the XML report by hand.</p>
<p>It's important to understand that reading a coverage report without reading the source code &nbsp;&nbsp;is not very useful. It may be tempting to compare two different projects based on the coverage percentages. But unless the actual code is analyzed, this type of comparison can lead to &nbsp;faulty conclusions about the quality of the software.</p>
<p>For example, a project with 85% coverage may appear, on the surface, to be better tested than one with 60%. However, if the 60% application has much more thoroughly exhaustive scenarios&mdash;as they are only covering the core parts of the system that are in heavy use&mdash;then it may be much more stable than the 85% application.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Coverage analysis serves a useful purpose when comparing test results between iterations, and when we use it to decide which scenarios need to be added to our testing repertoire.</p>
</div>
</div>
<h3>How to do it...</h3>
<p>With these steps, we will discover how to create an XML report that is consumable by other tools,&nbsp;using the coverage tool:</p>
<ol>
<li>Generate coverage metrics by following the steps in <em>Installing and running coverage</em> <em>on your test suite</em> recipe (mentioned in Chapter 1, <em>Using Unittest to Develop Basic</em> <em>Tests</em>) and only running the first test suite (which resulted in less than 100% coverage).</li>
<li>Generate an XML report by typing&nbsp;<kbd>coverage xml</kbd>.</li>
<li>Open <kbd>coverage.xml</kbd> using your favorite text or XML editor. The format of the XML is the same as Cobertura&mdash;a Java code coverage analyzer. This means that many tools, such as Jenkins, can parse the results:
<p></p>
</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000038.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The coverage tool has a built-in feature that generates an XML report. This provides a powerful way to parse the output using some type of external tool.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>In the previous screenshot, I opened it using Spring Tool Suite (you can download it from <a href="http://www.springsource.com/developer/sts)">http://www.springsource.com/developer/sts)</a>, partly because I happen to use STS every day, but you can use any text or XML editor you like.</p>
</div>
</div>
<h4>What use is an XML report?</h4>

<p>XML is not the best way to communicate coverage information to users. <em>Generating an HTML</em> <em>report with coverage</em> is a more practical recipe when it comes to human users.</p>
<p>What if we want to capture a coverage report and publish it inside a continuous integration system such as Jenkins? All we need to do is install the Cobertura plugin (refer to&nbsp;<a href="https://wiki.jenkins-ci.org/display/JENKINS/Cobertura+Plugin">https://wiki.jenkins-ci.org/display/JENKINS/Cobertura+Plugin</a>), and this report becomes traceable. Jenkins can nicely monitor trends in coverage and give us more feedback as we develop our system.</p>

<h3>See also</h3>
<ul>
<li><em>Letting Jenkins get nosy with coverage</em></li>
<li><em>Generating an HTML report using coverage</em></li>
</ul>
<h2>Getting nosy with coverage</h2>
<p>Install the coverage nose plugin, and run your test suite using nose. This provides a quick and convenient report using the ubiquitous nosetests tool. This recipe assumes you have already created the network management application as described in the <em>Building a network</em> <em>management application</em> section.</p>
<h3>How to do it...</h3>
<p>With these steps, we will see how to combine the coverage tool with nose:</p>
<ol>
<li>Create a new file called <kbd>recipe55.py</kbd> to store our test code.</li>
<li>Create a test case that injects a faulty alarm:</li>
</ol>
<pre><code class="lang-python">from network import*
import unittest
from springpython.database.factory import*
from springpython.database.core import*
class EventCorrelationTest(unittest.TestCase):
      def setUp(self):
         db_name = "recipe55.db"
         factory = Sqlite3ConnectionFactory(db=db_name)
         self.correlator = EventCorrelator(factory)
         dt = DatabaseTemplate(factory)
         sql = open("network.sql").read().split(";")
         for statement in sql:
            dt.execute(statement + ";")
      def test_process_events(self):
         evt1 = Event("pyhost1", "serverRestart", 5)
         stored_event, is_active, \ 
              updated_services, updated_equipment = \
         self.correlator.process(evt1)
         print "Stored event: %s" % stored_event
         if is_active:
            print "This event was an active event."
            print "Updated services: %s" % updated_services
            print "Updated equipment: %s" % updated_equipment
            print "---------------------------------"</code></pre>
<ol start="3">
<li>Run the test module using the coverage plugin by typing <kbd>nosetests recipe55 &ndash; with-coverage</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000029.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>The nose plugin for coverage invokes the coverage tool and provides a formatted report. For each module, it displays the following:</p>
<ul>
<li>Total number of statements</li>
<li>Number of missed statements</li>
<li>Percentage of covered statements</li>
<li>Line numbers for the missed statements</li>
</ul>
<h3>There's more...</h3>
<p>A common behavior of nose is to alter <kbd>stdout</kbd>, disabling the <kbd>print</kbd> statements embedded in the test case.</p>
<h4>Why use the nose plugin instead of the coverage tool directly?</h4>
<p>The coverage tool works fine by itself, as was demonstrated in other recipes in this chapter. However, nose is a ubiquitous testing tool used by many developers. Providing a plugin makes it easy to support this vast community by empowering users to run the exact set of test plugins they want, with coverage being part of that test complement.</p>
<h4>Why are SQLite3 and Spring Python included?</h4>
<p>SQLite3 is a relational database library that is included with Python. It is file-based, which means that no separate processes are required to create and use a database. Details about Spring Python can be found in the earlier sections of this chapter.</p>
<p>The purpose of this recipe was to measure the coverage of our network management application and the corresponding test case. So why are these third-party libraries included? The coverage tool has no way of automatically knowing what we want and the things we don't want to see from a coverage perspective. To delve into this, refer to the next section,&nbsp;<em>Filtering out</em> <em>test noise from coverage</em>.</p>
<h2>Filtering out test noise from coverage</h2>
<p>Using command-line options, you can filter out counted lines. This recipe assumes you have already created the network management application as described in the <em>Building a network</em> <em>management application</em> section.</p>
<h3>How to do it...</h3>
<p>With these steps, we will see how to filter out certain modules from being counted in our coverage report:</p>
<ol>
<li>Create a test suite that exercises all the code functionality:</li>
</ol>
<pre><code class="lang-python">from network import*
import unittest
from springpython.database.factory import*
from springpython.database.core import *
class EventCorrelationTest(unittest.TestCase):
   def setUp(self):
      db_name = "recipe56.db"
      factory = Sqlite3ConnectionFactory(db=db_name)
      self.correlator = EventCorrelator(factory)
      dt = DatabaseTemplate(factory)
      sql = open("network.sql").read().split(";")
      for statement in sql:
        dt.execute(statement + ";")
   def test_process_events(self):
       evt1 = Event("pyhost1", "serverRestart", 5)
       evt2 = Event("pyhost2", "lineStatus", 5)
       evt3 = Event("pyhost2", "lineStatus", 1)
       evt4 = Event("pyhost1", "serverRestart", 1)
       for event in [evt1, evt2, evt3, evt4]:
           stored_event, is_active,\
              updated_services, updated_equipment=\
                  self.correlator.process(event)
           print "Stored event: %s" % stored_event
       if is_active:
          print "This event was an active event."
          print "Updated services: %s" % updated_services
          print "Updated equipment: %s" % updated_equipment
          print "---------------------------------"
if __name__=="__main__":
   unittest.main()</code></pre>
<ol start="2">
<li>Clear out any previous coverage data by running <kbd>coverage -e</kbd>.</li>
<li>Run it using <kbd>coverage -x recipe56.py</kbd>.</li>
<li>Generate a console report using <kbd>coverage -r</kbd>. In the following screenshot, observe how Spring Python is included in the report and reduces the total metric to 73%:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000031.png" class="lazyload" /></p>
<ol start="2">
<li>Clean out coverage data by running <kbd>coverage -e</kbd>.</li>
<li>Run the test again using <kbd>coverage run &ndash;source network.py,recipe56.py,recipe56.py</kbd>.</li>
</ol>
<ol start="4">
<li>Generate another console report using <kbd>coverage -r</kbd>. Notice in the next screenshot how Spring Python is no longer listed, bringing our total coverage back up to 100%:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000009.png" class="lazyload" /></p>
<ol start="2">
<li>Clean out coverage data by running <kbd>coverage -e</kbd>.</li>
<li>Run the test using <kbd>coverage -x recipe56.py</kbd>.</li>
<li>Generate a console report using <kbd>coverage -r recipe56.py network.py</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000155.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Coverage provides the ability to decide which files will be analyzed and what files will be reported. The steps in the previous section gather metrics several times, either by running coverage with a restricted set of source files (in order to filter out Spring Python), or by requesting an explicit set of modules in the report.</p>
<p>One question that arises from all this is,&nbsp;<em>What's the best choice?</em> For our test scenario, the two choices were equivalent. With approximately the same amount of typing, we filtered out Spring Python and got a report showing <kbd>network.py</kbd> and <kbd>recipe56.py</kbd> with 100% coverage either way. However, a real project with a lot of modules and possibly different teams working in different areas would probably do better by gathering all the metric data available and filtering at the report level.</p>
<p>This way, different reports on subsystems can be run as needed without having to keep recapturing metric data, and an overall report can still be run for the whole system coverage, all from the same gathered data.</p>
<h3>There's more...</h3>

<p>The options used in the previous section were inclusive. We picked what was to be included. The coverage tool also comes with an <kbd>&ndash;omit</kbd> option. The challenge is that it's a file-based option, not module-based. It doesn't work to use&nbsp;<kbd>&ndash;omit springpython</kbd>. Instead, every file must be specified, and in this case that would have required four complete files to exclude it all.</p>
<p>To further complicate this, the full paths for the Spring Python files need to be included. This results in a very lengthy command, not providing much of a benefit over the ways we demonstrated.</p>
<p>In other situations, if the file to be excluded is local to where coverage is being run, then it might be more practical.</p>
<p>The coverage tool has other options not covered in this chapter, such as measuring branch coverage instead of statement coverage, excluding lines, and the ability to run in parallel to manage collecting metrics from multiple processes.</p>

<p>As mentioned previously, the coverage tool has the ability to filter out individual lines. In my opinion, this sounds very much like trying to get the coverage report to meet some mandated percentage. The coverage tool is best used to work towards writing more comprehensive tests, fixing bugs, and improving development, and not towards building a better report.</p>
<h3>See also</h3>
<p>The <em>Building a network management application</em> recipe earlier in the chapter</p>
<h2>Letting Jenkins get nosy with coverage</h2>
<p>Configure Jenkins to run a test suite using nose, generating a coverage report. This recipe assumes you have already created the network management application as described in the <em>Building a network management application</em> section.</p>
<h3>Getting ready</h3>
<p>Let's look at the following steps:</p>
<ol>
<li>If you have already downloaded Jenkins and used it for previous recipes, look for a <kbd>.jenkins</kbd> folder in your home directory and delete it, to avoid unexpected variances caused by this recipe.</li>
<li>Install Jenkins.</li>
<li>Open the console to confirm that Jenkins is working:
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000145.png" class="lazyload" /></p>
</li>
</ol>
<ol start="4">
<li>Click on Manage Jenkins.</li>
<li>Click on Manage Plugins.</li>
<li>Click on the Available tab.</li>
<li>Find the <kbd>Cobertura Plugin</kbd> and click the checkbox next to it.</li>
<li>Find the <kbd>Git Plugin</kbd> and click the checkbox next to it.</li>
<li>At the bottom of the page, click on the Install button.</li>
<li>Navigate back to the dashboard screen.</li>
<li>Shut down Jenkins and start it again.</li>
<li>Install Git source code control on your machine.</li>
<li>Create an empty folder for this recipe:</li>
</ol>
<pre><code class="lang-python">gturnquist$ mkdir /tmp/recipe57</code></pre>
<ol start="14">
<li>Initialize the folder for source code maintenance:</li>
</ol>
<pre><code class="lang-python">gturnquist$ git init /tmp/recipe57</code></pre>
<ol start="15">
<li>Copy the network application and SQL script into the folder, add it, and commit the changes:</li>
</ol>
<pre><code class="lang-python">gturnquist$ cp network.py /tmp/recipe57/ 
gturnquist$ cp network.sql /tmp/recipe57/ 
gturnquist$ cd /tmp/recipe57/
gturnquist$ git add network.py network.sql
gturnquist$ git commit -m "Add network app"
[master (root-commit) 7f78d46] Add network app
2 files changed, 221 insertions(+), 0 deletions(-)
create mode 100644 network.py
create mode 100644 network.sql
</code></pre>
<h3>How to do it...</h3>
<p>With these steps, we will explore how to configure Jenkins to build a coverage report and&nbsp;serve it through Jenkins' interface.</p>
<ol>
<li>Create a new file called <kbd>recipe57.py</kbd> to contain our test code for this recipe.</li>
<li>Write a test case that partially exercises the network management application:</li>
</ol>
<pre><code class="lang-python">from network import*
import unittest
from springpython.database.factory import*
from springpython.database.core import*
class EventCorrelationTest(unittest.TestCase):
    def setUp(self):
        db_name = "recipe57.db"
        factory = Sqlite3ConnectionFactory(db=db_name)
        self.correlator = EventCorrelator(factory)
        dt = DatabaseTemplate(factory)
        sql = open("network.sql").read().split(";")
        for statement in sql:
           dt.execute(statement + ";")
    def test_process_events(self):
        evt1 = Event("pyhost1", "serverRestart", 5)
        stored_event, is_active, updated_services, updated_equipment = \
            self.correlator.process(evt1)
        print "Stored event: %s" % stored_event
        if is_active:
        print "This event was an active event."
        print "Updated services: %s" % updated_services
        print "Updated equipment: %s" % updated_equipment
        print "---------------------------------"</code></pre>
<ol start="3">
<li>Copy it into the source code repository. Add it and commit the changes:</li>
</ol>
<pre><code class="lang-python">gturnquist$ cp recipe57.py /tmp/recipe57/
gturnquist$ cd /tmp/recipe57/
gturnquist$ git add recipe57.py
gturnquist$ git commit -m "Added tests."
[master 0bf1761] Added tests.
1 files changed, 37 insertions(+), 0 deletions(-)
create mode 100644 recipe57.py</code></pre>
<ol start="4">
<li>Open the Jenkins console.</li>
<li>Click on New Job.</li>
<li>Enter <kbd>recipe57</kbd> as the Job Name and pick Build a free-style software project.</li>
<li>Click on <strong>Ok</strong>.</li>
<li>In the <strong>Source Code Management</strong> section, select&nbsp;<strong>Git</strong>. For <strong>URL</strong>, enter <kbd>/tmp/ recipe57/</kbd>.</li>
<li>In the Build Triggers section, pick <strong>Poll</strong> <strong>SCM</strong> and enter <kbd>* * * * *</kbd> into the schedule box in order to trigger a poll once a minute.</li>
<li>In the Build section, select <strong>Execute</strong> <strong>Shell</strong> and enter the following script, which loads virtualenv and runs the test suite:</li>
</ol>
<pre><code class="lang-python">. /Users/gturnquist/ptc/bin/activate
coverage -e
coverage run /Users/gturnquist/ptc/bin/nosetests
recipe57.py coverage xml --include=network.py,recipe57.py</code></pre>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>You need to include the step to activate your virtualenv and then run the coverage tool, as shown in the following steps.</p>
</div>
</div>
<ol start="11">
<li>In the Post-build Actions section, pick <strong>Publish Cobertura Coverage Report</strong>.</li>
<li>Enter <kbd>coverage.xml</kbd> for <strong>Cobertura xml report pattern</strong>.</li>
<li>Click on <strong>Save</strong> to store all job settings.</li>
<li>Navigate back to the dashboard.</li>
<li>Click on <strong>Enable Auto Refresh</strong>.</li>
<li>Wait about a minute for the build job to run:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000135.png" class="lazyload" /></p>
<ol start="11">
<li>Click on results (<strong>#1</strong> in the previous screenshot).</li>
</ol>
<ol start="18">
<li>Click on <strong>Coverage</strong> <strong>Report</strong>. Observe the next screenshot where it reports 89% coverage:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000127.png" class="lazyload" /></p>
<ol start="18">
<li>Click on module <kbd>.</kbd> (dot) to see <kbd>network.py</kbd> and <kbd>recipe57.py</kbd>.</li>
<li>Click on <kbd>recipe57.py</kbd> to see which lines were covered and which ones were missed.</li>
</ol>
<h3>How it works...</h3>

<p>The coverage tool generates a useful XML file that the Jenkins Cobertura plugin can harvest. It's possible to just generate the HTML report and serve it up through Jenkins, but the XML file allows Jenkins to nicely chart the coverage trend. It also provides the means to drill-down and view the source code along with lines covered and missed.</p>
<p>We also integrated it with source control so that, as changes are committed to the repository, new jobs will be run.</p>

<h3>There's more...</h3>
<p>It's important not to get too wrapped up in coverage reports. The coverage tool is useful to track testing, but working purely to increase coverage doesn't guarantee building better code. It should be used as a tool to illuminate what test scenarios are missing instead of thinking about testing a missing line of code.</p>
<h4>Nose doesn't directly support coverage's XML option</h4>
<p>The nose plugin for the coverage tool doesn't include the ability to generate XML files. This is because the coverage plugin is part of nose and <em>not</em> part of the coverage project. It is not up to date with the latest features, including XML reports.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>I asked Ned Batchelder, the creator of the coverage project, about this lack of XML support from nose. He recommended that I run <kbd>nosetests</kbd> inside <kbd>coverage</kbd>, as shown previously in the Jenkins job. It generates the same&nbsp;<kbd>.coverage</kbd> trace data file. It is easy to then execute <kbd>coverage</kbd> <kbd>xml</kbd> with the required arguments in order to get our desired report. In fact, we can use any&nbsp;of coverage's&nbsp;reporting features at this stage. Unfortunately, the coverage tool needs the explicit path to <kbd>nosetests</kbd>, and running inside Jenkins requires the path to be spelled out.</p>
</div>
</div>
<h2>Updating the project-level script to provide coverage reports</h2>
<p>Update the project-level script to generate HTML, XML, and console coverage reports as runnable options.</p>
<h3>Getting ready</h3>
<ul>
<li>Install coverage by typing <kbd>pip install coverage</kbd></li>
<li>Create the network management application described in the <em>Building a network</em> <em>management application</em> recipe</li>
</ul>
<h3>How to do it...</h3>
<p>With these steps, we will explore how to use coverage programmatically in a project management script:</p>
<ol>
<li>Create a new file called <kbd>recipe58.py</kbd> to store this command-line script.</li>
<li>Create a script that uses <kbd>getopt</kbd> to parse command-line arguments:</li>
</ol>
<pre><code class="lang-python">import getopt
import logging
import nose 
import os
import os.path
import re
import sys
from glob import glob
def usage():
print
print "Usage: python recipe58.py [command]"
print "\t--help"
print "\t--test"
print "\t--package"
print "\t--publish"
print "\t--register"
print
try:
 optlist, args = getopt.getopt(sys.argv[1:],
 "h",
 ["help", "test", "package", "publish", "register"])
except getopt.GetoptError:
# print help information and exit:
 print "Invalid command found in %s" % sys.argv
 usage()
 sys.exit(2)</code></pre>
<ol start="3">
<li>Add a test function that uses coverage's API to gather metrics and then generate a console report, an HTML report, and an XML report, while also using nose's API to run the tests:</li>
</ol>
<pre><code class="lang-python">def test():
   from coverage import coverage
   cov = coverage() cov.start()
   suite = ["recipe52", "recipe52b", "recipe55", "recipe56", "recipe57"]
   print("Running suite %s" % suite)
   args = [""]
   args.extend(suite)
   nose.run(argv=args)
   cov.stop()
   modules_to_report = [module + ".py" for module in suite]
   modules_to_report.append("network.py")
   cov.report(morfs=modules_to_report)
   cov.html_report(morfs=modules_to_report, \
                         directory="recipe58")
   cov.xml_report(morfs=modules_to_report, \
                        outfile="recipe58.xml")      </code></pre>
<ol start="4">
<li>Add some other stubbed out functions to simulate packaging, publishing, and registering this project:</li>
</ol>
<pre><code class="lang-python">def package():
  print "This is where we can plug in code to run " + \ 
        "setup.py to generate a bundle."
def publish():
  print "This is where we can plug in code to upload " +\
        "our tarball to S3 or some other download site."
def publish():
  print "This is where we can plug in code to upload " +\
        "our tarball to S3 or some other download site."
def register():
  print "setup.py has a built in function to " + \
        "'register' a release to PyPI. It's " + \
        "convenient to put a hook in here."
# os.system("%s setup.py register" % sys.executable)</code></pre>
<ol start="5">
<li>Add code that processes command-line arguments and calls the functions defined earlier:</li>
</ol>
<pre><code class="lang-python">if len(optlist) == 0:
   usage()
   sys.exit(1)
# Check for help requests, which cause all other
# options to be ignored. for option in optlist:
if option[0] in ("--help", "-h"):
    usage()
    sys.exit(1)
# Parse the arguments, in order for option in optlist:
if option[0] in ("--test"):
   test()
if option[0] in ("--package"):
   package()
if option[0] in ("--publish"):
   publish()
if option[0] in ("--register"):
   </code></pre>
<ol start="6">
<li>Run the script using the <kbd>--test</kbd> option:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000116.png" class="lazyload" /></p>
<ol start="7">
<li>Open the HTML report using your favorite browser:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000107.png" class="lazyload" /></p>
<ol start="6">
<li>Inspect <kbd>recipe58.xml</kbd>.</li>
</ol>
<h3>How it works...</h3>
<p>The coverage API is easy to use as shown in the following steps:</p>
<ol>
<li>In the test method, we create a <kbd>coverage()</kbd> instance:</li>
</ol>
<pre><code class="lang-python">from coverage import coverage
cov = coverage()</code></pre>
<ol start="2">
<li>We need to call the <kbd>start</kbd> method to begin tracing:</li>
</ol>
<pre><code class="lang-python">cov.start()</code></pre>
<ol start="3">
<li>Next, we need to exercise the main code. In this case, we are using the nose API. We will use it to run the various recipes coded in this chapter:</li>
</ol>
<pre><code class="lang-python">suite = ["recipe52", "recipe52b", "recipe55", "recipe56", "recipe57"]
print("Running suite %s" % suite) 
args = [""]
args.extend(suite)
nose.run(argv=args)</code></pre>
<ol start="4">
<li>Then we need to stop coverage from tracing:</li>
</ol>
<pre><code class="lang-python">cov.stop()</code></pre>
<ol start="5">
<li>Now that we have gathered metrics, we can generate a console report, an HTML report, and an XML report:</li>
</ol>
<pre><code class="lang-python">modules_to_report = [module + ".py" for module in suite] 
modules_to_report.append("network.py")
cov.report(morfs=modules_to_report)
cov.html_report(morfs=modules_to_report, directory="recipe58")
cov.xml_report(morfs=modules_to_report, outfile="recipe58.xml")</code></pre>
<p>The first report is a console report. The second report is an HTML report written into the&nbsp;<kbd>recipe58</kbd>&nbsp;subdirectory. The third report is an XML report in Cobertura format written to <kbd>recipe58.xml</kbd>.</p>
<h3>There's more...</h3>
<p>There are many more options to fine-tune gathering as well as reporting. Just visit the online documentation at <a href="http://nedbatchelder.com/code/coverage/api.html">http://nedbatchelder.com/code/coverage/api.html</a> for more details.</p>
<h4>Can we only use getopt?</h4>

<p>Python 2.7 introduced&nbsp;<kbd>argparse</kbd> as an alternative to <kbd>getopt</kbd>. The current documentation gives no indication that <kbd>getopt</kbd> is deprecated, so it's safe to use as we have just done. The <kbd>getopt</kbd> module is a nice, easy-to-use command-line parser.</p>

</div>


<!--Chapter 8-->


<div class="chapter" data-chapter-number="8">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 8 </span></div>
<h1 class="chaptertitle">Smoke/Load Testing &ndash; Testing Major Parts</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>


<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Defining a subset of test cases using import statements</li>
<li>Leaving out integration tests</li>
<li>Targeting end to end scenarios</li>
<li>Targeting the test server</li>
<li>Coding a data simulator</li>
<li>Recording and playing back live data in real time</li>
<li>Recording and playing back live data as fast as possible</li>
<li>Automating your management demo</li>
</ul>
<h2>Introduction</h2>
<p>Smoke testing is not very widely embraced by teams that write automated tests. Writing tests to verify things are working or to expose bugs is a commonly adopted practice, and many teams pick up the idea of using acceptance testing to verify whether their applications are meeting customer demands.</p>
<p>But smoke testing is a little different. One of the key ideas with smoke testing is to see whether the system has a pulse. What does this mean? It's similar to when a doctor first sees a patient. The first thing they do is check the patient's pulse, along with other vital signs. No pulse; critical pulse! So, what exactly in software constitutes a pulse? That is what we'll explore in the recipes in this chapter.</p>
<p>Instead of thinking about comprehensive test suites that make sure every corner of the system has been checked, smoke testing takes a much broader perspective. A set of smoke tests is meant to make sure the system is up and alive. It's almost like a ping check. Compare it to sanity tests. Sanity tests are used to prove a small set of situations actually work. Smoke testing, which is similar in the sense that it is quick and shallow, is meant to see whether the system is in an adequate state to proceed with more extensive testing.</p>
<p>If you imagine an application built to ingest invoices, a set of smoke tests could include the following:</p>
<ul>
<li>Verify the test file has been consumed</li>
<li>Verify the number of lines parsed</li>
<li>Verify the grand total of the bill</li>
</ul>
<p>Does this sound like a small set of tests? Is it incomplete? Yes it is. And that's the idea. Instead of verifying our software parsed everything correctly, we are verifying just a few key areas that <em>must</em> be working. If it fails to read one file, then there is a major issue that needs to be addressed. If the grand total of the bill is incorrect, again, something big must be taken care of.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>A key side effect of smoke testing is that these tests should be quick to run. What if we altered the function that handles files? If our test suite involves parsing lots of different file types, it could take a long time to verify we didn't break anything. Instead of spending 30 minutes to run a comprehensive test suite, wouldn't it be better to run a one minute quick test and then spend the other 29 minutes working on the software?</p>
</div>
</div>
<p>Smoke tests are also good to use when preparing for a customer demo. With the tension turned up, it's good to run tests more often to make sure we haven't broken anything. Before launching a demo, one last pulse check to know the system is alive may be needed.</p>
<p>This chapter also dives into load testing. Load testing is crucial to verify whether our applications can handle the strain of real-world situations. This often involves collecting real-world data and playing it back through our software for a reproducible environment. While we need to know our system can handle today's load, how likely is it that tomorrow's load will be the same? Not very likely.</p>
<p>It is very useful to seek out the next bottleneck in our application. That way, we can work towards eliminating it before we hit that load in production. One way to stress the system is to play back real-world data as quickly as possible.</p>
<p>In this chapter, we will look at some recipes in which we both smoke test and load test the network management application. The types of load we will be placing on the application could also be described as <strong>soak testing</strong> and <strong>stress testing</strong>. <strong>Soak testing</strong> is described as putting a significant load on the system over a significant period of time. <strong>Stress testing</strong> is described as loading down a system until it breaks.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>In my opinion, soak testing and stress testing are different sides of the same coin of load testing. That is why this chapter simply uses the term load testing when the various recipes can easily extend to these types of testing.</p>
</div>
</div>
<p>The code in this chapter also uses several utilities provided by Spring Python (<a href="http://springpython.webfactional.com">http://springpython.webfactional.com</a>).</p>
<p>Many of the recipes in this chapter interact with a MySQL database. Install the Python MySQLdb library by typing&nbsp;<kbd>pip install mysql-python</kbd>.</p>
<p>Several of the recipes in this chapter use <strong>Python Remote Objects</strong> (<strong>Pyro</strong>) (<a href="http://www.xs4all.nl/~irmen/pyro3/">http://www.xs4all.nl/~irmen/pyro3/</a>). It is a <strong>Remote</strong><strong>Procedure</strong><strong>Call</strong> (<strong>RPC</strong>) library that supports communicating between threads and processes. Install Pyro by typing&nbsp;<kbd>pip install pyro</kbd>.</p>
<h2>Defining a subset of test cases using import statements</h2>
<p>Create a Python module that selectively imports which test cases to run.</p>
<h3>How to do it...</h3>
<p>With these steps, we will explore selectively picking a smaller set of tests to facilitate a faster test run:</p>
<ol>
<li>Create a test module called <kbd>recipe59_test.py</kbd>, which will be used to write some tests against our network application, as shown here:</li>
</ol>
<pre><code class="lang-python">import logging
from network import *
import unittest
from springpython.database.factory import *
from springpython.database.core import *</code></pre>
<ol start="2">
<li>Create a test case that removes the database connection and stubs out the data access functions, as shown here:</li>
</ol>
<pre><code class="lang-python">class EventCorrelatorUnitTests(unittest.TestCase):
def setUp(self):
  db_name = "recipe59.db"
  factory = Sqlite3ConnectionFactory(db=db_name)
  self.correlator = EventCorrelator(factory)
  # We "unplug" the DatabaseTemplate so that
  # we don't talk to a real database.
  self.correlator.dt = None
  # Instead, we create a dictionary of
  # canned data to return back
  self.return_values = {}
  # For each sub-function of the network app,
  # we replace them with stubs which return our
  # canned data.
def stub_store_event(event):
  event.id = self.return_values["id"]
  return event, self.return_values["active"]
  self.correlator.store_event = stub_store_event
def stub_impact(event):
  return (self.return_values["services"],
  self.return_values["equipment"])
  self.correlator.impact = stub_impact
def stub_update_service(service, event):
  return service + " updated"self.correlator.update_service = 
  tub_update_service

def stub_update_equip(equip, event):
  return equip + " updated"
  self.correlator.update_equipment = stub_update_equip</code></pre>
<ol start="3">
<li>Create a test method that creates a set of canned data values, invokes the application's process method, and then verifies the values, as shown here:</li>
</ol>
<pre><code class="lang-python">def test_process_events(self):
  # For this test case, we can preload the canned data,
  # and verify that our process function is working
  # as expected without touching the database.
  self.return_values["id"] = 4668
  self.return_values["active"] = True
  self.return_values["services"] = ["service1",
                                    "service2"]
  self.return_values["equipment"] = ["device1"]
  evt1 = Event("pyhost1", "serverRestart", 5)
  stored_event, is_active,
  updated_services, updated_equipment =
  self.correlator.process(evt1)
  self.assertEquals(4668, stored_event.id)
  self.assertTrue(is_active)
self.assertEquals(2, len(updated_services))
self.assertEquals(1, len(updated_equipment))</code></pre>
<ol start="4">
<li>Create another test case that preloads the database using a SQL script, as shown:</li>
</ol>
<pre><code class="lang-python">class EventCorrelatorIntegrationTests(unittest.TestCase):
  def setUp(self):
      db_name = "recipe59.db"
      factory = Sqlite3ConnectionFactory(db=db_name)
      self.correlator = EventCorrelator(factory)
      dt = DatabaseTemplate(factory)
      sql = open("network.sql").read().split(";")
for statement in sql:
   dt.execute(statement + ";")</code></pre>
<ol start="5">
<li>Write a test method that calls the network application's process method and then prints out the results, as shown here:</li>
</ol>
<pre><code class="lang-python">def test_process_events(self):
    evt1 = Event("pyhost1", "serverRestart", 5)
    stored_event, is_active,
       updated_services, updated_equipment =
                 self.correlator.process(evt1)
    print "Stored event: %s" % stored_event
    if is_active:
         print "This event was an active event."
    print "Updated services: %s" % updated_services
    print "Updated equipment: %s" % updated_equipment
    print "---------------------------------"</code></pre>
<ol start="6">
<li>Create a new file called <kbd>recipe59.py</kbd> that only imports the SQL-based test case, as shown here:</li>
</ol>
<pre><code class="lang-python">from recipe59_test import EventCorrelatorIntegrationTests
if __name__ == "__main__":
     import unittest
     unittest.main()</code></pre>
<ol start="7">
<li>Run the test module. Take a look at the following screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000093.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>We need to write various test cases to cover the different levels of testing we need. By separating the test runner from the test case, we are able to decide to only run the test that integrated with the database.</p>
<p>Why would we do this? In our situation, we have only one unit test, and it runs pretty quickly. Do you think that a real-world application with months or years of development and a corresponding test suite will run as quickly? Of course not!</p>
<p>Some of the tests may be complex. They may involve talking to real systems, parsing huge sample data files, and other time-consuming tasks. This could realistically take minutes or hours to run.</p>
<p>When we are about to make a presentation to a customer, we don't need a long-running test suite. Instead, we need to be able to run a quick subset of these tests that gives us the confidence that things are working. Using Python's import statements makes this easy to define.</p>
<p>Some suites we may want to think about include the following:</p>
<ul>
<li><kbd>pulse.py</kbd>: Import a set of test cases that provide broad, yet shallow testing of the application, to verify the system has a pulse</li>
<li><kbd>checkin.py</kbd>: Import a set of test cases that are currently functioning, and provide enough confidence that code is ready to be committed</li>
<li><kbd>integration.py</kbd>: Import a set of test cases that start up, interact, and then shut down external systems such as LDAP, databases, or other subsystems</li>
<li><kbd>security.py</kbd>: Import a set of test cases that are focused on various security scenarios, confirming both good and bad credential handling</li>
<li><kbd>all.py</kbd>: Import all test cases to make sure everything is working</li>
</ul>
<p>This is just a sample of the types of test module we could define. It's possible to define a module for each subsystem we handle. But since we are talking about smoke testing, we may want to think more broadly, and instead pick some key tests from each subsystem and tie them together to give us a sense that the application is working.</p>
<h3>There's more...</h3>
<p>Let's have a look at these too.</p>
<h4>Security, checking, and integration aren't smoke tests!</h4>
<p>That is absolutely right. The previous list shows that using Python import statements isn't confined to defining smoke test suites. It can be used to bundle together test cases that serve a variety of needs. So why bring this up, since we are talking about smoke tests? Well, because I wanted to convey how useful this mechanism is for organizing tests, and that it extends beyond smoke testing.</p>
<h4>What provides good flexibility?</h4>
<p>To have good flexibility in being able to pick test classes, we should avoid making the test classes too big. But putting each test method inside a different class is probably too much.</p>
<h3>See also</h3>
<p>The <em>Leaving out integration tests</em> recipe in this chapter</p>
<h2>Leaving out integration tests</h2>
<p>A fast test suite avoids connecting to remote systems, such as databases and LDAP. Just verifying the core units and avoiding external systems can result in a faster-running test suite with more coverage. This can lead to a useful smoke test that gives developers confidence in the system without running all the tests.</p>
<h3>How to do it...</h3>
<p>With these steps, we will see how to cut out test cases that interact with external systems:</p>
<ol>
<li>Create a test module called <kbd>recipe60_test.py</kbd>, which will be used for writing some tests for our network application, as shown here:</li>
</ol>
<pre><code class="lang-python">import logging
from network import *
import unittest
from springpython.database.factory import *
from springpython.database.core import *</code></pre>
<ol start="2">
<li>Create a test case that removes the database connection and stubs out the data access functions:</li>
</ol>
<pre><code class="lang-python">class EventCorrelatorUnitTests(unittest.TestCase):
def setUp(self):
db_name = "recipe60.db"
factory = Sqlite3ConnectionFactory(db=db_name)
self.correlator = EventCorrelator(factory)
# We "unplug" the DatabaseTemplate so that
# we don't talk to a real database.
self.correlator.dt = None
# Instead, we create a dictionary of
# canned data to return back
self.return_values = {}
# For each sub-function of the network app,
# we replace them with stubs which return our
# canned data.
def stub_store_event(event):
event.id = self.return_values["id"]
return event, self.return_values["active"]
self.correlator.store_event = stub_store_event
def stub_impact(event):
return (self.return_values["services"],self.return_values["equipment"])
self.correlator.impact = stub_impact
def stub_update_service(service, event):
return service + " updated"
self.correlator.update_service = stub_update_service
def stub_update_equip(equip, event):
return equip + " updated"
self.correlator.update_equipment = stub_update_equip</code></pre>
<ol start="3">
<li>Create a test method that creates a set of canned data values, invokes the applications process method, and then verifies the values, as shown here:</li>
</ol>
<pre><code class="lang-python">def test_process_events(self):
# For this test case, we can preload the canned data,
# and verify that our process function is working
# as expected without touching the database.
self.return_values["id"] = 4668
self.return_values["active"] = True
self.return_values["services"] = ["service1",
"service2"]
self.return_values["equipment"] = ["device1"]
evt1 = Event("pyhost1", "serverRestart", 5)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt1)
self.assertEquals(4668, stored_event.id)
self.assertTrue(is_active)
self.assertEquals(2, len(updated_services))
self.assertEquals(1, len(updated_equipment))</code></pre>
<ol start="4">
<li>Create another test case that preloads the database using a SQL script:</li>
</ol>
<pre><code class="lang-python">class EventCorrelatorIntegrationTests(unittest.TestCase):
def setUp(self):
db_name = "recipe60.db"
factory = Sqlite3ConnectionFactory(db=db_name)
self.correlator = EventCorrelator(factory)
dt = DatabaseTemplate(factory)
sql = open("network.sql").read().split(";")
for statement in sql:
dt.execute(statement + ";")</code></pre>
<ol start="5">
<li>Write a test method that calls the network application's process method and then prints out the results:</li>
</ol>
<pre><code class="lang-python">def test_process_events(self):
evt1 = Event("pyhost1", "serverRestart", 5)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt1)
print "Stored event: %s" % stored_event
if is_active:
print "This event was an active event."
print "Updated services: %s" % updated_services
print "Updated equipment: %s" % updated_equipment
print "---------------------------------"</code></pre>
<ol start="6">
<li>Create a module called <kbd>recipe60.py</kbd> that only imports the unit test that avoids making SQL calls. Take a look at this code:</li>
</ol>
<pre><code class="lang-python">from recipe60_test import EventCorrelatorUnitTests
if __name__ == "__main__":
import unittest
unittest.main()</code></pre>
<ol start="7">
<li>Run the test module. Take a look at the following screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000087.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This test suite runs the unit tests and avoids running test cases that integrate with a live database. It uses Python <kbd>import</kbd> statements to decide which test cases to include.</p>
<p>In our contrived scenario, there is little gained performance. But with a real project, there are probably a lot more computer cycles spent on integration testing, due to the extra costs of talking to external systems.</p>
<p>The idea is to create a subset of tests that verify to some degree that our application works by covering a big chunk of it in a small amount of time.</p>
<p>The trick with smoke testing is deciding what constitutes a good enough test. Automated testing cannot completely confirm that our application has no bugs. We are foiled by the fact that either a particular bug doesn't exist, or we haven't written a test case that exposes such a bug. To engage in smoke testing, we are deciding to use a subset of these tests for a quick pulse read. Again, deciding which subset gives us a good enough pulse may be more art than science.</p>
<p>This recipe focuses on the idea that unit tests will probably run more quickly, and that cutting out the integration tests will remove the slower test cases. If all the unit tests pass, then we have some confidence that our application is in good shape.</p>
<h3>There's more...</h3>
<p>I must point out that test cases don't just easily fall into the category of <strong>unit test</strong> or <strong>integration test</strong>. It is more of a continuum. In this recipe's sample code, we wrote one unit test and one integration test, and then we picked the unit test for our smoke test suite.</p>
<p>Does this appear arbitrary and perhaps contrived? Sure it does. That is why smoke testing isn't cut and dried but instead requires some analysis and judgment about what to pick. And as development proceeds, there is room for fine-tuning.</p>
<p>I once developed a system that ingested invoices from different suppliers. I wrote unit tests that set up empty database tables, ingested files of many formats, and then examined the contents of the database to verify processing. The test suite took over 45 minutes to run. This pressured me to not run the test suite as often as desired. I crafted a smoke test suite that involved running only the unit tests that did <em>not</em> talk to the database (since they were quick), combined with ingesting one supplier invoice. It ran in fewer than five minutes, and provided a quicker means to assure myself that fundamental changes to the code did not break the entire system. I could run this many times during the day, and only run the comprehensive suite about once a day.</p>
<h4>Should a smoke test include integration or unit tests?</h4>
<p>Does this code appear to be similar to that shown in the&nbsp;<em>Defining a subset of test cases using import statements&nbsp;</em>recipe? Yes, it does. So, why include it in this recipe? Because what is picked for the smoke test suite is just as critical as the tactics used to make it happen. The other recipe decided to pick up an integration test while cutting out the unit tests to create a smaller, faster-running test suite.</p>
<p>This recipe shows that another possibility is to cut out the lengthier integration tests and instead run as many unit tests as possible, considering they are probably faster.</p>
<p>As stated earlier, smoke testing isn't cut and dried. It involves picking the best representation of tests without taking up too much time running them. It is quite possible that none the tests written up to this point precisely target the idea of capturing a pulse of the system. A good smoke test suite may involve mixing together a subset of unit and integration tests.</p>
<h3>See also</h3>
<p>The <em>Defining a subset of test cases using import statements</em> recipe</p>
<h2>Targeting end-to-end scenarios</h2>
<p>Pick a complement of tests that runs enough parts to define a thread of execution. This is sometimes referred to as thread testing. Not because we are using software threading, but instead because we are focusing on a story thread. Many times, our threads either come from customer scenarios, or they are at least inspired by them. Other threads can involve other groups, such as operations.</p>
<p>For example, a network management system may push out customer-affecting alarms, but the internal operations team that has to solve the network problems may have a totally different perspective. Both of these situations demonstrate valid end-to-end threads that are good places to invest in automated testing.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>If the different teams are viewed as different types of customers, then the concepts of acceptance testing certainly apply. And it's also possible to overlap this with the concepts of BDD.</p>
</div>
</div>
<h3>Getting ready</h3>
<ol>
<li>Copy the SQL script into a new file called <kbd>recipe61_network.sql</kbd> and replace the insert statements at the bottom with the following:</li>
</ol>
<pre><code class="lang-python">INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (1, 'pyhost1', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (2, 'pyhost2', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (3, 'pyhost3', 1);
INSERT into SERVICE (ID, NAME, STATUS) values (1, 'service-abc', 
'Operational');
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,1);
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,2);</code></pre>
<p>In this set of test data, <kbd>pyhost1</kbd> and <kbd>pyhost2</kbd> map into <kbd>service-abc</kbd>. <kbd>pyhost3</kbd> doesn't map into any service.</p>
<h3>How to do it...</h3>
<p>With these steps, we will build up an end-to-end test scenario.</p>
<ol>
<li>Create a test module called <kbd>recipe61_test.py</kbd>.</li>
<li>Create a test case where each test method captures a different thread of execution, as shown here:</li>
</ol>
<pre><code class="lang-python">import logging
from network import *
import unittest
from springpython.database.factory import *
from springpython.database.core import *
class EventCorrelatorEquipmentThreadTests(unittest.TestCase):
def setUp(self):
db_name = "recipe61.db"
factory = Sqlite3ConnectionFactory(db=db_name)
self.correlator = EventCorrelator(factory)
dt = DatabaseTemplate(factory)
sql = open("recipe61_network.sql").read().split(";")
for statement in sql:
dt.execute(statement + ";")
def tearDown(self):
self.correlator = None</code></pre>
<ol start="3">
<li>Create a test method that captures the thread of failing and recovering a piece of equipment, as shown here:</li>
</ol>
<pre><code class="lang-python">def test_equipment_failing(self):
# This alarm maps to a device
# but doesn't map to any service.</code></pre>
<ol start="4">
<li>Have the test method inject a single, faulting alarm and then confirm that a related piece of equipment has failed, as shown here:</li>
</ol>
<pre><code class="lang-python">evt1 = Event("pyhost3", "serverRestart", 5)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt1)
self.assertTrue(is_active)
self.assertEquals(len(updated_services), 0)
self.assertEquals(len(updated_equipment), 1)
self.assertEquals(updated_equipment[0]["HOST_NAME"],
"pyhost3")
# 5 is the value for a failed piece of equipment
self.assertEquals(updated_equipment[0]["STATUS"], 5)</code></pre>
<ol start="5">
<li>In the same test method, add code that injects a single, clearing alarm and confirms that the equipment has recovered, as shown here:</li>
</ol>
<pre><code class="lang-python">evt2 = Event("pyhost3", "serverRestart", 1)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt2)
self.assertFalse(is_active)
self.assertEquals(len(updated_services), 0)
self.assertEquals(len(updated_equipment), 1)
self.assertEquals(updated_equipment[0]["HOST_NAME"],
"pyhost3")
# 1 is the value for a clear piece of equipment
self.assertEquals(updated_equipment[0]["STATUS"], 1)</code></pre>
<ol start="6">
<li>Create another test method that captures the thread of failing and clearing a service, as shown here:</li>
</ol>
<pre><code class="lang-python">def test_service_failing(self):
# This alarm maps to a service.</code></pre>
<ol start="7">
<li>Write a test method that injects a single, faulting alarm and confirms that both a piece of equipment and a related service fails, as shown here:</li>
</ol>
<pre><code class="lang-python">evt1 = Event("pyhost1", "serverRestart", 5)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt1)
self.assertEquals(len(updated_services), 1)
self.assertEquals("service-abc",
updated_services[0]["service"]["NAME"])
self.assertEquals("Outage",
updated_services[0]["service"]["STATUS"])</code></pre>
<ol start="8">
<li>In the same test method, add code that injects a single, clearing alarm and confirms that both the equipment and the service have recovered, as shown here:</li>
</ol>
<pre><code class="lang-python">evt2 = Event("pyhost1", "serverRestart", 1)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt2)
self.assertEquals(len(updated_services), 1)
self.assertEquals("service-abc",
updated_services[0]["service"]["NAME"])
self.assertEquals("Operational",
updated_services[0]["service"]["STATUS"])</code></pre>
<ol start="9">
<li>Create a test runner called <kbd>recipe61.py</kbd> that imports both of these thread tests, as shown here:</li>
</ol>
<pre><code class="lang-python">from recipe61_test import *
if __name__ == "__main__":
import unittest
unittest.main()</code></pre>
<ol start="10">
<li>Run the test suite. Look at the following screenshot:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000078.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>In this recipe we coded two end to end test scenarios. Now consider the following:</p>
<ul>
<li>The first scenario tested how our application processes a fault, followed by a clear that only impacts a piece of the equipment.</li>
<li>The second scenario tested how our application processes a fault, followed by a clear that impacts a service.</li>
</ul>
<p>We injected a fault and then checked the results to confirm that the proper piece of inventory failed. Then we injected a clear, and again we confirmed that the proper piece of inventory recovered.</p>
<p>Both of these scenarios show how our application processes different types of events from the beginning to the end.</p>
<h3>There's more...</h3>
<p>In a more complex, realistic version of this application, what other systems do you think would be involved in an end to end thread? What about security? Transactions? Publishing results to an external interface?</p>
<p>This is where we need to define where the ends are. Imagine that our application was grown to the point where incoming events are received by a web request and equipment and service updates are pushed out as JSON data to be received by a web page.</p>
<p>A good end-to-end test would include these parts as well. For the JSON output, we can use Python's JSON library to decode the output and then confirm the results. For the incoming web request, we can use many different techniques, including&nbsp; acceptance testing tools such as the Robot Framework.</p>
<h4>How does this define smoke tests?</h4>
<p>If it takes too long run all the end-to-end tests, we should pick a subset of them that covers some key parts. For example, we could skip the equipment-based thread but keep the service-based one.</p>
<h3>See also</h3>
<ul>
<li><em>Testing Web Basics with the Robot Framework</em> recipe in <a href="https://www.packtpub.com/sites/default/files/downloads/Web_UI_Testing_Using_Selenium.pdf">Chapter 10</a>,&nbsp;<em>Web UI Testing Using Selenium</em></li>
<li><em>Using Robot to Verify Web Application Security&nbsp;</em>recipe in <a href="https://www.packtpub.com/sites/default/files/downloads/Web_UI_Testing_Using_Selenium.pdf">Chapter 10</a>,&nbsp;<em>Web UI Testing Using Selenium</em></li>
</ul>
<h2>Targeting the test server</h2>
<p>Does your test server have all the parts? If not, then define an alternative set of tests.</p>
<p>This recipe assumes that the production server has an enterprise grade MySQL database system, while the developer's workstation does not. We will explore writing some tests that use the MySQL database. But when we need to run them in the development lab, we will make adjustments so they run on SQLite, which comes bundled with Python.</p>
<p>Are you wondering why MySQL isn't on the developer's workstation? It is true that MySQL is easy to install and not a huge performance load. But this scenario applies just the same if the production server is Oracle and management deems it too costly for our developers to be granted an individual license. Due to the cost of setting up a commercial database, this recipe is uses MySQL and SQLite rather than Oracle and SQLite.</p>
<h3>Getting ready</h3>
<p>Let's look at the following steps:</p>
<ol>
<li>Make sure the MySQL production database server is up and running:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000042.png" class="lazyload" /></p>
<ol start="2">
<li>Open a command-line MySQL client shell as the root user.</li>
<li>Create a database for this recipe called <kbd>recipe62</kbd> and a user with permission to access it.</li>
<li>Exit the shell. Contrary to what is shown in the following screenshot, never, ever, ever create a live production database with passwords stored in the clear. This database is for demonstration purposes only:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000157.png" class="lazyload" /></p>
<h3>How to do it...</h3>
<p>In these steps, we will see how to build tests that are aimed at different servers:</p>
<ol>
<li>Create an alternate version of the SQL script called <kbd>recipe62_network.mysql</kbd> used in earlier recipes that uses MySQL conventions, as shown here:</li>
</ol>
<pre><code class="lang-python">DROP TABLE IF EXISTS SERVICE_MAPPING;
DROP TABLE IF EXISTS SERVICE_EVENTS;
DROP TABLE IF EXISTS ACTIVE_EVENTS;
DROP TABLE IF EXISTS EQUIPMENT;
DROP TABLE IF EXISTS SERVICE;
DROP TABLE IF EXISTS EVENTS;
CREATE TABLE EQUIPMENT (
ID SMALLINT PRIMARY KEY AUTO_INCREMENT,
HOST_NAME TEXT,
STATUS SMALLINT
);
CREATE TABLE SERVICE (
ID SMALLINT PRIMARY KEY AUTO_INCREMENT,
NAME TEXT,
STATUS TEXT
);
CREATE TABLE SERVICE_MAPPING (
ID SMALLINT PRIMARY KEY AUTO_INCREMENT,
SERVICE_FK SMALLINT,
EQUIPMENT_FK SMALLINT
);
CREATE TABLE EVENTS (
ID SMALLINT PRIMARY KEY AUTO_INCREMENT,
HOST_NAME TEXT,
SEVERITY SMALLINT,
EVENT_CONDITION TEXT
);
CREATE TABLE SERVICE_EVENTS (
ID SMALLINT PRIMARY KEY AUTO_INCREMENT,
SERVICE_FK SMALLINT,
EVENT_FK SMALLINT
);
CREATE TABLE ACTIVE_EVENTS (
ID SMALLINT PRIMARY KEY AUTO_INCREMENT,
EVENT_FK SMALLINT
);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (1, 'pyhost1', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (2, 'pyhost2', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (3, 'pyhost3', 1);
INSERT into SERVICE (ID, NAME, STATUS) values (1, 'service-abc', 
'Operational');
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,1);
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,2)</code></pre>
<p>You might not have noticed, but this schema definition has no foreign key constraints. In a real-world SQL script, they should definitely be included. They were left out in this case to reduce complexity.</p>
<ol start="2">
<li>Create a new module called <kbd>recipe62_test.py</kbd> to put our test code.</li>
<li>Create an abstract test case that has one test method verifying event-to-service correlation, as shown here:</li>
</ol>
<pre><code class="lang-python">import logging
from network import *
import unittest
from springpython.database.factory import *
from springpython.database.core import *
class AbstractEventCorrelatorTests(unittest.TestCase):
def tearDown(self):
self.correlator = None
def test_service_failing(self):
# This alarm maps to a service.
evt1 = Event("pyhost1", "serverRestart", 5)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt1)
self.assertEquals(len(updated_services), 1)
self.assertEquals("service-abc",
updated_services[0]["service"]["NAME"])
self.assertEquals("Outage",
updated_services[0]["service"]["STATUS"])
evt2 = Event("pyhost1", "serverRestart", 1)
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt2)
self.assertEquals(len(updated_services), 1)
self.assertEquals("service-abc",
updated_services[0]["service"]["NAME"])
self.assertEquals("Operational",
updated_services[0]["service"]["STATUS"])</code></pre>
<ol start="4">
<li>Create a concrete subclass that connects to the MySQL database and uses the MySQL script, as shown here:</li>
</ol>
<pre><code class="lang-python">class MySQLEventCorrelatorTests(AbstractEventCorrelatorTests):
def setUp(self):
factory = MySQLConnectionFactory("user", "password",
"localhost", "recipe62")
self.correlator = EventCorrelator(factory)
dt = DatabaseTemplate(factory)
sql = open("recipe62_network.mysql").read().split(";")
for statement in sql:
dt.execute(statement + ";")</code></pre>
<ol start="5">
<li>Create a corresponding production test runner called <kbd>recipe62_production.py</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">from recipe62_test import MySQLEventCorrelatorTests
if __name__ == "__main__":
import unittest
unittest.main()</code></pre>
<p>Run it and verify that it connects with the production database:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000021.png" class="lazyload" /></p>
<ol start="6">
<li>Now create a SQLite version of the SQL script called <kbd>recipe62_network.sql</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">DROP TABLE IF EXISTS SERVICE_MAPPING;
DROP TABLE IF EXISTS SERVICE_EVENTS;
DROP TABLE IF EXISTS ACTIVE_EVENTS;
DROP TABLE IF EXISTS EQUIPMENT;
DROP TABLE IF EXISTS SERVICE;
DROP TABLE IF EXISTS EVENTS;
CREATE TABLE EQUIPMENT (
ID INTEGER PRIMARY KEY,
HOST_NAME TEXT UNIQUE,
STATUS INTEGER
);
CREATE TABLE SERVICE (
ID INTEGER PRIMARY KEY,
NAME TEXT UNIQUE,
STATUS TEXT
);
CREATE TABLE SERVICE_MAPPING (
ID INTEGER PRIMARY KEY,
SERVICE_FK,
EQUIPMENT_FK,
FOREIGN KEY(SERVICE_FK) REFERENCES SERVICE(ID),
FOREIGN KEY(EQUIPMENT_FK) REFERENCES EQUIPMENT(ID)
);
CREATE TABLE EVENTS (
ID INTEGER PRIMARY KEY,
HOST_NAME TEXT,
SEVERITY INTEGER,
EVENT_CONDITION TEXT
);
CREATE TABLE SERVICE_EVENTS (
ID INTEGER PRIMARY KEY,
SERVICE_FK,
EVENT_FK,
FOREIGN KEY(SERVICE_FK) REFERENCES SERVICE(ID),
FOREIGN KEY(EVENT_FK) REFERENCES EVENTS(ID)
);
CREATE TABLE ACTIVE_EVENTS (
ID INTEGER PRIMARY KEY,
EVENT_FK,
FOREIGN KEY(EVENT_FK) REFERENCES EVENTS(ID)
);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (1, 'pyhost1', 1);
INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (2, 'pyhost2', 1);INSERT into EQUIPMENT (ID, HOST_NAME, STATUS) values (3, 'pyhost3', 1);
INSERT into SERVICE (ID, NAME, STATUS) values (1, 'service-abc', 'Op
erational');
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,1);
INSERT into SERVICE_MAPPING (SERVICE_FK, EQUIPMENT_FK) values (1,2);</code></pre>
<ol start="7">
<li>Create another concrete subclass of the abstract test case, have it connect as SQLite using the SQLite script, and add it to <kbd>recipe62_test.py</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">class Sqlite3EventCorrelatorTests(AbstractEventCorrelatorTests):
def setUp(self):
factory = Sqlite3ConnectionFactory("recipe62.db")
self.correlator = EventCorrelator(factory)
dt = DatabaseTemplate(factory)
sql = open("recipe62_network.sql").read().split(";")for statement in sql:
dt.execute(statement + ";")</code></pre>
<ol start="8">
<li>Create a corresponding development workstation test runner called <kbd>recipe62_dev.py</kbd>, as shown here:</li>
</ol>
<pre><code class="lang-python">from recipe62_test import Sqlite3EventCorrelatorTests
if __name__ == "__main__":
import unittest
unittest.main()</code></pre>
<ol start="9">
<li>Run it and verify that it connects with the development database:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000148.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>It is not uncommon to have a production environment with full-fledged servers and software installed while at the same time having a smaller development environment. Some shops even have a test bed that is somewhere in between these configurations.</p>
<p>Our network application handles this situation by allowing database connection information to get injected into it. In each test case, we used the exact same application, but with different database systems.</p>
<p>We wrote a test case that used the production MySQL database, and we wrote a test case that used the development SQLite database. Of course, MySQL, even though it is used in many production environments, doesn't sound like something that's not available to developers. But it provides an easy-to-see example of having to switch database systems.</p>
<h3>There's more...</h3>
<p>In this recipe, we showed the need to switch database systems. This isn't the only type of external system that may require alternate configurations for test purposes. Other things, such as LDAP servers, third-party web services, and separate subsystems, may have totally different configurations.</p>
<p>I have worked on several contracts and have often seen members of management cut development lab resources to save costs. They seem to conclude that the cost of maintaining multiple configurations and handling non-reproducible bugs is less than the cost of having the exact same complement of equipment and software. I feel that this conclusion is faulty, because, at some time in the future, they end up buying more hardware and upgrade things due to increasing issues involving platform variance.</p>
<p>This means we can't always write tests that target the production environment. Writing our software so that it has maximum flexibility, such as injecting database configuration, as we did earlier, is a minimum requirement.</p>
<p>It's important that we write as many tests as possible that work on the developer's platform. When developers have to start sharing server-side resources, then we run into resource collisions. For example, two developers sharing a single database server will have to do one of these things:</p>
<ul>
<li>Have separate schemas so they can empty and load test data</li>
<li>Coordinate times when they each have access to the same schema</li>
<li>Have different servers set up for each developer</li>
</ul>
<p>The third option is highly unlikely, given that we are talking about a development lab with a smaller footprint than the production one.</p>
<p>A positive note is that developers are getting faster and more powerful machines. Compared to 10 years ago, a commonly seen workstation far exceeds old server machines. But, even though we may each be able to run the entire software stack on our machine, it doesn't mean management will pay for all the necessary licensing.</p>
<p>Unfortunately, this limitation may never change. Hence, we have to be ready to write tests for alternate configurations and manage the discrepancies with the production environment.</p>
<h4>How likely is it that a development and production environment would use two different database systems?</h4>
<p>Admittedly, it is unlikely to have something as big as switching between SQLite and MySQL. That alone requires slightly different dialects of SQL to define the schema. Some would immediately consider this too difficult to manage. But there are smaller differences in environment that can still yield the same need for reduced testing.</p>
<p>I worked on a system for many years where the production system used Oracle 9i RAC, while the development lab just had Oracle 9i. RAC required extra hardware, and we were never allocated the resources for it. To top it off, Oracle 9i was too big to install on the relatively lightweight PCs we developed with. While everything spoke Oracle's dialect of SQL, the uptime differences between RAC and non-RAC generated a fair number of bugs that we couldn't reproduce in the development lab. It really did qualify as two different database systems. Given that we couldn't work in the production environment, we tested as much as we could in the development lab and then scheduled time in the test lab where an RAC instance existed. Since many people needed access to that lab, we confined our usage to RAC-specific issues to avoid schedule delays.</p>
<h4>This isn't just confined to database systems</h4>
<p>As stated earlier, this isn't just about database systems. We have discussed MySQL, SQLite, and Oracle, but this also involves any sort of system we work with or depend on that varies between production and development environments.</p>
<p>Being able to code subsets of tests to achieve confidence can help cut down on the actual issues we will inevitably have to deal with.</p>
<h2>Coding a data simulator</h2>
<p>Coding a simulator that spits out data at a defined rate can help simulate real load.</p>
<p>This recipe assumes that the reader's machine is installed with MySQL.</p>
<h3>Getting ready</h3>
<ol>
<li>Make sure the MySQL production database server is up and running.</li>
<li>Open a command-line MySQL client shell as the root user.</li>
<li>Create a database for this recipe called <kbd>recipe63</kbd> as well as a user with permission to access it.</li>
<li>Exit the shell as shown here:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000158.png" class="lazyload" /></p>
<h3>How to do it...</h3>
<p>With these steps, we will explore coding a test simulator:</p>
<ol>
<li>Create a test generator script called <kbd>recipe63.py</kbd> that uses various Python libraries, as shown here:</li>
</ol>
<pre><code class="lang-python">import getopt
import random
import sys
import time
from network import *
from springpython.remoting.pyro import *</code></pre>
<ol start="2">
<li>Create a usage method that prints out command-line options, as shown here:</li>
</ol>
<pre><code class="lang-python">def usage():
print "Usage"
print "====="
print "-h, --help read this help"
print "-r, --rate [arg] number of events per second"
print "-d, --demo demo by printing events"</code></pre>
<ol start="3">
<li>Use Python's getopt library to parse command-line arguments, as shown here:</li>
</ol>
<pre><code class="lang-python">try:
opts, args = getopt.getopt(sys.argv[1:], "hr:d", ["help", "rate=",
"demo"])
except getopt.GetoptError, err:
print str(err)
usage()
sys.exit(1)
rate = 10
demo_mode = False
for o, a in opts:
if o in ("-h", "--help"):
usage()
sys.exit(1)
elif o in ("-r", "--rate"):
rate = a
elif o in ("-d", "--demo"):
demo_mode = True</code></pre>
<ol start="4">
<li>Add a switch so when it's not in demo mode, it uses Spring Python's <kbd>PyroProxyFactory</kbd> to connect to a server instance of the network management application<em>:</em></li>
</ol>
<pre><code class="lang-python">if not demo_mode:
print "Sending events to live network app. Ctrl+C to exit..."
proxy = PyroProxyFactory()
proxy.service_url = "PYROLOC://127.0.0.1:7766/network"</code></pre>
<ol start="5">
<li>Code an infinite loop that creates a random event, as shown here:</li>
</ol>
<pre><code class="lang-python">while True:
hostname = random.choice(["pyhost1","pyhost2","pyhost3"])
condition = random.choice(["serverRestart", "lineStatus"])
severity = random.choice([1,5])
evt = Event(hostname, condition, severity)</code></pre>
<ol start="6">
<li>If in demo mode, print out the event, as shown here:</li>
</ol>
<pre><code class="lang-python">if demo_mode:
now = time.strftime("%a, %d %b %Y %H:%M:%S +0000",
time.localtime())
print "%s: Sending out %s" % (now, evt)</code></pre>
<ol start="7">
<li>If not in demo mode, make a remote call through the proxy to the network app's process method, as shown here:</li>
</ol>
<pre><code class="lang-python">else:
stored_event, is_active, updated_services,
updated_equipment = proxy.process(evt)
print "Stored event: %s" % stored_event
print "Active? %s" % is_active
print "Services updated: %s" % updated_services
print "Equipment updated; %s" % updated_equipment
print "================"</code></pre>
<ol start="8">
<li>Sleep for a certain amount of time before repeating the loop, by using this code line:</li>
</ol>
<pre><code class="lang-python">time.sleep(1.0/float(rate))</code></pre>
<ol start="9">
<li>Run the generator script. In the following screenshot, notice there is an error because we haven't started the server process yet. This can also happen if the client and server have mismatched URLs:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000106.png" class="lazyload" /></p>
<ol start="10">
<li>Create a server script called <kbd>recipe63_server.py</kbd> that will run our network management app connected to MySQL using the&nbsp;<kbd>recipe62_network.sql</kbd>&nbsp;SQL script&nbsp;from the&nbsp;<em>Targeting the test server</em> recipe, as shown here:</li>
</ol>
<pre><code class="lang-python">from springpython.database.factory import *
from springpython.database.core import *
from springpython.remoting.pyro import *
from network import *
import logging
logger = logging.getLogger("springpython")
loggingLevel = logging.DEBUG
logger.setLevel(loggingLevel)
ch = logging.StreamHandler()
ch.setLevel(loggingLevel)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s -
%(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)
# Initialize the database
factory = MySQLConnectionFactory("user", "password",
"localhost", "recipe63")
dt = DatabaseTemplate(factory)
sql = open("recipe62_network.mysql").read().split(";")
for statement in sql:
dt.execute(statement + ";")</code></pre>
<ol start="11">
<li>Add code to expose the app using Pyro, as shown here:</li>
</ol>
<pre><code class="lang-python"># Create an instance of the network management app
target_service = EventCorrelator(factory)
# Expose the network app as a Pyro service
exporter = PyroServiceExporter()
exporter.service_name = "network"
exporter.service = target_service
exporter.after_properties_set()</code></pre>
<ol start="12">
<li>Run the server script in a different shell:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000137.png" class="lazyload" /></p>
<ol start="13">
<li>The default rate is 10 events/second. Run the generator script with a rate of one event/second. In the following screenshot, notice how the script generated a clear, fault, and then another fault. The service started at Operational, moved to Outage, and stayed there:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000111.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Python's <kbd>random.choice</kbd> method makes it easy to create a range of random events. By using the <kbd>time.sleep</kbd> method, we can control the rate at which the events are created.</p>
<p>We used Pyro to connect the test generator to the network management application. This isn't the only way to connect things together. We could have exposed the application through other means, such as REST, JSON, or perhaps by communicating through a database table. That's not important. What is important is that we built an independent tool that fed data into our application as if it came from a live network.</p>
<h3>There's more...</h3>
<p>We built a test generator. It's easy to run multiple copies of it in different shells, at different rates. We have an easy way to simulate different subnets producing different volumes of traffic.</p>
<p>We could also add more command-line options to fine-tune the events. For example, we could make the event condition a parameter, and emulate different rates for different types of events.</p>
<h4>Why does the server script initialize the database?</h4>
<p>A production version of the server wouldn't do this. For the demonstration purposes of this recipe, it is convenient to put it there. Every time we stop and start the server script, it relaunches the database.</p>
<h4>Why MySQL instead of SQLite?</h4>
<p>SQLite has some limitations when it comes to multithreading. Pyro uses multithreading, and SQLite can't pass objects across threads. SQLite is also relatively lightweight and probably not well-suited for a real network management application.</p>
<h3>See also</h3>
<p>The <em>Targeting the test server</em> recipe</p>
<h2>Recording and playing back live data in real time</h2>
<p>Nothing beats live production data. With this recipe, we will write some code to record live data. Then we will play it back with delays added to simulate playing back the live data stream.</p>
<h3>Getting ready</h3>
<p>Let's look at the following steps:</p>
<ol>
<li>Make sure the MySQL production database server is up and running.</li>
<li>Open a command line MySQL client shell as the root user.</li>
<li>Create a database for this recipe called <kbd>recipe64</kbd> as well as a user with permission to access it.</li>
</ol>
<ol start="4">
<li>Exit the shell, as shown here:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000119.png" class="lazyload" /></p>
<h3>How to do it...</h3>
<p>With these steps, we will see how to record and play back data in real-time pace:</p>
<ol>
<li>Write a script called <kbd>recipe64_livedata.py</kbd> that simulates live data being sent every one to ten seconds, as shown here:</li>
</ol>
<pre><code class="lang-python">import random
import sys
import time
from network import *
from springpython.remoting.pyro import *print "Sending events to live network app. Ctrl+C to exit..."
proxy = PyroProxyFactory()
proxy.service_url = "PYROLOC://127.0.0.1:7766/network_advised"
while True:
hostname = random.choice(["pyhost1","pyhost2","pyhost3"])
condition = random.choice(["serverRestart", "lineStatus"])
severity = random.choice([1,5])
evt = Event(hostname, condition, severity)
stored_event, is_active, updated_services,
updated_equipment = proxy.process(evt)
print "Stored event: %s" % stored_event
print "Active? %s" % is_active
print "Services updated: %s" % updated_services
print "Equipment updated; %s" % updated_equipment
print "================"
time.sleep(random.choice(range(1,10)))</code></pre>
<ol start="2">
<li>Write a server script called <kbd>recipe64_server.py</kbd> that initializes the database using the SQL script <kbd>recipe62_network.mysql</kbd> from <em>Targeting the test server</em>, as shown here:</li>
</ol>
<pre><code class="lang-python">from springpython.database.factory import *
from springpython.database.core import *
from springpython.remoting.pyro import *
from springpython.aop import *
from network import *
from datetime import datetime
import os
import os.path
import pickle
import logging
logger = logging.getLogger("springpython.remoting")
loggingLevel = logging.DEBUG
logger.setLevel(loggingLevel)
ch = logging.StreamHandler()
ch.setLevel(loggingLevel)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s -
%(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)
# Initialize the database
factory = MySQLConnectionFactory("user", "password",
"localhost", "recipe64")
dt = DatabaseTemplate(factory)
sql = open("recipe62_network.mysql").read().split(";")
for statement in sql:
dt.execute(statement + ";")</code></pre>
<ol start="3">
<li>Add some code that creates an instance of the network management application and advertises it using Pyro and Spring Python, as shown here:</li>
</ol>
<pre><code class="lang-python"># Create an instance of the network management app
target_service = EventCorrelator(factory)
# Expose the original network app as a Pyro service
unadvised_service = PyroServiceExporter()
unadvised_service.service_name = "network"
unadvised_service.service = target_service
unadvised_service.after_properties_set()</code></pre>
<ol start="4">
<li>Add some more code that defines an interceptor that captures incoming event data along with a time stamp to disk, as shown here:</li>
</ol>
<pre><code class="lang-python">class Recorder(MethodInterceptor):
"""
An interceptor that catches each event,
write it to disk, then proceeds to the
network management app.
"""
def __init__(self):
self.filename = "recipe64_data.txt"
self.special_char = "&amp;&amp;&amp;"
if os.path.exists(self.filename):
os.remove(self.filename)
def invoke(self, invocation):
# Write data to disk
with open(self.filename, "a") as f:
evt = invocation.args[0]
now = datetime.now()
output = (evt, now)
print "Recording %s" % evt
f.write(pickle.dumps(output).replace("n", "&amp;&amp;&amp;") + "n")
# Now call the advised service
return invocation.proceed()</code></pre>
<ol start="5">
<li>Add some code that wraps the network management application with the interceptor and advertises it using Pyro, as shown here:</li>
</ol>
<pre><code class="lang-python"># Wrap the network app with an interceptor
advisor = ProxyFactoryObject()
advisor.target = target_service
advisor.interceptors = [Recorder()]
# Expose the advised network app as a Pyro service
advised_service = PyroServiceExporter()
advised_service.service_name = "network_advised"
advised_service.service = advisor
advised_service.after_properties_set()</code></pre>
<ol start="6">
<li>Start up the server app by typing <kbd>python recipe64_server.py</kbd>. Notice in the following screenshot that there is both a <kbd>network</kbd> service and a <kbd>network_advised</kbd> service registered with Pyro:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000016.png" class="lazyload" /></p>
<p>Run the live data simulator by typing <kbd>python recipe64_livedata.py</kbd> until it generates a few events, and then hit Ctrl+C to break out of it:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000099.png" class="lazyload" /></p>
<ol start="7">
<li>Look at the server-side of things, and notice how it recorded several events:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000074.png" class="lazyload" /></p>
<ol start="8">
<li>Inspect the <kbd>recipe64_data.txt</kbd> data file, noting how each line represents a separate event and time stamp. While it's hard to decipher the data stored in a pickled format, it's possible to spot bits and pieces.</li>
<li>Create a script called <kbd>recipe64_playback.py</kbd> that de-pickles each line of the data file, as shown here:</li>
</ol>
<pre><code class="lang-python">from springpython.remoting.pyro import *
from datetime import datetime
import pickle
import time
with open("recipe64_data.txt") as f:
lines = f.readlines()
events = [pickle.loads(line.replace("&amp;&amp;&amp;", "n"))
for line in lines]</code></pre>
<ol start="10">
<li>Add a function that finds the time interval between the current event and the previous one, as shown here:</li>
</ol>
<pre><code class="lang-python">def calc_offset(evt, time_it_happened, previous_time):
if previous_time is None:
return time_it_happened - time_it_happened
else:
return time_it_happened - previous_time</code></pre>
<ol start="11">
<li>Define a client proxy to connect to the unadvised interface to our network management application, as shown here:</li>
</ol>
<pre><code class="lang-python">print "Sending events to live network app. Ctrl+C to exit..."
proxy = PyroProxyFactory()
proxy.service_url = "PYROLOC://127.0.0.1:7766/network"</code></pre>
<ol start="12">
<li>Add code that iterates over each event, calculating the difference, and then delaying the next event by that many seconds, as shown here:</li>
</ol>
<pre><code class="lang-python">previous_time = None
for (e, time_it_happened) in events:
diff = calc_offset(e, time_it_happened, previous_time)
print "Original: %s Now: %s" % (time_it_happened, datetime.now())
stored_event, is_active, updated_services,
updated_equipment = proxy.process(e)
print "Stored event: %s" % stored_event
print "Active? %s" % is_active
print "Services updated: %s" % updated_services
print "Equipment updated; %s" % updated_equipment
print "Next event in %s seconds" % diff.seconds
print "================"
time.sleep(diff.seconds)
previous_time = time_it_happened</code></pre>
<ol start="13">
<li>Run the playback script by typing <kbd>python recipe64_playback.py</kbd> and observe how it has the same delays as the original live data simulator did:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000080.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Normally, we would be recording data coming in from the live network. In this situation, we need a simulator that generates random data. The simulator we coded in this recipe is very similar to the one shown in the <em>Coding a data simulator</em> recipe.</p>
<p>To capture the data, we coded an interceptor that is embedded between Pyro and the network management application. Every event published to the <kbd>network_advised</kbd> Pyro service name seamlessly passes through this interceptor. Consider the following:</p>
<ol>
<li>Each event that comes in is appended to the data file that was initialized when the interceptor was first created.</li>
<li>The event is also stored with a copy of <kbd>datetime.now()</kbd>&nbsp;to capture a time stamp.</li>
<li>The event and time stamp are combined into a tuple and pickled, making it easy to write and later read back from disk.</li>
<li>The data is pickled to make it easy to transfer to and from disk.</li>
<li>After writing it on to the disk, the interceptor calls the target service and passes the results back to the original caller.</li>
</ol>
<p>Finally, we have a playback script that reads in the data file, one event per line. It de-pickles each line into the tuple format it was originally stored in and builds a list of events.</p>
<p>The list of events is then scanned, one at a time. By comparing the current event's time stamp with the previous one, a difference in seconds is calculated to use Python's <kbd>time.sleep()</kbd> method to play the events back at the same rate they were recorded.</p>
<p>The playback script uses Pyro to send the events into the network management application. But it talks to a different exposure point. This is to avoid re-recording the same event.</p>
<h3>There's more...</h3>
<p>The code in this recipe uses Pyro as the mechanism connecting clients and servers communicate in a publish/subscribe paradigm. This isn't the only way to build such a service. Python has XML-RPC built in as well. It just isn't as flexible as Pyro. A more thorough analysis of real traffic is needed to determine whether this interface is good enough. Alternatives include pushing events through a database EVENT table where the client inserts rows and the server polls the table for new rows, and then removes them as they are consumed.</p>
<p>This recipe also makes heavy use of Spring Python for its <strong>aspect-oriented programming</strong> features to insert the data recording code (<a href="http://static.springsource.org/spring-python/1.1.x/reference/html/aop.html">http://static.springsource.org/spring-python/1.1.x/reference/html/aop.html</a>). This provides a clean way to add the extra layer of functionality we need to sniff and record network traffic without having to touch the already-built network management code.</p>
<h4>I thought this recipe was about live data!</h4>
<p>Well, the recipe is more about <strong>recording</strong> the live data and controlling the speed of playback. To capture this concept in a reusable recipe requires that we simulate the live system. But the fundamental concept of inserting a tap point in front of the network management processor, as we have done, is just as valid.</p>
<h4>Is opening and closing a file for every event a good idea?</h4>
<p>The recipe was coded to ensure that stopping the recording would incur a minimal risk of losing captured data not yet written to disk. Analysis of production data is required to determine the most efficient way of storing data. For example, it may take less I/O intense to write data in batches of 10, or perhaps 100 events. But the risk is that data can be lost in similar bundles.</p>
<p>If the volume of traffic is low enough, writing each event one by one, as shown in this recipe, may not be a problem at all.</p>
<h4>What about offloading the storage of data?</h4>
<p>It is not uncommon to have the actual logic of opening the file, appending the data, and then closing the file contained in a separate class. This utility could then be injected into the interceptor we built. This may become important if some more elaborate means to storing or piping the data is needed. For example, another Pyro service may exist in another location that wants a copy of the live data feed.</p>
<p>Injecting the data consumer into the aspect we coded would give us more flexibility. In this recipe, we don't have such requirements, but it's not hard to imagine making such adjustments as new requirements arrive.</p>
<h3>See also</h3>
<ul>
<li>The <em>Writing</em> <em>a data simulator</em> recipe</li>
<li>The <em>Recording and playing back live data as fast as possible</em> recipe</li>
</ul>
<h2>Recording and playing back live data as fast as possible</h2>
<p>Replaying production data as fast as possible (instead of in real time) can give you insight into where your bottlenecks are.</p>
<h3>Getting ready</h3>
<ol>
<li>Make sure the MySQL production database server is up and running.</li>
<li>Open a command-line MySQL client shell as the root user.</li>
<li>Create a database for this recipe called <kbd>recipe65</kbd> as well as a user with permission to access it.</li>
<li>Exit the shell, as shown here:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000134.png" class="lazyload" /></p>
<h3>How to do it...</h3>
<p>In these steps, we will write some code that lets us put a big load on our system:</p>
<ol>
<li>Write a script called <kbd>recipe65_livedata.py</kbd> that simulates live data being sent every one to ten seconds, as shown here:</li>
</ol>
<pre><code class="lang-python">import random
import sys
import time
from network import *
from springpython.remoting.pyro import *
print "Sending events to live network app. Ctrl+C to exit..."
proxy = PyroProxyFactory()
proxy.service_url = "PYROLOC://127.0.0.1:7766/network_advised"
while True:
hostname = random.choice(["pyhost1","pyhost2","pyhost3"])
condition = random.choice(["serverRestart", "lineStatus"])
severity = random.choice([1,5])
evt = Event(hostname, condition, severity)
stored_event, is_active, updated_services,
updated_equipment = proxy.process(evt)
print "Stored event: %s" % stored_event
print "Active? %s" % is_active
print "Services updated: %s" % updated_services
print "Equipment updated; %s" % updated_equipment
print "================"
time.sleep(random.choice(range(1,10)))</code></pre>
<ol start="2">
<li>Write a server script called <kbd>recipe65_server.py</kbd> that initializes the database using the SQL script <kbd>recipe62_network.mysql</kbd> from the&nbsp;<em>Targeting the test server&nbsp;</em>recipe, as shown here:</li>
</ol>
<pre><code class="lang-python">from springpython.database.factory import *
from springpython.database.core import *
from springpython.remoting.pyro import *
from springpython.aop import *
from network import *
from datetime import datetime
import os
import os.path
import pickle
import logging
logger = logging.getLogger("springpython.remoting")
loggingLevel = logging.DEBUG
logger.setLevel(loggingLevel)
ch = logging.StreamHandler()
ch.setLevel(loggingLevel)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s -%(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)
# Initialize the database
factory = MySQLConnectionFactory("user", "password",
"localhost", "recipe65")
dt = DatabaseTemplate(factory)
sql = open("recipe62_network.mysql").read().split(";")
for statement in sql:
dt.execute(statement + ";")</code></pre>
<ol start="3">
<li>Add some code that creates an instance of the network management application and advertises it using Pyro and Spring Python, as shown here:</li>
</ol>
<pre><code class="lang-python"># Create an instance of the network management app
target_service = EventCorrelator(factory)
# Expose the original network app as a Pyro service
unadvised_service = PyroServiceExporter()
unadvised_service.service_name = "network"
unadvised_service.service = target_service
unadvised_service.after_properties_set()</code></pre>
<ol start="4">
<li>Add some more code that defines an interceptor that captures incoming event data along with a time stamp on disk, as shown here:</li>
</ol>
<pre><code class="lang-python">class Recorder(MethodInterceptor):
"""
An interceptor that catches each event,
write it to disk, then proceeds to the
network management app.
"""
def __init__(self):
self.filename = "recipe65_data.txt"
self.special_char = "&amp;&amp;&amp;"
if os.path.exists(self.filename):
os.remove(self.filename)
def invoke(self, invocation):
# Write data to disk
with open(self.filename, "a") as f:
evt = invocation.args[0]
now = datetime.now()
output = (evt, now)
print "Recording %s" % evt
f.write(pickle.dumps(output).replace(
"n", "&amp;&amp;&amp;") + "n")
# Now call the advised service
return invocation.proceed()</code></pre>
<ol start="5">
<li>Add some code that wraps the network management application with the interceptor and advertises it using Pyro, as shown here:</li>
</ol>
<pre><code class="lang-python"># Wrap the network app with an interceptor
advisor = ProxyFactoryObject()
advisor.target = target_service
advisor.interceptors = [Recorder()]
# Expose the advised network app as a Pyro service
advised_service = PyroServiceExporter()
advised_service.service_name = "network_advised"
advised_service.service = advisor
advised_service.after_properties_set()</code></pre>
<ol start="6">
<li>Start up the server app by typing <kbd>python recipe65_server.py</kbd>. In the following screenshot, notice that there is both a <kbd>network</kbd> service and a <kbd>network_advised</kbd> service registered with Pyro:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000062.png" class="lazyload" /></p>
<ol start="7">
<li>Run the live data simulator by typing <kbd>python recipe65_livedata.py</kbd> and watch it run until it generates a few events, and then hit <em>Ctrl</em>+<em>C</em> to break out of it:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000070.png" class="lazyload" /></p>
<ol start="8">
<li>Look at the server side of things, and notice how it recorded several events:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000045.png" class="lazyload" /></p>
<ol start="9">
<li>Inspect the <kbd>recipe65_data.txt</kbd> data file, noting how each line represents a separate event and time stamp. While it's hard to decipher the data stored in a pickled format, it's possible to spot bits and pieces.</li>
<li>Create a playback script called <kbd>recipe65_playback.py</kbd> that de-pickles each line of the data file, as shown here:</li>
</ol>
<pre><code class="lang-python">from springpython.remoting.pyro import *
from datetime import datetime
import pickle
import time
with open("recipe65_data.txt") as f:
lines = f.readlines()
events = [pickle.loads(line.replace("&amp;&amp;&amp;", "n"))
for line in lines]</code></pre>
<ol start="11">
<li>Define a client proxy to connect to the unadvised interface to our network management application, as shown here:</li>
</ol>
<pre><code class="lang-python">print "Sending events to live network app. Ctrl+C to exit..."
proxy = PyroProxyFactory()
proxy.service_url = "PYROLOC://127.0.0.1:7766/network"</code></pre>
<p>Add code that iterates over each event, playing back the events as quickly as possible, as shown here:</p>
<pre><code class="lang-python">for (e, time_it_happened) in events:
stored_event, is_active, updated_services,
updated_equipment = proxy.process(e))
print "Stored event: %s" % stored_event
print "Active? %s" % is_active
print "Services updated: %s" % updated_services
print "Equipment updated; %s" % updated_equipment
print "================"</code></pre>
<ol start="12">
<li>Run the playback script by typing <kbd>python recipe65_playback.py</kbd>, observing how it doesn't delay events but instead plays them back as quickly as possible:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000061.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>Normally, we would be recording data coming in from the live network. In this situation, we need a simulator that generates random data. The simulator we coded in this recipe is very similar to the one shown in the <em>Coding a data simulator</em> recipe.</p>
<p>To capture the data, we coded an interceptor that is embedded between Pyro and the network management application. Every event published to the <kbd>network_advised</kbd> Pyro service name seamlessly passes through this interceptor. Consider the following:</p>
<ul>
<li>Each event that comes in is appended to the data file that was initialized when the interceptor was first created.
<ul>
<li>The event is also stored with a copy of <kbd>datetime.now()</kbd>&nbsp;to capture a time stamp.</li>
</ul>
</li>
<li>The event and time stamp are combined into a tuple, and pickled, making it easy to write and later read back from disk.</li>
</ul>
<ul>
<li>The data is pickled to make it easy to transfer to and from disk.</li>
<li>After writing it to disk, the interceptor calls the target service and passes the results back to the original caller.</li>
</ul>
<p>Finally, we have a playback script that reads in the data file, one event per line. It de-pickles each line into the tuple format it was originally stored in, and builds a list of events.</p>
<p>The list of events is then scanned, one at a time. Instead of evaluating the time stamps to figure out how long to delay playing back the events, they are injected immediately into the network management application.</p>
<p>The playback script uses Pyro to send the events in to the network management application, but it talks to a different exposure point. This is to avoid re-recording the same event.</p>
<h3>There's more...</h3>
<p>The code in this recipe uses Pyro as the mechanism connecting clients and servers communicates in a publish/subscribe paradigm. This isn't the only way to build such a service. Python has XML-RPC built in as well. It just isn't as flexible as Pyro. A more thorough analysis of real traffic is needed to determine whether this interface is good enough. Alternatives include pushing events through a database EVENT table where the client inserts rows and the server polls the table for new rows, and then removes them as they are consumed.</p>
<p>This recipe also makes heavy use of Spring Python for its <strong>aspect-oriented programming</strong> features to insert the data recording code (<a href="http://static.springsource.org/spring-python/1.1.x/reference/html/aop.html">http://static.springsource.org/spring-python/1.1.x/reference/html/aop.html</a>). This provides a clean way to add the extra layer of functionality we need to sniff and record network traffic without having to touch the existing network management code.</p>
<h4>What is the difference between this and playing back in real time?</h4>
<p>Real-time playback is useful to see how the system handles production load. But this doesn't answer the question of where the system is expected to break. Traffic flow is never steady. Instead, it often has bursts that are not expected. That is when playing back live data at an accelerated rate will help expose the system's next breakpoints.</p>
<p>Preemptively addressing some of these concerns will make our system more resilient.</p>
<h4>Where are the breaking points of this application?</h4>
<p>Admittedly, this recipe didn't break when we played back four events as fast as possible. Would this be the same result in production? Things break in different ways. We may not get a real exception or error message but instead discover that certain parts of the system become backlogged.</p>
<p>That is where this recipe reaches its limit. While we have demonstrated how to overload the system with a large volume of traffic, we are not showing how to monitor where the bottlenecks are.</p>
<p>If the application under load uses database tables to queue up work, then we would need to write the code that monitors them all and report the following:</p>
<ul>
<li>Which one is the longest</li>
<li>Which one is getting longer, and showing no sign of catching up</li>
<li>Which one is the earliest in the pipeline of activity</li>
</ul>
<p>In systems with stages of processing, there is often one bottleneck that makes itself known. When that bottleneck is fixed, it is rarely the only bottleneck. It was simply either the most critical one or the first one in a chain.</p>
<p>Also, this recipe cannot solve your bottleneck. The purpose of this recipe is to find it.</p>
<p>I once built a network load tester very much like this one. The code could handle processing lots of traffic in parallel, but events from the same device had to be processed in order. Replaying a days worth of events all at once exposed the fact that too many events from the same device caused the entire queue system to become overloaded and starve out handling other devices. After improving the service update algorithm, we were able to replay the same load test and verify it could keep up. This helped avoid non-reproducible outages that happened after hours or on weekends.</p>
<h4>What amount of live data should be collected?</h4>
<p>It is useful for capturing things such as a 24-hour block of traffic to allow&nbsp;an entire day of events to be&nbsp;played back. Another possibility is an entire week. Live systems may be apt to have different loads on weekends rather than weekdays, and a week of data will allow better investigation.</p>
<p>The problem with this much data is that it is hard to pick out a window to investigate. This is why 24 hours of data from the weekend and 24 hours of data during the week may be more practical.</p>
<p>If there is some sort of network instability where huge outages are occurring and causing a huge flow of traffic, it may be useful to turn on the collector and wait for another similar outage to occur. After such an outage occurs, it may be useful to shift through the data file and trim it down to where the uptick in traffic occurred.</p>
<p>These types of captured scenarios are invaluable in load testing new releases, because they confirm that new patches either improve performance as expected, or at least don't reduce performance when fixing non-performance issues.</p>
<h3>See also</h3>
<ul>
<li>The <em>Writing a data simulator</em> recipe</li>
<li>The <em>Recording and playing back live data in real-time</em> recipe</li>
</ul>
<h2>Automating your management demo</h2>
<p>Got a demo coming? Write automated tests that simulate the steps you'll be taking. Then print out your test suite, and use it like a script.</p>
<h3>How to do it...</h3>
<p>With these steps, we will see how to write our management demo script in a runnable fashion:</p>
<ol>
<li>Create a new file called <kbd>recipe66.py</kbd>&nbsp;for the test code for our management demo.</li>
<li>Create a <kbd>unittest</kbd> test scenario to capture your demo.</li>
<li>Write a series of operations as if you were driving the application from this automated test.</li>
<li>Include asserts at every point where you will vocally point out something during the demo. Take a look at this code:</li>
</ol>
<pre><code class="lang-python">import unittest
from network import *
from springpython.database.factory import *
class ManagementDemo(unittest.TestCase):
def setUp(self):
factory = MySQLConnectionFactory("user", "password",
"localhost", "recipe62")
self.correlator = EventCorrelator(factory)
dt = DatabaseTemplate(factory)
sql = open("recipe62_network.mysql").read().split(";")
for statement in sql:
dt.execute(statement + ";")
def test_processing_a_service_affecting_event(self):
# Define a service-affecting event
evt1 = Event("pyhost1", "serverRestart", 5)
# Inject it into the system
stored_event, is_active,
updated_services, updated_equipment =
self.correlator.process(evt1)
# These are the values I plan to call
# attention to during my demo
self.assertEquals(len(updated_services), 1)
self.assertEquals("service-abc",
updated_services[0]["service"]["NAME"])
self.assertEquals("Outage",
updated_services[0]["service"]["STATUS"])
if __name__ == "__main__":
unittest.main()</code></pre>
<ol start="5">
<li>Run the test suite by typing <kbd>python recipe66.py</kbd>:</li>
</ol>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-testing-cookbook/images/000024.png" class="lazyload" /></p>
<h3>How it works...</h3>
<p>This recipe is more philosophical and less code-based. While the concept of this recipe is valuable, it is hard to capture in a single nugget of reusable code.</p>
<p>In this test case, I inject an event, process it, and then confirm what it impacts. This test case is headless, but our demo probably won't be. So far in this chapter, we haven't built any user screens. As we develop user screens, we need to ensure they call the same APIs this automated test calls.</p>
<p>Given this, we are set up to use the screens to define the same event shown in the test. After the event is digested, another screen will probably exist that shows current service status. We would expect it to reflect the update to Outage.</p>
<p>During our management demo, we will then point out/zoom in to this part of the screen and show how <kbd>service-abc</kbd> switched from <em>Operational</em> to <em>Outage.</em></p>
<p>If the screens are built to delegate to this underlying logic, then the screen logic is little more than components put together to display information. The core logic being tested maintains its headless and easy-to-test nature.</p>
<p>Our code sample isn't complete, and wouldn't amount to more than a one minute demo. But the concept is sound. By capturing the steps we plan to execute in our demo in a runnable form, our management demo should go off without a hitch.</p>
<p>Did I say without a hitch? Well, demos rarely work that well. Doesn't something about management appearances cause things to break? At one time, I began prepping for a senior management demo a month in advance using this recipe. I uncovered and subsequently fixed several bugs, such that my demo worked flawlessly. Management was impressed. I'm not making any promises here, but sincerely making your demo 100% runnable will greatly increase your odds.</p>
<h3>There's more...</h3>
<p>What is the secret to this recipe? It seems to be a bit short on code. While it's important to make the demo 100 percent runnable, the key is then printing out the test and using it like a script. That way, the only steps you are taking have already been proven to work.</p>
<h4>What if my manager likes to take detours?</h4>
<p>If your manager likes to ask lots of what-if questions that pull you off-script, then you are sailing into uncharted territory. Your odds for a successful demo may drop quickly.</p>
<p>You can politely dodge this by capturing their what-ifs for a future demo and try to keep the current one on track. If you take the plunge to try other things out, realize the risk you are taking.</p>
<p>Don't be afraid to promise a future demo where you will travel down the path requested instead of risking it in this demo. Managers are actually pretty open to accepting a response such as: <em>I haven't tested that yet. How about another demo next month where we cover that?</em>&nbsp;Failed demos leave a bad taste in the mouth of management and put your reputation in jeopardy. Successful ones have an equally positive effect on your reputation as a developer. Management tends to have a more optimistic view of seeing 70% of the system succeed 100% rather than 100% of the system succeed 70%.</p>
<p>This is where the line between engineer and manager needs to be observed. While managers want to see what's available, it is our job to show them what is currently working and give an accurate status on what is and isn't available. Asking to see something we haven't tested yet definitely warrants pushing back and telling them such a demo isn't ready yet.</p>

</div>



<!--Chapter 9-->

<div class="chapter" data-chapter-number="9">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 9 </span></div>
<h1 class="chaptertitle">Good Test Habits for New and Legacy Systems</h1>
<h3 class="author">Greg L. Turnquist & Bhaskar N. Das</h3>
</div>


<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Something is better than nothing</li>
<li>Coverage isn't everything</li>
<li>Be willing to invest in text fixtures</li>
<li>If you aren't convinced about the value of testing, your team won't be either</li>
<li>Harvesting metrics</li>
<li>Capturing a bug in an automated test</li>
<li>Separating algorithms from concurrency</li>
<li>Pausing to refactor when a test suite takes too long to run</li>
<li>Cashing in on your confidence</li>
<li>Be willing to throw away an entire day's changes</li>
<li>Instead of shooting for 100 percent coverage, try to have steady growth</li>
<li>Randomly breaking&nbsp;up your application can lead to better code</li>
</ul>
<h2>Introduction</h2>
<p>I hope you have enjoyed the previous chapters of this book. Up to this point, we have explored a lot of areas of automated testing, such as:</p>
<ul>
<li>Unit testing</li>
<li>Nose testing</li>
<li>Doctest testing</li>
<li>Behavior-driven development</li>
<li>Acceptance testing</li>
<li>Continuous integration</li>
<li>Smoke and load testing</li>
</ul>
<p>In this chapter, we will do something different. Instead of providing lots of code samples for various&nbsp;tips and tricks, I want to share some ideas I have picked up in my career as a software engineer.</p>
<p>All of the previous recipes in this book had very detailed steps on how to write the code, run it, and review its results. Hopefully, you have taken those ideas, expanded on them, improvised them, and, ultimately, applied them to help solve your own software problems.</p>
<p>In this chapter, let's explore some of the bigger ideas behind testing and how they can empower our development of quality systems.</p>
<h2>Something is better than nothing</h2>
<p>Don't get caught up in the purity of total isolation or worry about obscure test methods. The first thing to do is to start testing.</p>
<h3>How to do it...</h3>
<p>You have just been handed an application that was developed by others who are no longer with your company. Been there before? We all have, and probably on several occasions. Can we predict some of the common symptoms? Well, they could be similar to these:</p>
<ul>
<li>There are few (if any) automated tests.</li>
<li>There is little documentation.</li>
<li>There are chunks of code that have been commented out.</li>
<li>There are either no comments in the code, or there are comments that were written ages ago and are no longer correct.</li>
</ul>
<p>And here is the fun part&mdash;we don't know about all of these issues up front. We are basically told where to check the source tree, and to get cracking. For example, it's only when we run into an issue and seek&nbsp; documentation that we discover what does (or does not) exist.</p>
<p>Maybe I didn't catch everything you have encountered in that list, but I bet you've experienced a lot of those things. I don't want to sound like an embittered software developer, because I'm not. Not every project is like this. But I'm sure we have all had to deal with this at one time or another. So, what do we do? We start testing.</p>
<p>But the devil is in the details. Do we write a unit test? What about a thread test or an integration test? You know what? It doesn't matter what type of test we write. In fact, it doesn't even matter whether we use the right name.</p>
<p>When it's just you and the code sitting in a cubicle, terminology doesn't matter. Writing a test is what matters. If you can pick out one small unit of code and write a test, then go for it! But what if you picked up a jumbled piece of spaghetti code that doesn't come with nicely isolated units?</p>
<p>Consider a system where the smallest unit you can get hold of is a module that parses an electronic file and then stores the parsed results in a database. The parsed results aren't handed back through the API. They just silently, mysteriously end up in the database. How do we automate that? Well, we can do the following things:</p>
<ol>
<li>Write a test that starts by emptying all the tables relevant to the application.</li>
<li>Find one of your users who has one of these files and get a copy of it.</li>
<li>Add code to the test that invokes the top-level API to ingest the file.</li>
<li>Add some more code that pulls data out of the database and checks the results. (You may have to grab that user to make sure the code is working correctly.)</li>
</ol>
<p>Congratulations! You just wrote an automated test! It probably didn't qualify as a unit test. In fact, it may look kind of ugly to you, but so what? Maybe it took five minutes to run, but isn't that better than no test at all?</p>
<h3>How it works...</h3>
<p>Since the database is the place where we can assert results, we need to have a cleaned-out version before every run of our test. This will definitely require coordination if other developers are using some of the same tables. We may need our own schema allocated to us so that we can empty tables at will.</p>
<p>The modules probably suffer from a lack of cohesion and too much tight coupling. While we can try to identify why the code is bad, it doesn't advance our cause of building automated tests.</p>
<p>Instead, we must recognize that if we try to jump immediately into the unit level test, we will have to refactor the modules to support us. With little or no safety net, the risk is incredibly high, and we can feel it! If we tried to stick to a textbook unit test, then we would probably give up and consider automated testing an impossibility.</p>
<p>So, we have to take the first step and write an expensive, end-to-end, automated test to build the first link of the chain. That test may take a long time to run and may not be very comprehensive in what we can assert, but it's a start, and that is what's important. Hopefully, after making steady progress writing more tests like this, we will build up a safety net that will prevent us from having to go back and refactor this code.</p>
<h4>That can't be everything!</h4>
<p>Does <em>just write the test</em> sound a little too simple? Well, the concept is simple, but the work is going to be hard&mdash;very hard.</p>
<p>You will be forced to crawl through lots of APIs and find out exactly how they work. And, guess what? You probably won't be handed lots of intermediate results to assert. Understanding the API is just so that you can track down where the data travels to.</p>
<p>When I described the data of our situation as <em>mysteriously ending up in the database</em>, I was referring to the likelihood that the APIs you have probably weren't designed with lots of return values aimed at testability.</p>
<p>Just don't let anyone tell you that you are wasting your time building a long-running test case. An automated test suite that takes an hour to run and is exercised at least once a day probably instills more confidence than clicking through the screens manually. Something is better than nothing.</p>
<h3>See also</h3>
<ul>
<li>The <em>Cash in on your confidence</em> recipe</li>
</ul>
<h2>Coverage isn't everything</h2>
<p>You've figured out how to run coverage reports. However, don't assume that more coverage is automatically better. Sacrificing test quality in the name of coverage is a recipe for failure.</p>
<h3>How to do it...</h3>
<p><strong>Coverage reports</strong> provide good feedback. They tell us what is getting exercised and what is not. But, just because a line of code is exercised, that doesn't mean it is doing everything it is meant to do.</p>
<p>Are you ever tempted to brag about coverage percentage scores in the break room? Taking pride in good coverage isn't unwarranted, but when it leads to comparing different projects using these statistics, we are wandering into risky territory.</p>
<h3>How it works...</h3>
<p>Coverage reports are meant to be read in the context of the code they were run against. The reports show us what was covered and what was not, but this isn't where things stop. Instead, it's where they begin. We need to look at what was covered and analyze how well the tests exercised the system.</p>
<p>It's obvious that 0% coverage of a module indicates we have work to do. But what does it mean when we have 70% coverage? Do we need to code tests that go after the other 30%? Sure we do! But there are two different schools of thought on how to approach this. One is right, and one is wrong.</p>
<ul>
<li>The first approach is to write the new tests specifically targeting the uncovered parts while trying to avoid overlapping the original 70%. Redundantly testing code already covered in another test is an inefficient use of resources.</li>
<li>The second approach is to write the new tests so that they target scenarios the code is expected to handle, but that we haven't tackled yet. What was not covered should give us a hint about which scenarios haven't been tested yet.</li>
</ul>
<p>The right approach is the second one. OK, I admit I wrote that in a leading fashion. But the point is that it's very easy to look at what wasn't hit and write a test that aims to close the gap as fast as possible.</p>
<h3>There's more...</h3>
<p>Python gives us incredible power to monkey patch, inject alternate methods, and do other tricks to exercise the uncovered code. But doesn't this sound a little suspicious? Here are some of the risks we are setting ourselves up for:</p>
<ul>
<li>The new tests may be more brittle when they aren't based on sound scenarios.</li>
<li>A major change to our algorithms may require us to totally rewrite these tests.</li>
<li>Ever written mock-based tests? It's possible to mock the target system out of existence and end up just testing the mocks.</li>
<li>Even though some (or even most) of our tests may have good quality, the low-quality ones will cast our entire test suite as low quality.</li>
</ul>
<p>The coverage tool may not let us <em>get away</em> with some of these tactics if we do things that interfere with line counting mechanisms. But whether or not the coverage tool counts the code should not be the gauge by which we determine the quality of tests.</p>
<p>Instead, we need to look at our tests and see whether they are trying to exercise real-use cases we should be handling. When we are merely looking for ways to get more coverage percentage, we stop thinking about how our code is meant to operate, and that is not good.</p>
<h4>Are we not supposed to increase coverage?</h4>
<p>We are supposed to increase coverage by improving our tests, covering more scenarios, and removing code that's no longer supported. These things all lead us toward overall better quality. Increasing coverage for the sake of coverage doesn't lend itself to improving the quality of our system.</p>
<h4>But I want to brag about the coverage of my system!</h4>
<p>I think it's all right to celebrate good coverage. Sharing a coverage report with your manager is all right. But don't let it consume you.</p>
<p>If you start to post weekly coverage reports, double check your motives. The same goes if your manager requests postings as well.</p>
<p>If you find yourself comparing the coverage of your system to another system, then watch out! Unless you are familiar with the code of both systems, and really know more than the bottom line of the reports, you will probably wander into risky territory. You may be headed into the faulty competition that could drive your team to write brittle tests.</p>
<h2>Be willing to invest in test fixtures</h2>
<p>Spend time working on some test fixtures. You may not get a lot of tests written at first, but this investment will pay off.</p>
<h3>How to do it....</h3>
<p>When we start building a new greenfield project, it's a lot easier to write test-oriented modules, but when dealing with legacy systems, it may take more time to build a working test fixture. This may be tough to go through, but it's a valuable investment.</p>
<p>As an example, in the section <em>Something is better than nothing</em>, we talked about a system that scanned electronic files and put the parsed results into database tables. What steps would our test fixture require? Perhaps we should consider the following issues:</p>
<ul>
<li>Set up steps to clean out the appropriate tables.</li>
<li>Quite possibly, we may need to use code or a script to create a new database schema to avoid collisions with other developers.</li>
<li>It may be necessary to place&nbsp;the file in a certain location so the parser can find it.</li>
</ul>
<p>These are all steps that take time to build a working test case. More complex legacy systems may require even more steps to gear up for a test run.</p>
<h3>How it works...</h3>
<p>All of this can become intimidating and may push us to drop automated testing and just continue with clicking through the screens to verify things. But taking the time to invest in coding this fixture will begin to pay off as we write more test cases that use our fixture.</p>
<p>Have you ever built a test fixture and had to alter it for certain scenarios? After having developed enough test cases using our fixture, we will probably encounter another use case we need to test that exceeds the limits of our fixture. Since we are now familiar with it, it is probably easier to create another fixture.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>This is another way that coding the first fixture pays off. Future fixtures have a good chance of being easier to code. However, this isn't a cut-and-dried guarantee of improvement. Often, the first variation of our test fixture is a simple one.</p>
</div>
</div>
<h3>There's more...</h3>
<p>We will probably run into a situation where we need another test fixture that is totally different than what we've built. At this point, investing in the first test fixture doesn't have the same payoff. But, by this time, we will have become more seasoned test writers and have a better handle on what works and what doesn't when it comes to testing the system.</p>
<p>All the work done up to this point will have sharpened our skill set and that, in and of itself, is a great payoff for investing in the test fixture.</p>
<h4>Is this just about setting up a database?</h4>
<p>This is not just about setting up a database. If our system interacts extensively with an LDAP server, we may need to code a fixture that cleans out the directory structure and loads it up with test data.</p>
<p>If the legacy system is flexible enough, we can put this whole test structure into a sub node in the hierarchy. But it's just as likely that it expects the data to exist at a certain location. In that situation, we may have to develop a script that spins up a separate, empty LDAP server, and then shuts it down after the test is complete.</p>
<p>Setting up and tearing down an LDAP server may not be the fastest, nor the most efficient test fixture. But if we invest time into building this fixture to empower ourselves to write automated tests, we will eventually be able to refactor the original system to decouple it from a live LDAP server. And this whole process will sharpen our skill set. That is why creating the original test fixture truly is an investment.</p>
<h2>If you aren't convinced about the value of testing, your team won't be either</h2>
<p>Test-bitten developers exhibit zeal; they are excited to run their test suite and see things<br />complete with 100% success. This sort of emotion and pride tends to rub off on their fellow developers.</p>
<p>But the reverse is also true. If you aren't excited by all this and don't spread the word, none of your teammates will either. The idea of adding automated tests to your system will die a sad death.</p>
<p>This isn't just confined to my own personal experience. At the 2010 DevLink conference,&nbsp;I attended an open-space discussion about testing, and saw this sort of reaction among a&nbsp;dozen other developers I don't work with (<kbd>pythontestingcookbook.posterous.com</kbd>/<br /><kbd>greetings-programs</kbd>). The testers showed a certain type of excitement as they relayed<br />their experiences with testing. The ones that were on the fence about embracing automated testing were listening with glee, drinking it in. Those not interested simply weren't there for the discussion.</p>
<p>If you are reading this book (which of course you are), there is a fair chance you are the only person on your team seriously interested in automated testing. Your teammates may have heard of it, but aren't as bitten by the idea as you. To add it to your system will require a lot of investment by you, but don't confine yourself to just sharing the code; consider the following:</p>
<ul>
<li>Demonstrate the excitement you feel as you make progress, and tackle thorny issues.</li>
<li>Share your test results by posting them on your walls, where others can see them.</li>
<li>Talk about your accomplishments while chatting with co-workers in the break room.</li>
</ul>
<p>Testing isn't a cold, mechanical process; it's an exciting, fiery area of development. Test-bitten developers can't wait to share it with others. If you look for ways to spread the fire of automated testing, eventually others will warm up to it, and you will find yourself talking about new testing techniques with them.</p>
<h2>Harvesting metrics</h2>
<p>Start a spreadsheet that shows lines, code, a&nbsp;number of tests, the total test execution time, and the number of bugs, and track this with every release. The numbers will defend your investment.</p>
<h3>How to do it...</h3>
<p>These high-level steps show how to capture metrics over time:</p>
<ol>
<li>Create a spreadsheet to track the number of test cases, the time&nbsp;taken to run the test suite, the date of the test run, any bugs, and the average time per test.</li>
<li>Check the spreadsheet into your code base as another controlled artifact.</li>
<li>Add some graphs to show the curve of the test time versus the test quantity.</li>
<li>Add a new row of data at least each time you do a release. If you can capture data more often, such as once a week or even once a day, that is better.</li>
</ol>
<h3>How it works...</h3>
<p>As you write more tests, the test suite will take longer to run. But you will also find that the number of bugs tends to decrease. The more testing you do, and the more often you do it, the better your code will be. Capturing the metrics of your testing can act as hard evidence that the time spent writing and running tests is a well-placed investment.</p>
<h3>There's more...</h3>
<p>Why do I need this document? Don't I already know that testing works? Think of it as a&nbsp;backup for your assertion of quality. Months down the road, you may be challenged by management to speed things up. Maybe they need something faster, and they think you are simply spending too much time on this <em>testing stuff</em>.</p>
<p>If you can pull out your spreadsheet and show how bugs decreased with testing effort, they will have little to argue with. But if you don't have this, and simply argue that <em>testing makes things better</em>, you may lose the argument.</p>
<h4>Metrics aren't just for defending yourself to management</h4>
<p>I personally enjoyed seeing the tests grow and the bugs decline. It was a personal way to&nbsp;track myself and keep a handle on how much progress was made. And, to be honest, my last manager gave me full support for automated testing. He had his own metrics of success, so I never had to pull out mine.</p>
<h2>Capturing a bug in an automated test</h2>
<p>Before you fix that one-line bug you spotted, write an automated test instead, and make sure it's repeatable. This helps to build up insulation from our system, regressing back into failures we fixed in the past.</p>
<h3>How to do it...</h3>
<p>These high-level steps capture the workflow of capturing bugs in automated tests before<br />we fix them:</p>
<ol>
<li>When a new bug is discovered, write a test case that recreates it. It doesn't matter if the test case is long-running, complex, or integrates with lots of components. The&nbsp;critical thing is to reproduce the bug.</li>
<li>Add the bug to your suite of tests.</li>
<li>Fix the bug.</li>
<li>Verify that the test suite passes before checking your changes.</li>
</ol>
<h3>How it works...</h3>
<p>The simplest way to introduce automated testing to an application that never had it before is to test one bug at a time. This method ensures that newly discovered bugs won't sneak back into the system later on.</p>
<p>The tests may have a loose-knit feel instead of a comprehensive one, but that doesn't matter. What does matter is that, over time, you will slowly develop a solid safety net of test cases that verify that the system performs as expected.</p>
<h3>There's more...</h3>
<p>I didn't say this would be easy. Writing an automated test for software that wasn't built with testability in mind is hard work. As mentioned in the recipe <em>Something is better than nothing</em>, the first test case is probably the hardest. But, over time, as you develop more tests, you will gain the confidence to go back and refactor things. You will definitely feel empowered by knowing that you can't break things without realizing.</p>
<h4>When the time comes to add a completely new module, you will&nbsp;be ready for it</h4>
<p>This approach of capturing a bug with a test case is useful, but slow. But that's OK, because slowly adding testing will give you time to grow your testing skills at a comfortable pace.</p>
<p>Where does this pay off? Well, eventually, you will need to add a new module to your&nbsp;system. Doesn't this always happen? By that time, your investment in testing and test fixtures should already be paying dividends in the improvement of the quality of existing code, but you will also have a head start on testing the new module. Consider the following:</p>
<ul>
<li>You will not just know, but <em>really understand</em>, the meaning of <em>test-oriented code.</em></li>
<li>You will be able to write both the code and its tests at the same time in a very effective way.</li>
<li>The new module will have a head start of higher quality and will not require as much effort to <em>catch up</em> as the legacy parts of your system did.</li>
</ul>
<h4>Don't give into the temptation to skip testing</h4>
<p>As I stated earlier, the first test case will be very hard to write. And the next few after that&nbsp;won't be much easier. This makes it very tempting to throw up your hands and skip automated testing. But, if you stick with it and write something that works, you can continue building on that successful bit of effort.</p>
<p>This may sound like a clich&eacute;, but if you stick with it for about a month, you will start to see some results from your work. This is also a great time to start <strong>harvesting metrics</strong>. Capturing your progress and being able to reflect on it can provide positive encouragement.</p>
<h2>Separating algorithms from concurrency</h2>
<p>Concurrency is very hard to test, but most algorithms are not when decoupled.</p>
<h3>How to do it...</h3>
<p>Herb Sutter wrote an article in 2005 entitled <em>The Free Lunch Is Over</em>, where he pointed out how microprocessors are approaching a physical limitation in serial processing, which will be forcing&nbsp;developers to turn towards concurrent solutions (<a href="http://www.gotw.ca/publications/concurrency-ddj.htm">http://www.gotw.ca/publications/concurrency-ddj.htm</a>).</p>
<p>Newer processors come with multiple cores. To build scalable applications, we can no longer just wait for a faster chip. Instead, we must use alternate, concurrent techniques. This issue is being played out in a whole host of languages. Erlang was one of the first languages on the&nbsp;scene that allowed a telecommunications system to be built with nine 9's of availability, which&nbsp;means about one second of downtime every 30 years.</p>
<p>One of its key features is the use of immutable data sent between actors. This provides nice isolation and allows multiple units to run across the CPU cores. Python has libraries that&nbsp;provide a similar style of decoupled, asynchronous message passing. The two most common&nbsp;ones are Twisted and Kamaelia.</p>
<p>But, before you dive into using either of these frameworks, there is something important to&nbsp;keep in mind: it's very hard to test concurrency while also testing algorithms. To use these&nbsp;libraries, you will register code that issues messages and also registers handlers to process&nbsp;messages.</p>
<h3>How it works...</h3>
<p>It's important to decouple the algorithms from the machinery of whatever concurrency&nbsp;library you pick. This will make it much easier to test the algorithms, but it doesn't mean that you&nbsp;shouldn't conduct load tests or try to overload your system with live data playback scenarios.</p>
<p>What it does mean is that starting with large volume test scenarios is the wrong priority. Your&nbsp;system needs to correctly handle one event in an automated test case before it can handle&nbsp;a thousand events.</p>
<h4>Research test options provided by your concurrency frameworks</h4>
<p>A good concurrency library should provide sound testing options. Seek them out and try to use&nbsp;them to their fullest. But don't forget to verify that your custom algorithms work in simple,&nbsp;serial fashion as well. Testing both sides will give you great confidence that the system is&nbsp;performing as expected under light and heavy loads.</p>
<h2>Pause to refactor when a test suite takes too long to run</h2>
<p>As you start to build a test suite, you may notice the runtime getting quite long. If it's so long that you aren't willing to run it at least once a day, you need to stop coding and focus on&nbsp;speeding up the tests, whether it involves the tests themselves or the code being tested.</p>
<h3>How to do it...</h3>
<p>This assumes you have started to build a test suite using some of the following practices:</p>
<ul>
<li>Something is better than nothing</li>
<li>Be willing to invest in test fixtures</li>
<li>Capturing a bug in an automated test</li>
</ul>
<p>These are slow-starting steps to start adding tests to a system that was originally built without&nbsp;any automated testing. One of the trade-offs to get moving on automated testing involves&nbsp;writing relatively expensive tests. For instance, if one of your key algorithms is not adequately&nbsp;decoupled from the database, you will be forced to write a test case that involves setting up&nbsp;some tables, processing the input data, and then making queries against the state of the&nbsp;database afterward.</p>
<p>As you write more tests, the time to run the test suite will certainly grow. At some point, you will feel less inclined to spend the time waiting for your test suite to run. Since a test suite is only&nbsp;good when used, you must pause development and pursue refactoring either the code or the&nbsp;test cases themselves to speed things up.</p>
<p>Here is a problem I ran into: my test suite initially took about 15 minutes to run. It eventually&nbsp;grew to take one-and-a-half hours to run all the tests. I reached a point where I would only run&nbsp;it once a day, and even skipped some days. One day, I tried to do a massive code edit. When&nbsp;most of the test cases failed, I realized that I had not run the test suite often enough to detect&nbsp;which step broke things. I was forced to throw away all the code edits and start over. Before&nbsp;proceeding further, I spent a few days refactoring the code as well as the tests, bringing the&nbsp;run time of the test suite back down to a tolerable 30 minutes.</p>
<h3>How it works...</h3>
<p>That is the key measurement: when you feel hesitant to run the test suite more than <em>once&nbsp;</em><em>a day</em>, this may be a sign that things need to be cleaned up. Test suites are meant to be run&nbsp;multiple times a day.</p>
<p>This is because we have competing interests: <em>writing code</em> and <em>running tests</em>. It's important to recognize these things:</p>
<ul>
<li>To run tests, we must suspend our coding efforts</li>
<li>To write more code, we must suspend testing efforts</li>
</ul>
<p>When testing takes a big chunk of our daily schedule, we must start choosing which is more important. We tend to migrate toward writing more code, and this is probably the key reason people abandon automated testing and consider it unsuitable for their situation.</p>
<p>It's tough, but if we can resist taking the easy way out, and instead do some refactoring of&nbsp;either the code or our tests, we will be encouraged to run the tests more often.</p>
<h3>There's more...</h3>
<p>It's less science and more voodoo when it comes to what to refactor. It's important to seek out opportunities that give us a good yield. It's important to understand that this can be either&nbsp;our test code, our production code, or a combination of both that needs to be refactored. Consider the following points:</p>
<ul>
<li>Performance analysis can show us where the hotspots are. Refactoring or rewriting these chunks can improve tests.</li>
<li>Tight coupling often forces us to put&nbsp;in more parts of the system than we want, such&nbsp;as database usage. If we can look for ways to decouple the code from the database and replace it with mocks or stubs, that sets us up to update the relevant tests to come up with a faster running test suite.</li>
</ul>
<p>Coverage obtained from tests can help. All of these approaches have positive consequences&nbsp;for our code's quality. More efficient algorithms lead to better performance, and looser&nbsp;coupling helps to keep our long-term maintenance costs down.</p>
<h3>See also</h3>
<ul>
<li><em>Be willing to throw away an entire day's changes</em></li>
</ul>
<h2>Cash in on your confidence</h2>
<p>After building up enough tests, you will feel confident enough to rewrite a big chunk of code or conduct shotgun surgery that touches almost every file. Go for it!</p>
<h3>How to do it...</h3>
<p>As you build more tests and run them several times a day, you will start to get a feel for what you know and don't know about the system. Even more so, when you've written enough expensive, long-running tests about a particular part of the system, you will feel a strong&nbsp;desire to rewrite that module.</p>
<p>What are you waiting for? This is the point of building a runnable safety net of tests.&nbsp;Understanding the ins and outs of a module gives you the knowledge to attack it. You may rewrite it, be able to better decouple its parts, or whatever else is needed to make it work better, as well as being able to better support tests.</p>
<h3>How it works...</h3>
<p>While you may feel a strong desire to attack the code, there may be an equal and opposing&nbsp;feeling to resist making such changes. This is risk aversion, and we all have to deal with it. We want to avoid diving in head first to a situation that could have drastic consequences.</p>
<p>Assuming we have built an adequate safety net, it's time to engage the code and start&nbsp;cleaning it up. If we run the test suite frequently while making these changes, we can safely move through the changes we need to make. This will improve the quality of the code and will possibly speed up the runtime of the test suite.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p><strong>While making changes, we don't have to go all in</strong>Cashing in on our confidence means we move in and make changes&nbsp;to the code base, but it doesn't mean we go into areas of code where&nbsp;the tests are shallow and inadequate. There may be several areas we&nbsp;want to clean up, but we should only go after the parts we are most&nbsp;confident about. There will be future opportunities to get the other&nbsp;parts as we add more tests in the future.</p>
</div>
</div>
<h2>Be willing to throw away an entire day's&nbsp;changes</h2>
<p>Have you ever worked for a whole day making changes, only to discover that half the tests failed because you forgot to run the test suite more often? Be ready to throw away the changes. This is what automated testing lets us do&hellip; back up to when everything ran perfectly. It will hurt, but next time you will remember to run the test suite more often.</p>
<h3>How to do it...</h3>
<p>This recipe assumes you are using version control and are making regular commits. This idea&nbsp;is no good if you haven't made a commit for two weeks.</p>
<p>If you run your test suite at least once a day, and when it passes, you commit the changes you have made, then it becomes easy to back up to some previous point, such as the beginning of the day.</p>
<p>I have done this many times. The first time was the hardest. It was a new idea to me, but I&nbsp;realized the real value of software was now resting on my automated test suite. In the middle of the afternoon, I ran the test suite for the first time that day after having edited half the system. Over half of the tests failed.</p>
<p>I tried to dig in and fix the issue. The trouble was, I couldn't figure out where the issue&nbsp;stemmed from. I spent a couple of hours trying to track it down. It began to dawn on me&nbsp;that I wasn't going to figure it out without wasting loads of time.</p>
<p>But I remembered that everything had passed with flying colors the previous day. I finally&nbsp;decided to throw away my changes, run the test suite, verifying everything passed, and then grudgingly go home for the day.</p>
<p>The next day, I attacked the problem again. Only, this time, I ran the tests more often. I was able to get it coded successfully. Looking back at the situation, I realize that this issue only cost me one lost day. If I had tried to ride it out, I could have spent a week and <em>still</em> probably ended up throwing things away.</p>
<h3>How it works...</h3>
<p>Depending on how your organization manages source control, you may have to do the following things:</p>
<ul>
<li>Simply do it yourself by deleting a branch or canceling your checkouts</li>
<li>Contact your CM team to delete the branch or the commits you made for the day</li>
</ul>
<p>This isn't really a technical issue. The source control system makes it easy to do this&nbsp;regardless of who is in charge of branch management. The hard part is making the decision to throw away the changes. We often feel the desire to fix what is broken. The more our efforts cause it to break further, the more we want to fix it. At some point, we must realize that it is more costly to move forward rather than to back up and start again.</p>
<p>There is an axis of agility that stretches from classic waterfall software production to heavily agile processes. Agile teams tend to work in smaller sprints and commit in smaller chunks. This makes it more palatable to throw away a day of work. The bigger the task and the longer the release cycle, the greater the odds are that your changes haven't been checked since you started a task two weeks ago.</p>
<p>Believe me, throwing away two weeks' work is totally different than throwing away one day's worth. I would never advocate throwing out two weeks' work.</p>
<p>The core idea is to <em>not</em> go home without your test suite passing. If that means you have to throw things away to make it happen, then that is what you must do. It really drives the point home of <em>code a little/test a little</em> until a new feature is ready for release.</p>
<h3>There's more...</h3>
<p>We also need to reflect on why didn't we run the test suite often enough. It may be because the test suite is taking too long to run, and you are hesitating to use up that time. It may be time to <em>pause to refactor when the test suite takes too long to run</em>. The time I really learned this lesson was when my test suite took one-and-a-half hours to run. After I got through this whole issue, I realized that I needed to speed things up and spent probably a week or two cutting it down to a tolerable 30 minutes.</p>
<h4>How does this mesh with "Something is better than nothing"</h4>
<p>Earlier in this chapter, we talked about writing a test case that may be quite expensive to run to get automated testing in action. What if our testing becomes so expensive that it is time prohibitive? After all, couldn't what we just said lead to the situation we are dealing with?</p>
<p><em>Code a little/test a little</em> may seem to be a very slow way to proceed. This is probably the reason many legacy systems never embrace automated testing. The hill we must climb is steep. But if we can hang in there, start building the tests, make sure they run at the end of the day, and then eventually pause to refactor our code and tests, we can eventually reach a happy balance of better code quality and system confidence.</p>
<h3>See also</h3>
<ul>
<li>Something is better than nothing</li>
<li>Pause to refactor when a test suite takes too long</li>
</ul>
<h2>Instead of shooting for 100 percent&nbsp;coverage, try to have a steady growth</h2>
<p>You won't know how you're doing without coverage analysis. However, don't aim too high. Instead, focus on a gradual increase. You will find that your code gets better over time&mdash;maybe even drops in volume&mdash;while the quality and coverage steadily improve.</p>
<h3>How to do it...</h3>
<p>If you start with a system that has no tests, don't get focused on a ridiculously high number. I worked on a system that had 16% coverage when I picked it up. A year later, I had worked it up to 65%. This was nowhere near 100%, but the quality of the system had grown in leaps and bounds due to capturing a bug in an automated test and harvesting metrics.</p>
<p>At one time, I was discussing the quality of my code with my manager, and he showed me a report he had developed. He had run a code-counting tool on every release of&nbsp;every application he was overseeing. He said my code counts had a unique shape. All the<br />other tools had a constant increase in lines of code. Mine had grown, peaked, and then&nbsp;started to decrease and were still on the decline.</p>
<p>This happened despite the fact that my software did more than ever. It's because I started&nbsp;throwing away unused features, bad code, and clearing out cruft during refactorings.</p>
<h3>How it works...</h3>
<p>By slowly building an automated test suite, you will gradually cover more of your code. By&nbsp;keeping a focus on building quality code with corresponding tests, the coverage will grow&nbsp;naturally. When we shift to focusing on coverage reports, the numbers&nbsp;may grow more quickly, but it will tend to be more artificial.</p>
<p>From time to time, as you cash in on your confidence and rewrite chunks, you should feel&nbsp;empowered to throw away old junk. This will also grow your coverage metrics in a healthy way.</p>
<p>All of these factors will lead to increased quality and efficiency. While your code may eventually peak and then decrease, it isn't unrealistic for it to eventually grow again due to new features.</p>
<p>By that time, the coverage will probably be much higher, because you will be building&nbsp;completely new features, hand in hand with tests, instead of just maintaining legacy parts.</p>
<h2>Randomly breaking&nbsp;up your app can lead to&nbsp;better code</h2>
<blockquote>
<p>"<em>The best way to avoid failure is to fail constantly.</em>"</p>
<p>&ndash; Netflix</p>
</blockquote>
<h3>How to do it...</h3>
<p>Netflix has built a tool they call&nbsp;<strong>Chaos Monkey</strong>. Its job is to randomly kill instances and&nbsp;services. This forces developers to make sure their system can fail smoothly and safely. To build our own version of this, some of the things we would need it to do are the following:</p>
<ul>
<li>Randomly kill processes</li>
<li>Inject faulty data at interface points</li>
<li>Shut down network interfaces between distributed systems</li>
<li>Issue shutdown commands to subsystems</li>
<li>Create denial-of-service attacks by overloading interface points with too much data</li>
</ul>
<p>This is a starting point. The idea is to inject errors wherever you can imagine them happening. This may require writing scripts, Cron jobs, or any means necessary to cause these errors to happen.</p>
<h3>How it works...</h3>
<p>Given that there is a chance of a remote system being unavailable in production, we should&nbsp;introduce ways for this to happen in our development environment. This will encourage&nbsp;us to code higher fault tolerance into our system.</p>
<p>Before we introduce a random-running Chaos Monkey such as Netflix has, we need to ensure that our system can handle these situations manually. For example, if our system includes&nbsp;communication between two servers, a fair test is unplugging the network cable to one box, simulating network failure. When we verify that our system can continue working with acceptable means, then we can add scripts to do this automatically and, eventually, randomly.</p>
<p>Audit logs are valuable tools to verify that our system is handling these random events. If we can read a log entry showing a forced network shutdown and then see log entries of similar timestamps, we can easily evaluate whether the system handled the situation.</p>
<p>After building that in, we can work on the next error to randomly introduce into the system. By following this cycle, we can build up the robustness of our system.</p>
<h3>There's more...</h3>
<p>This doesn't exactly fit into the realm of automated testing. This is also very high level. It's hard to go into much more detail, because the type of faulty data to inject requires an intimate understanding of the actual system.</p>
<h3>How does this compare to fuzz testing?</h3>
<p><strong>Fuzz testing</strong> is a style of testing where invalid, unexpected, and random data is injected into input points of our software (<a href="http://en.wikipedia.org/wiki/Fuzz_testing">http://en.wikipedia.org/wiki/Fuzz_testing</a>). If the application fails, this is considered a failure. If it doesn't, then it has passed. This type of testing goes in a similar direction, but the blog article written by Netflix appears to go much further than simply injecting different data. It talks about killing instances and interrupting distributed communications. Basically, anything you can think of that could happen in production, we should try to replicate in a test bed. <strong>Fusil</strong> (<a href="https://bitbucket.org/haypo/fusil">https://bitbucket.org/haypo/fusil</a>) is a Python tool that aims to provide fuzz testing. You may want to investigate whether it is useful for your project needs.</p>
<h3>Are there any tools to help with this?</h3>
<p><strong>Jester</strong> (for Java), <strong>Pester</strong> (for Python), and <strong>Nester</strong> (for C#) are used to conduct mutation testing (<a href="http://jester.sourceforge.net/">http://jester.sourceforge.net/</a>). These tools find out what code is not covered by&nbsp;test cases, alter the source code, and rerun the test suites. Finally, they give a report on what was changed, what passed, and what didn't pass. It can illuminate what is and is not covered by our test suites in ways coverage tools can't.</p>
<p>This isn't a complete Chaos Monkey, but it provides one area of assistance by trying to&nbsp;<em>break the system</em> and force us to improve our test regime. To really build a full-blown system probably wouldn't fit inside a test project, because it requires writing custom scripts based on the environment it's meant to run in.</p>

<h3>Leave a review - let other readers know what you think</h3>
<p>Please share your thoughts on this book with others by leaving a review on the site that you bought it from. If you purchased the book from Amazon, please leave us an honest review on this book's Amazon page. This is vital so that other potential readers can see and use your unbiased opinion to make purchasing decisions, we can understand what our customers think about our products, and our authors can see your feedback on the title that they have worked with Packt to create. It will only take a few minutes of your time, but is valuable to other potential customers, our authors, and Packt. Thank you!</p>
</div>


