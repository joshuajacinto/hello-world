
<!--Preface-->

<div class="frontmatters">
<div class="page-frontmatter preface front set-front-matter">
<div class="title-name">
<h2 id="maintitle">Python High Performance</h2>
</div>

<p>Build high-performing, concurrent, and distributed applications</p>
<p>Gabriele Lanaro</p>

<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000029.png" class="lazyload" /></p>

<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BIRMINGHAM - MUMBAI</strong></p>


<p>Copyright &copy; 2017 Packt Publishing</p>
<p>All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.</p>
<p>Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.</p>
<p>Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.</p>
<p>First published: December 2013</p>
<p>Second edition:&nbsp;May&nbsp;2017</p>
<p>Production reference: 2250517</p>
<p>Published by Packt Publishing Ltd.</p>
<p>Livery Place</p>
<p>35 Livery Street</p>
<p>Birmingham&nbsp;</p>
<p>B3 2PB, UK.</p>
<p>ISBN 978-1-78728-289-6</p>
<p><a href="http://www.packtpub.com" target="_blank" rel="noopener">www.packtpub.com</a></p>

<h2>Credits</h2>

<table>
<tbody>
<tr>
<td>
<p><strong>Author</strong></p>
<p>Gabriele Lanaro</p>
</td>
<td>
<p><strong>Copy Editor</strong></p>
<p>Shaila Kusanale</p>
</td>
</tr>
<tr>
<td>
<p><strong>Reviewer</strong></p>
<p>Will Brennan</p>
</td>
<td>
<p><strong>Project Coordinator</strong></p>
<p>Ulhas Kambali</p>
</td>
</tr>
<tr>
<td>
<p><strong>Commissioning Editor</strong></p>
<p>Kunal Parikh</p>
</td>
<td>
<p><strong>Proofreader</strong></p>
<p>Safis Editing</p>
</td>
</tr>
<tr>
<td>
<p><strong>Acquisition Editor</strong></p>
<p>Chaitanya Nair</p>
</td>
<td>
<p><strong>Indexer</strong></p>
<p>Tejal Daruwale Soni</p>
</td>
</tr>
<tr>
<td>
<p><strong>Content Development Editor</strong></p>
<p>Vikas Tiwari</p>
</td>
<td>
<p><strong>Graphics</strong></p>
<p>Abhinash Sahu</p>
</td>
</tr>
<tr>
<td>
<p><strong>Technical Editor</strong></p>
<p>Jijo Maliyekal</p>
</td>
<td>
<p><strong>Production Coordinator</strong></p>
<p>Shantanu Zagade</p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;&emsp;</p>

<h3>About the Author</h3>
<p><strong>Dr. Gabriele Lanaro</strong> has been conducting research to study the formation and growth of crystals using medium and large-scale computer simulations. In 2017, he obtained his PhD in theoretical chemistry. His interests span machine learning, numerical computing visualization, and web technologies. He has a sheer passion for good software and is the author of the chemlab and chemview open source packages. In 2013, he authored the first edition of the book &ldquo;<em>High Performance Python Programming</em>&rdquo;.</p>
<p>I'd like to acknowledge the support from Packt editors, including Vikas Tiwari. I would also like to thank my girlfriend, Harani, who had to tolerate the way-too-long writing nights, and friends who provided company and support throughout. Also, as always, I&rsquo;d love to thank my parents for giving me the opportunity to pursue my ambitions.</p>
<p>Lastly, I would like to thank Blenz coffee for powering the execution engine of this book through electricity and caffeine.</p>

<h3>About the Reviewer</h3>
<p><strong>Will Brennan</strong> is a C++/Python developer based in London with previous experience in writing molecular dynamics simulations. He&nbsp;is currently working on high-performance image processing and machine learning applications. You can refer to his repositories at&nbsp;<a href="https://github.com/WillBrennan">https://github.com/WillBrennan</a>.</p>





<h2>Preface</h2>
<p>The Python programming language has seen a huge surge in popularity in recent years, thanks to its intuitive, fun syntax, and its vast array of top-quality third-party libraries. Python has been the language of choice for many introductory and advanced university courses as well as for numerically intense fields, such as the sciences and engineering. Its primary applications also lies in machine learning, system scripting, and web applications.</p>
<p>The reference Python interpreter, CPython, is generally regarded as inefficient when compared to lower-level languages, such as C, C++, and Fortran. CPython&rsquo;s poor performance lies in the fact that the program instructions are processed by an interpreter rather than being compiled to efficient machine code. While using an interpreter has several advantages, such as portability and the additional compilation step, it does introduce an extra layer of indirection between the program and the machine, which causes a less efficient execution.</p>
<p>Over the years, many strategies have been developed to overcome CPython's performance shortcomings. This book aims to fill this gap and will teach how to consistently achieve strong performance out of your Python programs.</p>
<p>This book will appeal to a broad audience as it covers both the optimization of numerical and scientific codes as well as strategies to improve the response times of web services and applications.</p>
<p>The book can be read cover-to-cover ; however, chapters are designed to be self-contained so that you can skip to a section of interest if you are already familiar with the previous topics.</p>

<h2>What this book covers</h2>
<p>Chapter 1<em>, Benchmark and Profiling</em>, will teach you how to assess the performance of Python programs and practical strategies on how to identify and isolate the slow sections of your code.</p>
<p>Chapter 2<em>, Pure Python Optimizations</em>, discusses how to improve your running times by order of magnitudes using the efficient data structures and algorithms available in the Python standard library and pure-Python third-party modules.</p>
<p>Chapter 3<em>, Fast Array Operations with NumPy and Pandas</em>, is a guide to the NumPy and Pandas packages. Mastery of these packages will allow you to implement fast numerical algorithms with an expressive, concise interface.</p>
<p>Chapter 4<em>, C Performance with Cython</em>, is a tutorial on Cython, a language that uses a Python-compatible syntax to generate efficient C code.</p>
<p>Chapter 5, <em>Exploring Compilers</em>, covers tools that can be used to compile Python to efficient machine code. The chapter will teach you how to use Numba, an optimizing compiler for Python functions, and PyPy, an alternative interpreter that can execute and optimize Python programs on the fly.</p>
<p>Chapter 6<em>, Implementing Concurrency</em>, is a guide to asynchronous and reactive programming. We will learn about key terms and concepts, and demonstrate how to write clean, concurrent code using the asyncio and RxPy frameworks.</p>
<p>Chapter 7<em>, Parallel Processing</em>, is an introduction to parallel programming on multi-core processors and GPUs. In this chapter, you will learn to achieve parallelism using the multiprocessing module and by expressing your code using Theano and Tensorflow.</p>
<p>Chapter 8<em>, Distributed Processing</em>, extends the content of the preceding chapter&nbsp;by focusing on running parallel algorithms on distributed systems for large-scale problems and big data. This chapter will cover the Dask, PySpark, and mpi4py libraries.</p>
<p>Chapter 9<em>, Designing for High Performance</em>, discusses general optimization strategies and best practices to develop, test, and deploy your high-performance Python applications.</p>

<h2>What you need for this book</h2>
<p>The software in this book is tested on Python version 3.5 and on Ubuntu version 16.04. However, majority of the examples can also be run on the Windows and Mac OS X operating systems.</p>
<p>The recommended way to install Python and the associated libraries is through the Anaconda distribution, which can be downloaded from <a href="https://www.continuum.io/downloads">https://www.continuum.io/downloads</a>, for Linux, Windows, and Mac OS X.</p>

<h2>Who this book is for</h2>
<p>The book is aimed at Python developers who want to improve the performance of their application; basic knowledge of Python is expected.</p>

<h2>Conventions</h2>
<p>In this book, you will find a number of text styles that distinguish between different kinds of information. Here are some examples of these styles and an explanation of their meaning.</p>
<p>Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "To summarize, we will implement a method called <kbd>ParticleSimulator.evolve_numpy</kbd> and benchmark it against the pure Python version, renamed as <kbd>ParticleSimulator.evolve_python</kbd>"</p>
<p>A block of code is set as follows:</p>
<pre><code class="lang-python">    def square(x):
    return x * x

    inputs = [0, 1, 2, 3, 4]
    outputs = pool.map(square, inputs)
</code></pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre><code class="lang-python">    def square(x):
    return x * x

    inputs = [0, 1, 2, 3, 4]
    outputs = pool.map(square, inputs)
</code></pre>
<p>Any command-line input or output is written as follows:</p>
<pre><code>$ time python -c 'import pi; pi.pi_serial()' 
real 0m0.734s
user 0m0.731s
sys 0m0.004s
</code></pre>
<p><strong>New terms</strong> and <strong>important words</strong> are shown in bold. Words that you see on the screen, for example, in menus or dialog boxes, appear in the text like this: "On the right, clicking on the tab Callee Map will display a diagram of the function costs."</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Warnings or important notes appear in a box like this.</p>
</div>
</div>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>Tips and tricks appear like this.</p>
</div>
</div>

<h2>Reader feedback</h2>
<p>Feedback from our readers is always welcome. Let us know what you think about this book-what you liked or disliked. Reader feedback is important for us as it helps us develop titles that you will really get the most out of.</p>
<p>To send us general feedback, simply e-mail&nbsp;<kbd>feedback@packtpub.com</kbd>, and mention the book's title in the subject of your message.</p>
<p>If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, see our author guide at&nbsp;<a href="http://www.packtpub.com/authors" target="_blank" rel="noopener">www.packtpub.com/authors</a>.</p>

<h2>Customer support</h2>
<p>Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.</p>

<h3>Downloading the example code</h3>
<p>You can download the example code files for this book from your account at <a href="http://www.packtpub.com" target="_blank" rel="noopener">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support" target="_blank" rel="noopener">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register to our website using your e-mail address and password.</li>
<li>Hover the mouse pointer on the SUPPORT tab at the top.</li>
<li>Click on Code Downloads &amp; Errata.</li>
<li>Enter the name of the book in the Search box.</li>
<li>Select the book for which you're looking to download the code files.</li>
<li>Choose from the drop-down menu where you purchased this book from.</li>
<li>Click on Code Download.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR / 7-Zip for Windows</li>
<li>Zipeg / iZip / UnRarX for Mac</li>
<li>7-Zip / PeaZip for Linux</li>
</ul>
<p>The code bundle for the book is also hosted on GitHub at <a href="https://github.com/PacktPublishing/Python-High-Performance-Second-Edition">https://github.com/PacktPublishing/Python-High-Performance-Second-Edition</a>. We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>

<h3>Downloading the color images of this book</h3>
<p>We also provide you with a PDF file that has color images of the screenshots/diagrams used in this book. The color images will help you better understand the changes in the output. You can download this file from <a href="https://www.packtpub.com/sites/default/files/downloads/PythonHighPerformanceSecondEdition_ColorImages.pdf">https://www.packtpub.com/sites/default/files/downloads/PythonHighPerformanceSecondEdition_ColorImages.pdf</a>.</p>

<h3>Errata</h3>
<p>Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you find a mistake in one of our books-maybe a mistake in the text or the code-we would be grateful if you could report this to us. By doing so, you can save other readers from frustration and help us improve subsequent versions of this book. If you find any errata, please report them by visiting <a href="http://www.packtpub.com/submit-errata" target="_blank" rel="noopener">http://www.packtpub.com/submit-errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details of your errata. Once your errata are verified, your submission will be accepted and the errata will be uploaded to our website or added to any list of existing errata under the Errata section of that title.</p>
<p>To view the previously submitted errata, go to <a href="https://www.packtpub.com/books/content/support" target="_blank" rel="noopener">https://www.packtpub.com/books/content/support</a> and enter the name of the book in the search field. The required information will appear under the Errata section.</p>

<h3>Piracy</h3>
<p>Piracy of copyrighted material on the Internet is an ongoing problem across all media. At Packt, we take the protection of our copyright and licenses very seriously. If you come across any illegal copies of our works in any form on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.</p>
<p>Please contact us at&nbsp;<kbd>copyright@packtpub.com</kbd>&nbsp;with a link to the suspected pirated material.</p>
<p>We appreciate your help in protecting our authors and our ability to bring you valuable content.</p>

<h3>Questions</h3>
<p>If you have a problem with any aspect of this book, you can contact us at&nbsp;<kbd>questions@packtpub.com</kbd>, and we will do our best to address the problem.</p>
</div>
</div>



<!--Chapter 1-->


<div class="chapter" data-chapter-number="1">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 1 </span></div>
<h1 class="chaptertitle">Benchmarking and Profiling</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>Recognizing the slow parts of your program is the single most important task when it comes to speeding up your code. Luckily, in most cases, the code that causes the application to slow down is a very small fraction of the program. By locating those critical sections, you can focus on the parts that need improvement without wasting time in micro-optimization.</p>
<p><strong>Profiling</strong> is the technique that allows us to pinpoint the most resource-intensive spots in an application. A <strong>profiler</strong> is a program that runs an application and monitors how long each function takes to execute, thus detecting the functions in which your application spends most of its time.</p>
<p>Python provides several tools to help us find these bottlenecks and measure important&nbsp;performance metrics. In this chapter, we will learn how to use the standard <kbd>cProfile</kbd> module and the&nbsp;<kbd>line_profiler</kbd>&nbsp;third-party package.&nbsp; We will also learn how to profile an application's memory consumption through the&nbsp; <kbd>memory_profiler</kbd> tool. Another useful tool that we will cover is <em>KCachegrind</em>, which can be used&nbsp;to graphically display the data produced by various profilers.</p>
<p><strong>Benchmarks</strong> are small scripts used to&nbsp;assess the total execution time of your application. We will learn how to write benchmarks and how to accurately time your programs.</p>
<p>The list of topics we will cover in this chapter is as follows:</p>
<ul>
<li>General principles of high performance&nbsp;programming</li>
<li>Writing tests and benchmarks</li>
<li>The Unix <kbd>time</kbd> command</li>
<li>The Python <kbd>timeit</kbd> module</li>
<li>Testing and benchmarking with <kbd>pytest</kbd></li>
<li>Profiling your application</li>
<li>The <kbd>cProfile</kbd> standard tool</li>
<li>Interpreting profiling results with KCachegrind</li>
<li><kbd>line_profiler</kbd> and&nbsp;&nbsp;<kbd>memory_profiler</kbd>&nbsp;tools</li>
<li>Disassembling Python code through the <kbd>dis</kbd> module</li>
</ul>

<h2>Designing your application</h2>
<p>When designing a performance-intensive program, the very first step is to write your code without bothering with small optimizations:</p>
<blockquote>
<p>"Premature optimization is the root of all evil."</p>
<p>-&nbsp;<strong>Donald Knuth</strong></p>
</blockquote>
<p>In the early development stages, the design of the program can change quickly and may require large rewrites and reorganizations&nbsp;of the code base. By testing different prototypes without the burden of optimization, you are free to devote your time and energy to ensure that the program produces correct results and that the design is flexible. After all, who needs an application that runs fast but gives the wrong answer?</p>
<p>The mantras that you should remember when optimizing your code are as follows:</p>
<ul>
<li><strong>Make it run</strong>: We have to get the software in a working state, and ensure that it produces the correct results. This exploratory phase serves to better&nbsp;understand the application&nbsp;and to spot major design issues in the early stages.</li>
<li><strong>Make it right</strong>: We want to ensure that the design of the program is solid. Refactoring should be done before attempting any performance optimization. This really helps separate the application into independent and cohesive units that are easier to maintain.</li>
<li><strong>Make it fast</strong>: Once our program is working and is well structured, we can focus on performance optimization. We may also want to optimize memory usage if that constitutes an issue.</li>
</ul>
<p>In this section, we will write and profile a <em>particle simulator</em> test application. The <strong>simulator</strong> is a program that takes some particles and simulates their movement over&nbsp;time according to a set of laws that we impose. These particles can be abstract entities or correspond to physical objects, for example, billiard balls moving on a table, molecules in gas, stars moving through space, smoke particles, fluids in a chamber, and so on.</p>
<p>Computer simulations are useful in fields such as Physics, Chemistry, Astronomy, and many other disciplines. The applications used to simulate systems are particularly performance-intensive and scientists and engineers spend an inordinate amount of time optimizing these codes. In order to study realistic systems, it is often necessary to simulate a very high number of bodies and every small increase in performance counts.</p>
<p>In our first example, we will simulate a system containing particles that constantly rotate around a central point at various speeds, just like the hands of a clock.</p>
<p>The necessary information to run our simulation will be the starting positions of the particles, the speed, and the rotation direction. From these elements, we have to calculate the position of the particle in the next instant of time. An example system is shown in the following figure. The origin of the system is the <kbd>(0, 0)</kbd> point, the position is indicated by the <strong>x</strong>, <strong>y</strong> vector and the velocity is indicated by the <strong>vx</strong>, <strong>vy</strong> vector:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000011.png" class="lazyload" /></p>
</div>
<p>The basic feature of a circular motion is that the particles always move perpendicular to the direction connecting the particle and the center. To move the particle, we simply change the position by taking a series of very small steps (which correspond to advancing the system for a small interval of time) in the direction of motion, as shown in the following figure:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000024.png" class="lazyload" /></p>
</div>
<p>We will start by designing the application in an object-oriented way. According to our requirements, it is natural to have a generic <kbd>Particle</kbd> class that stores the particle positions,&nbsp;<kbd>x</kbd>&nbsp;and&nbsp;<kbd>y</kbd>, and their&nbsp;angular velocity, <kbd>ang_vel</kbd>:</p>
<pre><code class="lang-python">    class Particle: 
        def __init__(self, x, y, ang_vel): 
            self.x = x 
            self.y = y 
            self.ang_vel = ang_vel
</code></pre>
<p>Note that we accept positive and negative numbers for all the parameters (the sign of <kbd>ang_vel</kbd> will simply determine the direction of rotation).</p>
<p>Another class, called <kbd>ParticleSimulator</kbd>, will encapsulate the&nbsp;laws of motion and will be responsible for changing the positions of the particles over time. The <kbd>__init__</kbd> method will store a list of <kbd>Particle</kbd> instances and the <kbd>evolve</kbd> method will change the particle positions according to our laws.</p>
<p>We want the particles to rotate around the position corresponding to the&nbsp;<kbd>x=0</kbd> and <kbd>y=0</kbd>&nbsp;coordinates, at a constant speed. The direction of the particles will always be perpendicular to the direction from the center (refer to the first figure of this chapter). To find the direction of the movement along the <em>x</em> and <em>y</em> axes (corresponding to the Python&nbsp;<kbd>v_x</kbd> and <kbd>v_y</kbd>&nbsp;variables), it is sufficient to use these formulae:</p>
<pre><code class="lang-python">    v_x = -y / (x**2 + y**2)**0.5
    v_y = x / (x**2 + y**2)**0.5
</code></pre>
<p>If we let one of our particles move, after a certain time <em>t</em>, it will reach another position following a circular path. We can approximate a circular trajectory by dividing the time interval,&nbsp;<em>t</em>, into tiny time steps, <em>dt</em>, where the particle moves in a straight line tangentially to the circle. The final result is just an approximation of a circular motion. In order to avoid a strong divergence, such as the one illustrated in the following figure, it is necessary to take very small time steps:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000012.png" class="lazyload" /></p>
</div>
<p>In a more schematic way, we have to carry out the following steps to calculate the particle position at time <em>t</em>:</p>
<ol>
<li>Calculate the direction of motion ( <kbd>v_x</kbd> and <kbd>v_y</kbd>).</li>
<li>Calculate the displacement (<kbd>d_x</kbd> and <kbd>d_y</kbd>), which is the product of time step, angular velocity, and direction of motion.</li>
<li>Repeat steps 1 and 2 for enough times to cover the total time <em>t</em>.</li>
</ol>
<p>The following code shows the full <kbd>ParticleSimulator</kbd> implementation:</p>
<pre><code class="lang-python">    class ParticleSimulator: 

        def __init__(self, particles): 
            self.particles = particles 

        def evolve(self, dt): 
            timestep = 0.00001 
            nsteps = int(dt/timestep) 
     
            for i in range(nsteps):
                for p in self.particles:
                    # 1. calculate the direction 
                    norm = (p.x**2 + p.y**2)**0.5 
                    v_x = -p.y/norm 
                    v_y = p.x/norm 

                    # 2. calculate the displacement 
                    d_x = timestep * p.ang_vel * v_x 
                    d_y = timestep * p.ang_vel * v_y 

                    p.x += d_x 
                    p.y += d_y 
                    # 3. repeat for all the time steps
</code></pre>
<p>We can use the <kbd>matplotlib</kbd> library to visualize our particles. This library is not included in the Python standard library, and&nbsp;it can be easily installed using the <kbd>pip install matplotlib</kbd>&nbsp;command.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Alternatively, you can use the Anaconda Python distribution (<a href="https://store.continuum.io/cshop/anaconda/">https://store.continuum.io/cshop/anaconda/</a>) that includes <kbd>matplotlib</kbd> and most of the other third-party packages used in this book. Anaconda is free and is available for Linux, Windows, and Mac.</p>
</div>
</div>
<p>To make an interactive visualization, we will use the <kbd>matplotlib.pyplot.plot</kbd> function to&nbsp;display the&nbsp;particles as points&nbsp;and the <kbd>matplotlib.animation.FuncAnimation</kbd> class to animate the evolution of the&nbsp;particles over time.</p>
<p>The <kbd>visualize</kbd> function takes a particle <kbd>ParticleSimulator</kbd> instance as an argument and displays the trajectory in an animated plot. The steps necessary to display&nbsp;the particle trajectory using the <kbd>matplotlib</kbd> tools&nbsp;are as follows:</p>
<ul>
<li>Set up the axes and use the&nbsp;<kbd>plot</kbd> function to display the particles. <kbd>plot</kbd> takes a list of <em>x</em> and <em>y</em> coordinates.</li>
<li>Write an initialization function,&nbsp;<kbd>init</kbd>, and a function, <kbd>animate</kbd>, that updates the <em>x</em> and&nbsp;<em>y&nbsp;</em>coordinates using the <kbd>line.set_data</kbd> method.</li>
<li>Create a <kbd>FuncAnimation</kbd> instance by passing the <kbd>init</kbd> and <kbd>animate</kbd> functions plus the&nbsp;<kbd>interval</kbd>&nbsp;parameters, which specify the update interval, and <kbd>blit</kbd>, which improves the update rate of the image.</li>
<li>Run the animation with <kbd>plt.show()</kbd>:</li>
</ul>
<pre><code class="lang-python">    from matplotlib import pyplot as plt 
    from matplotlib import animation 

    def visualize(simulator): 

        X = [p.x for p in simulator.particles] 
        Y = [p.y for p in simulator.particles] 

        fig = plt.figure() 
        ax = plt.subplot(111, aspect='equal') 
        line, = ax.plot(X, Y, 'ro') 
     
        # Axis limits 
        plt.xlim(-1, 1) 
        plt.ylim(-1, 1) 

        # It will be run when the animation starts 
        def init(): 
            line.set_data([], []) 
            return line, # The comma is important!

        def animate(i): 
            # We let the particle evolve for 0.01 time units 
            simulator.evolve(0.01) 
            X = [p.x for p in simulator.particles] 
            Y = [p.y for p in simulator.particles] 

            line.set_data(X, Y) 
            return line, 

        # Call the animate function each 10 ms 
        anim = animation.FuncAnimation(fig,
                                       animate,
                                       init_func=init,
                                       blit=True,
                                       interval=10) 
        plt.show()
</code></pre>
<p>To test things out, we define a small function,&nbsp;<kbd>test_visualize</kbd>, that animates a system of three particles rotating in different directions. Note that the third particle completes a round three times faster than the others:</p>
<pre><code class="lang-python">    def test_visualize(): 
        particles = [Particle(0.3, 0.5, 1), 
                     Particle(0.0, -0.5, -1), 
                     Particle(-0.1, -0.4, 3)] 

        simulator = ParticleSimulator(particles) 
        visualize(simulator) 

    if __name__ == '__main__': 
        test_visualize()
</code></pre>
<p>The <kbd>test_visualize</kbd> function is helpful to graphically understand the system time evolution. In the following section, we will write more test functions to properly verify program correctness and measure performance.</p>

<h2>Writing tests and benchmarks</h2>
<p>Now that we have a working simulator, we can start measuring our performance and tune-up our code so that the&nbsp;simulator can handle as many particles as possible. As a first step, we will write a test and a benchmark.</p>
<p>We need a test that checks whether the results produced by the simulation are correct or not. Optimizing a program commonly requires employing multiple strategies; as we rewrite our code multiple times, bugs may easily be introduced. A&nbsp;solid test suite ensures that the implementation is correct at every iteration so that we are free to go wild and try different things with the confidence that, if the test suite passes, the code will still work as expected.</p>
<p>Our test will take three particles, simulate them&nbsp;for&nbsp;0.1 time units, and compare the results with those from a reference implementation. A good way to organize your tests is using a separate function for each different aspect (or unit) of your application. Since our current functionality is included in the&nbsp;<kbd>evolve</kbd> method, our function will be named <kbd>test_evolve</kbd>. The following code shows the <kbd>test_evolve</kbd> implementation. Note that, in this case, we compare floating point numbers up to a certain&nbsp;precision through the <kbd>fequal</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    def test_evolve(): 
        particles = [Particle( 0.3,  0.5, +1), 
                     Particle( 0.0, -0.5, -1), 
                     Particle(-0.1, -0.4, +3)] 

        simulator = ParticleSimulator(particles) 

        simulator.evolve(0.1) 

        p0, p1, p2 = particles 

        def fequal(a, b, eps=1e-5): 
            return abs(a - b) &lt; eps 

        assert fequal(p0.x, 0.210269) 
        assert fequal(p0.y, 0.543863) 

        assert fequal(p1.x, -0.099334) 
        assert fequal(p1.y, -0.490034) 

        assert fequal(p2.x,  0.191358) 
        assert fequal(p2.y, -0.365227) 

    if __name__ == '__main__': 
        test_evolve()
</code></pre>
<p>A test&nbsp;ensures the correctness of our functionality but gives little information about&nbsp;its running time. &nbsp;A benchmark is a simple and representative use case that can be run to assess the running time of an application. Benchmarks are very useful to keep score of&nbsp;how fast our program&nbsp;is with each new version that we implement.</p>
<p>We can write a representative benchmark by instantiating a thousand&nbsp;<kbd>Particle</kbd> objects with random coordinates and angular velocity, and feed them to a <kbd>ParticleSimulator</kbd> class. We then let the system evolve for&nbsp;0.1 time units:</p>
<pre><code class="lang-python">    from random import uniform 

    def benchmark(): 
        particles = [Particle(uniform(-1.0, 1.0), 
                              uniform(-1.0, 1.0), 
                              uniform(-1.0, 1.0)) 
                      for i in range(1000)] 

        simulator = ParticleSimulator(particles) 
        simulator.evolve(0.1) 

    if __name__ == '__main__': 
        benchmark()
</code></pre>

<h3>Timing your benchmark</h3>
<p>A very simple way to time a&nbsp;benchmark is through the Unix <kbd>time</kbd> command. Using the&nbsp;<kbd>time</kbd>&nbsp;command, as follows, you can easily measure the execution time of an arbitrary process:</p>
<pre><code class="lang-python">    $ time python simul.py
real    0m1.051s
user    0m1.022s
sys     0m0.028s
</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The <kbd>time</kbd> command is not available for Windows. To install Unix tools, such as <kbd>time</kbd>, on Windows you can use the&nbsp;<kbd>cygwin</kbd> shell, downloadable from the official website (<a href="http://www.cygwin.com/">http://www.cygwin.com/</a>). Alternatively, you can use similar PowerShell commands, such as <kbd>Measure-Command</kbd> (<a href="https://msdn.microsoft.com/en-us/powershell/reference/5.1/microsoft.powershell.utility/measure-command">https://msdn.microsoft.com/en-us/powershell/reference/5.1/microsoft.powershell.utility/measure-command</a>), to measure execution time.</p>
</div>
</div>
<p>By default, <kbd>time</kbd>&nbsp;displays three metrics:</p>
<ul>
<li><kbd>real</kbd>: The actual time spent running the process from start to finish, as if it was measured by a human with a stopwatch</li>
<li><kbd>user</kbd>: The cumulative time spent by all the CPUs during the computation</li>
<li><kbd>sys</kbd>: The cumulative time spent by all the CPUs during system-related tasks, such as memory allocation</li>
</ul>
<p>Note that sometimes <kbd>user</kbd> + <kbd>sys</kbd> might be greater than <kbd>real</kbd>, as multiple processors may work in parallel.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p><kbd>time</kbd> also offers richer&nbsp;formatting options. For an overview, you can explore its manual (using the <kbd>man time</kbd> command). If you want a summary of all the metrics available, you can use the <kbd>-v</kbd> option.</p>
</div>
</div>
<p>The Unix <kbd>time</kbd> command is one of the simplest and more direct ways to benchmark a program. For an&nbsp;accurate measurement, the benchmark should be designed to have a long enough&nbsp;execution time (in the order of seconds) so that the&nbsp;setup and tear-down of the process is small compared to the execution time of the application. The <kbd>user</kbd> metric is suitable as a monitor for the CPU performance, while the&nbsp;<kbd>real</kbd> metric also includes the time spent in other processes while&nbsp;waiting for I/O operations.</p>
<p>Another convenient way to time Python scripts is the <kbd>timeit</kbd> module. This module runs a snippet of code in a loop for <em>n</em> times and measures&nbsp;the total execution times. Then, it repeats the same operation <em>r</em> times (by default, the value of <em>r</em> is <kbd>3</kbd>) and records the time of the best run. Due to&nbsp;this timing scheme, <kbd>timeit</kbd>&nbsp;is an appropriate tool to accurately time small statements in isolation.</p>
<p>The <kbd>timeit</kbd> module can be used as a Python package, from the command line or from <em>IPython</em>.</p>
<p>IPython is a Python shell design that improves the interactivity of the Python interpreter. It boosts tab completion and many utilities to time, profile, and debug your code. We will use this shell to try out snippets throughout the book. The IPython shell accepts <strong>magic commands</strong>--statements that start with a <kbd>%</kbd> symbol--that enhance the shell with special behaviors. Commands that start with <kbd>%%</kbd> are called <strong>cell magics</strong>, which&nbsp;can be applied on multi-line snippets (termed as&nbsp;<strong>cells</strong>).</p>
<p>IPython is available on most Linux distributions through <kbd>pip</kbd>&nbsp;and is included in Anaconda.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>You can use IPython as a regular Python shell (<kbd>ipython</kbd>), but it is also available in a Qt-based version (<kbd>ipython qtconsole</kbd>) and as a powerful browser-based interface (<kbd>jupyter notebook</kbd>).</p>
</div>
</div>
<p>In IPython and command-line interfaces, it is possible to specify the number of loops or repetitions with the&nbsp;<kbd>-n</kbd> and <kbd>-r</kbd> options. If not specified, they will be automatically inferred by <kbd>timeit</kbd>. When invoking <kbd>timeit</kbd> from the command line, you can also pass&nbsp;some setup code, through the <kbd>-s</kbd> option, which&nbsp;will execute before the benchmark. In the following snippet, the IPython command line and Python module version of <kbd>timeit</kbd> are demonstrated:&nbsp;</p>

<blockquote>
<p><strong># IPython Interface </strong><br /><strong>$ ipython </strong><br /><strong>In [1]: from simul import benchmark </strong><br /><strong>In [2]: %timeit benchmark() </strong><br /><strong>1 loops, best of 3: 782 ms per loop </strong><br /><br /><strong># Command Line Interface </strong><br /><strong>$ python -m timeit -s 'from simul import benchmark' 'benchmark()'</strong><br /><strong>10 loops, best of 3: 826 msec per loop </strong><br /><br /><strong># Python Interface </strong><br /><strong># put this function into the simul.py script </strong><br /><br /><strong>import timeit</strong><br /><strong>result = timeit.timeit('benchmark()',</strong><br /><strong> setup='from __main__ import benchmark',</strong><br /><strong> number=10)</strong><br /><br /><strong># result is the time (in seconds) to run the whole loop </strong><br /><strong>result = timeit.repeat('benchmark()',</strong><br /><strong> setup='from __main__ import benchmark',</strong><br /><strong> number=10,</strong><br /><strong> repeat=3) </strong><br /><strong># result is a list containing the time of each repetition (repeat=3 in this case)</strong></p>
<p>Note that while the command line and IPython interfaces automatically infer a reasonable number of loops <kbd>n</kbd>, the Python interface requires you to explicitly specify a value through the&nbsp;<kbd>number</kbd> argument.</p>
</blockquote>

<h2>Better tests and benchmarks with pytest-benchmark</h2>
<p>The Unix <kbd>time</kbd> command is a versatile tool that can be used to assess the running time of small programs on a variety of platforms. For larger Python applications and libraries, a more comprehensive solution that deals with both testing and benchmarking is <kbd>pytest</kbd>, in combination with its <kbd>pytest-benchmark</kbd>&nbsp;plugin.</p>
<p>In this section, we will write a simple benchmark for our application using the <kbd>pytest</kbd> testing framework. For the interested reader, the <kbd>pytest</kbd> documentation, which can be found at&nbsp;<a href="http://doc.pytest.org/en/latest/">http://doc.pytest.org/en/latest/,</a> is the best resource to learn more about the framework and its uses.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>You can install <kbd>pytest</kbd> from the console using the <kbd>pip install pytest</kbd>&nbsp;command. The benchmarking plugin can be installed, similarly, by issuing the <kbd>pip install pytest-benchmark</kbd>&nbsp;command.</p>
</div>
</div>
<p>A testing framework is a set of tools that simplifies writing, executing, and debugging tests and provides rich reports and summaries of the test results. When using the <kbd>pytest</kbd> framework, it is recommended to place tests separately from the application code. In the following example, we create the <kbd>test_simul.py</kbd>&nbsp;file, which contains the <kbd>test_evolve</kbd> function:</p>
<pre><code class="lang-python">    from simul import Particle, ParticleSimulator

    def test_evolve():
        particles = [Particle( 0.3,  0.5, +1),
                     Particle( 0.0, -0.5, -1),
                     Particle(-0.1, -0.4, +3)]

        simulator = ParticleSimulator(particles)

        simulator.evolve(0.1)
    
        p0, p1, p2 = particles

        def fequal(a, b, eps=1e-5):
            return abs(a - b) &lt; eps

        assert fequal(p0.x, 0.210269)
        assert fequal(p0.y, 0.543863)

        assert fequal(p1.x, -0.099334)
        assert fequal(p1.y, -0.490034)

        assert fequal(p2.x,  0.191358)
        assert fequal(p2.y, -0.365227)
</code></pre>
<p>The &nbsp;<kbd>pytest</kbd> executable can be used from the command line to discover and run tests contained in Python modules. To execute a specific test, we can use the <kbd>pytest path/to/module.py::function_name</kbd>&nbsp;syntax. To execute <kbd>test_evolve</kbd>,&nbsp; we can type the following command in a console to obtain simple but informative output:</p>
<pre><code class="lang-python">$ pytest test_simul.py::test_evolve

platform linux -- Python 3.5.2, pytest-3.0.5, py-1.4.32, pluggy-0.4.0
rootdir: /home/gabriele/workspace/hiperf/chapter1, inifile: plugins:
collected 2 items 

test_simul.py .

=========================== 1 passed in 0.43 seconds ===========================
</code></pre>
<p>Once we have a test in place, it is possible for you to execute&nbsp;your test as a benchmark using the <kbd>pytest-benchmark</kbd> plugin. If we change our <kbd>test</kbd> function so that it accepts an argument named <kbd>benchmark</kbd>, the <kbd>pytest</kbd> framework will automatically pass the <kbd>benchmark</kbd> resource as an argument (in <kbd>pytest</kbd> terminology, these resources are called <em>fixtures</em>). The benchmark resource can be called by passing the function that we intend to benchmark as the first argument, followed by the additional arguments. In the following snippet, we illustrate the edits necessary to benchmark the <kbd>ParticleSimulator.evolve</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    from simul import Particle, ParticleSimulator

    def test_evolve(benchmark):
        # ... previous code
        benchmark(simulator.evolve, 0.1)
</code></pre>
<p>To run the benchmark, it is sufficient to rerun the <kbd>pytest test_simul.py::test_evolve</kbd>&nbsp;command. The resulting output will contain detailed timing information regarding the <kbd>test_evolve</kbd> function, as shown:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000001.png" class="lazyload" /></p>
<p>For each test collected,&nbsp;<kbd>pytest-benchmark</kbd>&nbsp;will execute the benchmark function several times and provide a statistic summary of its running time. The output shown earlier&nbsp;is very interesting as it shows how running times vary between runs.<br />In this example, the benchmark in <kbd>test_evolve</kbd> was run <kbd>34</kbd> times (column <kbd>Rounds</kbd>), its timings ranged between <kbd>29</kbd> and <kbd>41</kbd> ms (<kbd>Min</kbd> and <kbd>Max</kbd>), and the <kbd>Average</kbd> and <kbd>Median</kbd> times were fairly similar at about <kbd>30</kbd> ms, which is actually very close to the best timing obtained. This example demonstrates how there can be substantial performance variability between runs, and that when taking timings with one-shot tools such as <kbd>time</kbd>, it is a good idea to run the program multiple times and record a representative value, such as the minimum or the median.</p>
<p><kbd>pytest-benchmark</kbd> has many more features and options that can be used to take accurate timings and analyze the results. For more information, consult the documentation at <a href="http://pytest-benchmark.readthedocs.io/en/stable/usage.html">http://pytest-benchmark.readthedocs.io/en/stable/usage.html</a>.</p>

<h2>Finding bottlenecks with cProfile</h2>
<p>After assessing the correctness and timing the execution time of the program, we are ready to identify the parts of the code that need to be tuned for performance. Those parts are typically quite small compared to the size of the program.</p>
<p>Two&nbsp;profiling modules are available through the Python standard library:</p>
<ul>
<li><strong>The</strong> <kbd>profile</kbd> <strong>module</strong>: This module is written in pure Python and adds a significant overhead to the program execution. Its presence in the standard library is because of its vast platform support&nbsp;and because it is easier to extend.</li>
<li><strong>The</strong> <kbd>cProfile</kbd> <strong>module</strong>: This is the main profiling module, with an interface equivalent to <kbd>profile</kbd>. It is written in C, has a small overhead, and is suitable as a general purpose profiler.</li>
</ul>
<p>The <kbd>cProfile</kbd> module can be used in three&nbsp;different ways:</p>
<ul>
<li>From the command line</li>
<li>As a Python module</li>
<li>With&nbsp;IPython</li>
</ul>
<p><kbd>cProfile</kbd>&nbsp;does not require any change in the source code and can be executed directly on an existing Python script or function.&nbsp;You can use <kbd>cProfile</kbd> from the command line in this way:</p>
<pre><code class="lang-python">$ python -m cProfile simul.py
</code></pre>
<p>This will print a long output containing several profiling metrics of&nbsp;all of the functions called in the application. You can use the <kbd>-s</kbd> option to sort the output by a specific metric. In the following snippet ,the output is sorted by the <kbd>tottime</kbd>&nbsp;metric, which will be described here:</p>
<pre><code class="lang-python">$ python -m cProfile <strong>-s tottime</strong> simul.py
</code></pre>
<p>The data produced by <kbd>cProfile</kbd> can be saved in an&nbsp;output file by passing the <kbd>-o</kbd> option. The format that <kbd>cProfile</kbd> uses is readable by the <kbd>stats</kbd> module and other tools. The usage of the&nbsp; <kbd>-o</kbd> option is as follows:</p>
<pre><code class="lang-python">$ python -m cProfile <strong>-o prof.out</strong> simul.py
</code></pre>
<p>The usage of cProfile as a Python module requires invoking the <kbd>cProfile.run</kbd>&nbsp;function in the following way:</p>
<pre><code class="lang-python">    from simul import benchmark
    import cProfile

    cProfile.run("benchmark()")
</code></pre>
<p>You can also wrap a section of code between method calls of a <kbd>cProfile.Profile</kbd> object, as shown:</p>
<pre><code class="lang-python">    from simul import benchmark
    import cProfile

    pr = cProfile.Profile()
    pr.enable()
    benchmark()
    pr.disable()
    pr.print_stats()
</code></pre>
<p><kbd>cProfile</kbd>&nbsp;can also be used&nbsp;interactively with IPython. The <kbd>%prun</kbd> magic command lets you profile an individual function call, as&nbsp;illustrated:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000028.png" class="lazyload" /></p>
<p>The <kbd>cProfile</kbd> output is divided into five columns:</p>
<ul>
<li><kbd>ncalls</kbd>: The number of times the function was called.</li>
<li><kbd>tottime</kbd>: The total time spent in the function without taking into account the calls to other functions.</li>
<li><kbd>cumtime</kbd>: The time in the function including other function calls.</li>
<li><kbd>percall</kbd>: The time spent for a single call of the function--it can be obtained by dividing the total or cumulative time by the number of calls.</li>
<li><kbd>filename:lineno</kbd>: The filename and corresponding line numbers. This information is not available&nbsp;when calling C extensions modules.</li>
</ul>
<p>The most important metric is <kbd>tottime</kbd>, the actual time spent in the function body excluding subcalls, which tell us exactly where the bottleneck is.</p>
<p>Unsurprisingly, the largest portion of time is spent in the <kbd>evolve</kbd> function. We can imagine that the loop is the section of the code that needs&nbsp;performance tuning.<br /><kbd>cProfile</kbd> only provides information at the function level and does not tell us which specific statements&nbsp;are responsible for the bottleneck. Fortunately, as we will see in the next section, the &nbsp;<kbd>line_profiler</kbd>&nbsp;tool is capable of providing line-by-line information of the time spent in the function.</p>
<p>Analyzing the&nbsp;<kbd>cProfile</kbd>&nbsp;text output can be daunting for big programs with a lot of calls and subcalls. Some visual&nbsp;tools aid the task by improving navigation with an interactive, graphical interface.</p>
<p>KCachegrind is a <strong>Graphical User Interface (GUI)</strong>&nbsp;useful to analyze the profiling output emitted by <kbd>cProfile</kbd>.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>KCachegrind is available in the Ubuntu 16.04 official repositories. The Qt port, QCacheGrind, can be downloaded for Windows from <a href="http://sourceforge.net/projects/qcachegrindwin/">http://sourceforge.net/projects/qcachegrindwin/</a>.&nbsp;Mac users can compile QCacheGrind using Mac Ports (<a href="http://www.macports.org/">http://www.macports.org/</a>) by following the instructions present in the blog post at&nbsp;<a href="http://blogs.perl.org/users/rurban/2013/04/install-kachegrind-on-macosx-with-ports.html">http://blogs.perl.org/users/rurban/2013/04/install-kachegrind-on-macosx-with-ports.html</a>.</p>
</div>
</div>
<p>KCachegrind can't directly read the output files produced by <kbd>cProfile</kbd>. Luckily, the <kbd>pyprof2calltree</kbd> third-party Python module is able to convert the <kbd>cProfile</kbd> output file into a format readable by KCachegrind.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>You can install <kbd>pyprof2calltree</kbd>&nbsp;from the Python Package Index&nbsp;using the command <kbd>pip install pyprof2calltree</kbd>.</p>
</div>
</div>
<p>To best show the KCachegrind features, we will use another example with a more diversified structure. We define a <kbd>recursive</kbd> function,&nbsp;<kbd>factorial</kbd>, and two other functions that use <kbd>factorial</kbd>, named&nbsp;<kbd>taylor_exp</kbd> and <kbd>taylor_sin</kbd>. They represent the polynomial coefficients of the Taylor approximations of <kbd>exp(x)</kbd> and <kbd>sin(x)</kbd>:</p>
<pre><code class="lang-python">    def factorial(n): 
        if n == 0: 
            return 1.0 
        else: 
            return n * factorial(n-1) 

    def taylor_exp(n): 
        return [1.0/factorial(i) for i in range(n)] 

    def taylor_sin(n): 
        res = [] 
        for i in range(n): 
            if i % 2 == 1: 
               res.append((-1)**((i-1)/2)/float(factorial(i))) 
            else: 
               res.append(0.0) 
        return res 

    def benchmark(): 
        taylor_exp(500) 
        taylor_sin(500) 

    if __name__ == '__main__': 
        benchmark()
</code></pre>
<p>To access profile information, we first need to generate the <kbd>cProfile</kbd> output file:</p>
<pre><code class="lang-python">$ python -m cProfile -o prof.out taylor.py
</code></pre>
<p>Then, we can convert the output file with <kbd>pyprof2calltree</kbd> and launch KCachegrind:</p>
<pre><code class="lang-python">$ pyprof2calltree -i prof.out -o prof.calltree
$ kcachegrind prof.calltree # or qcachegrind prof.calltree
</code></pre>
<p>The output is shown in the following screenshot:</p>
<p></p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000025.png" class="lazyload" /></p>
<p>The preceding screenshot shows&nbsp;the KCachegrind user interface. On the left, we have an output fairly similar to <kbd>cProfile</kbd>. The actual column names are slightly different: Incl. translates to <kbd>cProfile</kbd> module's <kbd>cumtime</kbd>&nbsp;and&nbsp;Self translates to <kbd>tottime</kbd>. The values are given in percentages by clicking on the Relative button on the menu bar.&nbsp;By clicking on the column headers, you can sort them by the corresponding property.</p>
<p>On the top right, a click on the Callee Map tab will display a diagram of the function costs. In the diagram, the time percentage spent by the function is proportional to the area of the rectangle. Rectangles can contain sub-rectangles that represent subcalls to other functions. In this case, we can easily see that there are two rectangles for the <kbd>factorial</kbd> function. The one on the left corresponds to the calls made by <kbd>taylor_exp</kbd> and the one on the right to the calls made by <kbd>taylor_sin</kbd>.</p>
<p>On the bottom right, you can display another diagram, the <em>call graph</em>,&nbsp;by clicking on the Call Graph tab. A call graph is a graphical representation of the calling relationship between the functions; each square represents a function and the arrows imply a calling relationship. For example, <kbd>taylor_exp</kbd> calls <kbd>factorial</kbd>&nbsp;500 times, and <kbd>taylor_sin</kbd> calls&nbsp;factorial 250 times. KCachegrind also detects recursive calls: <kbd>factorial</kbd> calls itself 187250 times.</p>
<p>You can navigate to the Call Graph or the Caller Map tab by double-clicking on the rectangles; the interface will update accordingly, showing that the timing properties are relative to the selected function. For example, double-clicking on <kbd>taylor_exp</kbd> will cause the graph to change, showing only the contribution of&nbsp;<kbd>taylor_exp</kbd>&nbsp;to the total cost.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p><strong>Gprof2Dot</strong> (<a href="https://github.com/jrfonseca/gprof2dot">https://github.com/jrfonseca/gprof2dot</a>) is another popular tool used to produce call graphs. Starting from output files produced by one of the supported profilers, it will generate a <kbd>.dot</kbd> diagram representing the call graph.</p>
</div>
</div>

<h2>Profile line by line with line_profiler</h2>
<p>Now that we know which function we have to optimize, we can use the <kbd>line_profiler</kbd> module&nbsp;that provides information on how time is spent in a line-by-line fashion. This is very useful in situations where it's difficult to determine which statements are costly. The <kbd>line_profiler</kbd> module is a third-party module that is available on the Python Package Index and can be installed by following the instructions at&nbsp;<a href="https://github.com/rkern/line_profiler">https://github.com/rkern/line_profiler</a>.</p>
<p>In order to use <kbd>line_profiler</kbd>,&nbsp;we need to apply a <kbd>@profile</kbd> decorator to the functions we intend to monitor. Note that you don't have to import the <kbd>profile</kbd> function from another module as it gets injected in the global namespace when running the <kbd>kernprof.py</kbd>&nbsp;profiling script. To produce profiling output for our program, we need to add the <kbd>@profile</kbd> decorator to the <kbd>evolve</kbd> function:</p>
<pre><code class="lang-python">    @profile 
    def evolve(self, dt): 
        # code
</code></pre>
<p>The <kbd>kernprof.py</kbd> script will produce an output file and will print the result of the profiling on the standard output. We should run the script with&nbsp;two options:</p>
<ul>
<li><kbd>-l</kbd> to use the <kbd>line_profiler</kbd> function</li>
<li><kbd>-v</kbd> to immediately print the results on screen</li>
</ul>
<p>The usage of <kbd>kernprof.py</kbd> is illustrated in the following line of code:</p>
<pre><code class="lang-python">$ kernprof.py -l -v simul.py
</code></pre>
<p>It is also possible to run the profiler in an IPython shell for interactive editing. You should first load the <kbd>line_profiler</kbd> extension that will provide the <kbd>lprun</kbd>&nbsp;magic command. Using that command, you can avoid adding the <kbd>@profile</kbd> decorator:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000013.png" class="lazyload" /></p>
<p>The output is quite intuitive and is divided into six columns:</p>
<ul>
<li><kbd>Line #</kbd>: The number of the line that was run</li>
<li><kbd>Hits</kbd>: The number of times that line was run</li>
<li><kbd>Time</kbd>: The execution time of the line in microseconds (<kbd>Time</kbd>)</li>
<li><kbd>Per Hit</kbd>: Time/hits</li>
<li><kbd>% Time</kbd>: Fraction of the total time spent executing that line</li>
<li><kbd>Line Contents</kbd>: The content of the line</li>
</ul>
<p>By looking at the percentage column, we can get a pretty good idea of where the time is spent. In this case, there are a few statements in the <kbd>for</kbd> loop body with a cost of around 10-20 percent each.</p>

<h2>Optimizing our code</h2>
<p>Now that we have identified where exactly our application is spending most of its time, we can make&nbsp;some changes and assess the change in performance.</p>
<p>There are different ways to tune up our pure Python code. The way that produces the most remarkable results is to improve the <em>algorithms</em> used. In this case, instead of calculating the velocity and adding small steps, it will be more efficient (and correct as it is not an approximation) to express the equations of motion in terms of radius,&nbsp;<kbd>r</kbd>, and angle,&nbsp;<kbd>alpha</kbd>, (instead of <kbd>x</kbd> and <kbd>y</kbd>), and then calculate the points on a circle using the following equation:</p>
<pre><code class="lang-python">    x = r * cos(alpha) 
    y = r * sin(alpha)
</code></pre>
<p>Another way lies in minimizing the number of instructions. For example, we can precalculate the <kbd>timestep * p.ang_vel</kbd> factor that doesn't change with time. We can exchange the loop order (first we iterate on particles, then we iterate on time steps) and put the calculation of the factor outside the loop on the particles.</p>
<p>The line-by-line profiling also showed that even simple assignment operations can take a considerable amount of time. For example, the following statement takes more than 10&nbsp;percent of the total time:</p>
<pre><code class="lang-python">    v_x = (-p.y)/norm
</code></pre>
<p>We can improve the performance of the loop by reducing&nbsp;the number of assignment operations performed. To do that, we can avoid intermediate variables by rewriting the expression into a single, slightly more complex statement (note that the right-hand side gets evaluated completely before being assigned to the variables):</p>
<pre><code class="lang-python">    p.x, p.y = p.x - t_x_ang*p.y/norm, p.y + t_x_ang * p.x/norm
</code></pre>
<p>This leads to the following code:</p>
<pre><code class="lang-python">        def evolve_fast(self, dt): 
            timestep = 0.00001 
            nsteps = int(dt/timestep) 

            # Loop order is changed 
            for p in self.particles: 
                t_x_ang = timestep * p.ang_vel 
                for i in range(nsteps): 
                    norm = (p.x**2 + p.y**2)**0.5 
                    p.x, p.y = (p.x - t_x_ang * p.y/norm,
                                p.y + t_x_ang * p.x/norm)
</code></pre>
<p>After applying the changes, we should verify that the result is still the same by running our test. We can then compare the execution times using our benchmark:</p>
<pre><code class="lang-python">$ time python simul.py # Performance Tuned
real    0m0.756s
user    0m0.714s
sys    0m0.036s

$ time python simul.py # Original
real    0m0.863s
user    0m0.831s
sys    0m0.028s
</code></pre>
<p>As you can see, we obtained only a modest increment in speed by making a pure Python micro-optimization.</p>

<h2>The dis module</h2>
<p>Sometimes it's not easy to estimate how many operations a Python statement will take. In this section, we will dig into the&nbsp;Python internals to estimate the performance of individual statements. In the CPython interpreter, Python code is first converted to an intermediate representation, the <strong>bytecode</strong>, and then executed by the Python interpreter.</p>
<p>To inspect how the code is converted to bytecode, we can use the <kbd>dis</kbd> Python module (<kbd>dis</kbd> stands for disassemble). Its usage is really simple; all that is needed is to call the <kbd>dis.dis</kbd> function on the <kbd>ParticleSimulator.evolve</kbd> method:</p>
<pre><code class="lang-python">    import dis 
    from simul import ParticleSimulator 
    dis.dis(ParticleSimulator.evolve)
</code></pre>
<p>This will print, for each line in the function, a list of bytecode instructions. For example, the <kbd>v_x = (-p.y)/norm</kbd> statement is expanded in the following set of instructions:</p>
<pre><code class="lang-python">    29           85 LOAD_FAST                5 (p) 
                 88 LOAD_ATTR                4 (y) 
                 91 UNARY_NEGATIVE        
                 92 LOAD_FAST                6 (norm) 
                 95 BINARY_TRUE_DIVIDE    
                 96 STORE_FAST               7 (v_x)
</code></pre>
<p><kbd>LOAD_FAST</kbd> loads a reference of the <kbd>p</kbd> variable onto the stack and&nbsp;<kbd>LOAD_ATTR</kbd> loads the <kbd>y</kbd> attribute of the item present on top of the stack. The other instructions, <kbd>UNARY_NEGATIVE</kbd> and <kbd>BINARY_TRUE_DIVIDE</kbd>,&nbsp;simply do arithmetic operations on top-of-stack items. Finally, the result is stored in <kbd>v_x</kbd> (<kbd>STORE_FAST</kbd>).</p>
<p>By analyzing the <kbd>dis</kbd> output, we can see that the first version of the loop produces <kbd>51</kbd> bytecode instructions while the second gets converted into&nbsp;<kbd>35</kbd> instructions.</p>
<p>The <kbd>dis</kbd> module helps discover how the statements get converted and serves mainly as an exploration and learning tool of the Python bytecode representation.</p>
<p>To improve our performance even further, we can keep trying to figure out other approaches to reduce the amount of instructions. It's clear, however, that this approach is ultimately limited by the speed of the Python interpreter and it is probably not the right tool for the job. In the following chapters, we will see how to speed up interpreter-limited&nbsp;calculations by executing fast specialized versions written in a lower level language (such as C or Fortran).</p>

<h2>Profiling memory usage with memory_profiler</h2>
<p>In some cases, high memory usage constitutes an issue. For example, if we want to handle a huge number of particles, we will incur a memory overhead due to the creation of many <kbd>Particle</kbd> instances.</p>
<p>The <kbd>memory_profiler</kbd> module summarizes, in a way similar to <kbd>line_profiler</kbd>, the memory usage of the process.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The <kbd>memory_profiler</kbd> package is also available on the Python Package Index. You should also install the <kbd>psutil</kbd> module (<a href="https://github.com/giampaolo/psutil">https://github.com/giampaolo/psutil</a>) as an optional dependency that&nbsp;will make <kbd>memory_profiler</kbd>&nbsp;considerably faster.</p>
</div>
</div>
<p>Just like <kbd>line_profiler</kbd>, <kbd>memory_profiler</kbd> also requires the instrumentation of the source code by placing&nbsp;a <kbd>@profile</kbd> decorator on the function we intend to monitor. In our case, we want to analyze the <kbd>benchmark</kbd>&nbsp;function.</p>
<p>We can slightly change <kbd>benchmark</kbd> to instantiate a considerable amount (<kbd>100000</kbd>) of <kbd>Particle</kbd> instances and decrease the simulation time:</p>
<pre><code class="lang-python">    def benchmark_memory(): 
        particles = [Particle(uniform(-1.0, 1.0), 
                              uniform(-1.0, 1.0), 
                              uniform(-1.0, 1.0)) 
                      for i in range(100000)] 

        simulator = ParticleSimulator(particles) 
        simulator.evolve(0.001)
</code></pre>
<p>We can use <kbd>memory_profiler</kbd> from an IPython shell through the <kbd>%mprun</kbd>&nbsp;magic command as shown in the following screenshot:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000002.png" class="lazyload" /></p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>It is possible to run&nbsp;<kbd>memory_profiler</kbd> from the shell using the <kbd>mprof run</kbd> command after adding the <kbd>@profile</kbd> decorator.</p>
</div>
</div>
<p>From the <kbd>Increment</kbd> column, we can see that 100,000 <kbd>Particle</kbd> objects take <kbd>23.7 MiB</kbd> of memory.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>1 MiB (mebibyte) is equivalent to&nbsp; 1,048,576 bytes. It is different from 1 MB (<em>megabyte</em>), which is equivalent to 1,000,000 bytes.</p>
</div>
</div>
<p>We can use <kbd>__slots__</kbd> on the <kbd>Particle</kbd> class to reduce its memory footprint. This feature saves some memory by avoiding storing the variables of the instance in an internal dictionary. This strategy, however,&nbsp;has a drawback--it prevents the addition of attributes other than the ones specified in <kbd>__slots__</kbd> :</p>
<pre><code class="lang-python">    class Particle:
        __slots__ = ('x', 'y', 'ang_vel') 

        def __init__(self, x, y, ang_vel): 
            self.x = x 
            self.y = y 
            self.ang_vel = ang_vel
</code></pre>
<p>We can now rerun our benchmark to assess the change in memory consumption, the result is displayed in the following screenshot:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000032.png" class="lazyload" /></p>
<p>By rewriting the <kbd>Particle</kbd> class using <kbd>__slots__</kbd>, we can save about <kbd>10 MiB</kbd> of memory.</p>

<h2>Summary</h2>
<p>In this chapter, we introduced the basic principles of optimization and applied those principles to a&nbsp;test application. When optimizing, the first thing to do is test and identify the bottlenecks in the application. We saw how to write and time a benchmark using the <kbd>time</kbd> Unix command, the Python <kbd>timeit</kbd> module, and the full-fledged <kbd>pytest-benchmark</kbd> package. We learned how to profile our application using <kbd>cProfile</kbd>, <kbd>line_profiler</kbd>, and <kbd>memory_profiler</kbd>, and how to analyze and navigate the profiling data graphically with KCachegrind.</p>
<p>In the next chapter, we will explore how to improve performance using algorithms and data structures available in the Python standard library. We will cover scaling, sample usage of several data structures, and learn techniques such as caching and memoization.</p>

</div>


<!--CHapter 2-->

<div class="chapter" data-chapter-number="2">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 2 </span></div>
<h1 class="chaptertitle">Pure Python Optimizations</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>



<p>As mentioned in the last&nbsp;chapter, one of the most effective ways of improving the performance of applications is through the use of better algorithms and data structures. The Python standard library provides a large variety of ready-to-use algorithms and data structures that can be directly incorporated in your applications. With the tools learned from this chapter, you will be able to use the right algorithm for the task and achieve massive speed gains.</p>
<p>Even though many algorithms have been around for quite a while, they are especially relevant in today's world as we continuously produce, consume, and analyze ever increasing amounts of data. Buying a larger server or microoptimizing can work for some time, but achieving better scaling through algorithmic improvement can solve the problem once and for all.</p>
<p>In this chapter, we will understand how to achieve better scaling using standard algorithms and data structures. More advanced use cases will also be covered by taking advantage of third-party libraries. We will also learn about tools to implement caching, a technique used to achieve faster response times by sacrificing some space on memory or on disk.</p>
<p>The list of topics to be covered in this chapter is as follows:</p>
<ul>
<li>Introduction to computational complexity</li>
<li>Lists and deques</li>
<li>Dictionaries</li>
<li>How to build an inverted index using a dictionary</li>
<li>Sets</li>
<li>Heaps and priority queues</li>
<li>Implementing autocompletion using tries</li>
<li>Introduction to caching</li>
<li>In-memory caching with the <kbd>functools.lru_cache</kbd> decorator</li>
<li>On-disk cache with <kbd>joblib.Memory</kbd></li>
<li>Fast and memory-efficient loops with comprehensions and generators</li>
</ul>

<h2>Useful algorithms and data structures</h2>
<p>Algorithmic improvements are especially effective in increasing&nbsp;performance because they typically allow the application to scale better&nbsp;with increasingly large inputs.</p>
<p>Algorithm running times can be classified according to their computational complexity, a characterization&nbsp;of the resources required to perform a task. Such classification is expressed through the Big-O notation, an upper bound on the operations required to execute the task, which usually depends on the input size.</p>
<p>For example, incrementing each element of a list can be implemented using a <kbd>for</kbd> loop, as follows:</p>
<pre><code class="lang-python">    input = list(range(10))
    for i, _ in enumerate(input):
        input[i] += 1 
</code></pre>
<p>If the operation does not depend on the size of the input (for example, accessing the first element of a list), the algorithm is said to take constant, or <em>O</em>(1), time. This means that, no matter how much data we have, the time to run the algorithm will&nbsp;always be the same.</p>
<p>In this simple algorithm, the <kbd>input[i] += 1</kbd> operation will be repeated 10 times, which is the size of the input. If we double the size of the input array, the number of operations will increase proportionally. Since the number of operations is proportional to the input size, this algorithm is said to take <em>O</em>(<em>N</em>) time, where <em>N</em> is the size of the input array.</p>
<p>In some instances, the running time may depend on the structure of the input (for example, if the collection is sorted or contains many duplicates). In these cases, an algorithm may have different&nbsp;best-case, average-case, and worst-case running times. Unless stated otherwise, the running times presented in this chapter are considered to be average running times.</p>
<p>In this section,&nbsp;we will examine the running times of the main algorithms and data structures that are implemented in the Python standard library, and understand how improving running times results in massive&nbsp;gains and allows us to solve large-scale problems with elegance.</p>
<p>You can find the code used to run the benchmarks in this chapter in the <kbd>Algorithms.ipynb</kbd>&nbsp;notebook, which can be opened using Jupyter.</p>

<h3>Lists and deques</h3>
<p>Python lists are ordered collections of elements and, in Python, are implemented as resizable arrays. An array is a basic data structure that&nbsp;consists of a series of contiguous memory locations, and each location contains a reference to a Python object.</p>
<p>Lists shine in accessing, modifying, and appending elements. Accessing or modifying an element involves fetching the object reference from the appropriate position of the underlying array and has complexity <em>O</em>(1). Appending an element is also very fast. When an empty list is created, an array of fixed size is allocated and, as we insert elements, the slots in the array are gradually filled up. Once all the slots are occupied, the list needs to increase the size of its underlying array, thus triggering a memory reallocation that can&nbsp;take <em>O</em>(<em>N</em>) time. Nevertheless, those memory allocations are infrequent, and the time complexity for the append operation is referred to as amortized O(1) time.</p>
<p>The&nbsp;list operations that may have efficiency problems are those that add or remove&nbsp;elements at the beginning (or somewhere in the middle) of the list. When an item is inserted, or removed, from the beginning of a list, all the subsequent elements of the array need to be shifted by a position, thus taking <em>O</em>(<em>N</em>) time.</p>
<p>In the following table, the timings for different operations on a list of size 10,000 are shown; you can see how insertion and removal performances vary quite dramatically&nbsp;if performed at the beginning or at the end of the list:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td><kbd>list.pop()</kbd></td>
<td>0.50</td>
<td>0.59</td>
<td>0.58</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>list.pop(0)</kbd></td>
<td>4.20</td>
<td>8.36</td>
<td>12.09</td>
<td><em>O</em>(<em>N</em>)</td>
</tr>
<tr>
<td><kbd>list.append(1)</kbd></td>
<td>0.43</td>
<td>0.45</td>
<td>0.46</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>list.insert(0, 1)</kbd></td>
<td>6.20</td>
<td>11.97</td>
<td>17.41</td>
<td><em>O</em>(<em>N</em>)</td>
</tr>
</tbody>
</table>
<p>In some cases, it is necessary to&nbsp;efficiently perform insertion or removal of elements both at the&nbsp;beginning and at the end of the collection. Python provides a data structure with those properties in the&nbsp;<kbd>collections.deque</kbd>&nbsp;class. The word <strong>deque</strong> stands for double-ended queue because this data structure is designed to efficiently put and remove elements at the beginning and at the end of the collection, as it is in the case of queues. In Python, deques are implemented as doubly-linked lists.</p>
<p>Deques, in addition to <kbd>pop</kbd> and <kbd>append</kbd>, expose the <kbd>popleft</kbd>&nbsp;and <kbd>appendleft</kbd>&nbsp;methods&nbsp;that have <em>O</em>(1) running time:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td><kbd>deque.pop()</kbd></td>
<td>0.41</td>
<td>0.47</td>
<td>0.51</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque.popleft()</kbd></td>
<td>0.39</td>
<td>0.51</td>
<td>0.47</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque.append(1)</kbd></td>
<td>0.42</td>
<td>0.48</td>
<td>0.50</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque.appendleft(1)</kbd></td>
<td>0.38</td>
<td>0.47</td>
<td>0.51</td>
<td><em>O</em>(1)</td>
</tr>
</tbody>
</table>
<p>Despite these advantages, deques should not be used to&nbsp;replace regular lists in most cases. The efficiency gained by the&nbsp;<kbd>appendleft</kbd> and <kbd>popleft</kbd> operations comes at a cost: accessing an element in the middle of a deque is a O(N) operation, as shown in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td><kbd>deque[0]</kbd></td>
<td>0.37</td>
<td>0.41</td>
<td>0.45</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque[N -&nbsp; 1]</kbd></td>
<td>0.37</td>
<td>0.42</td>
<td>0.43</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque[int(N / 2)]</kbd></td>
<td>1.14</td>
<td>1.71</td>
<td>2.48</td>
<td><em>O</em>(N)</td>
</tr>
</tbody>
</table>
<p>Searching for an item in a list is generally&nbsp;a <em>O</em>(<em>N</em>) operation and is performed using the <kbd>list.index</kbd>&nbsp;method. A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the&nbsp;<kbd>bisect</kbd> module.</p>
<p>The <kbd>bisect</kbd> module allows fast searches on sorted arrays. The <kbd>bisect.bisect</kbd> function can be used on a sorted list to find the index to place an element while maintaining the array in sorted order. In the following example, we can see that if we want to insert the <kbd>3</kbd> element in the array while keeping <kbd>collection</kbd> in sorted order, we should put <kbd>3</kbd> in the third position (which corresponds to index 2):</p>
<pre><code class="lang-python">    insert bisect
    collection = [1, 2, 4, 5, 6]
    bisect.bisect(collection, 3)
    # Result: 2
</code></pre>
<p>This function uses the binary search algorithm that has <em>O</em>(<em>log</em>(<em>N</em>)) running time. Such a running time is exceptionally fast, and basically means that your running time will increase by a constant amount every time you <em>double</em> your input size. This means that if, for example, your program takes <kbd>1</kbd> second to run on an input of size <kbd>1000</kbd>, it will take <kbd>2</kbd> seconds to process an input of size <kbd>2000</kbd>, <kbd>3</kbd> seconds to process an input of size <kbd>4000</kbd>, and so on. If you had <kbd>100</kbd> seconds, you could theoretically process an input of size <kbd>10<sup>33</sup></kbd>, which is larger than the number of atoms in your body!</p>
<p>If the value we are trying to insert is already present in the list, the <kbd>bisect.bisect</kbd> function will return the location <em>after</em> the already present value.&nbsp; Therefore, we can use the <kbd>bisect.bisect_left</kbd>&nbsp;variant, which returns the correct index in the following way (taken from the module documentation at <a href="https://docs.python.org/3.5/library/bisect.html">https://docs.python.org/3.5/library/bisect.html</a>):</p>
<pre><code class="lang-python">    def index_bisect(a, x):
      'Locate the leftmost value exactly equal to x'
      i = bisect.bisect_left(a, x)
      if i != len(a) and a[i] == x:
      return i
      raise ValueError
</code></pre>
<p>In the following table, you can see how the running time of the <kbd>bisect</kbd> solution is barely affected at these input sizes, making it a suitable solution when searching through very large collections:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong>N=10000 (</strong><strong>&micro;s)</strong></td>
<td><strong>N=20000 (</strong><strong>&micro;s)</strong></td>
<td><strong>N=30000 (</strong><strong>&micro;s)</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td><kbd>list.index(a)</kbd></td>
<td>87.55</td>
<td>171.06</td>
<td>263.17</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td><kbd>index_bisect(list, a)</kbd></td>
<td>3.16</td>
<td>3.20</td>
<td>4.71</td>
<td><em>O</em>(log(N))</td>
</tr>
</tbody>
</table>

<h3>Dictionaries</h3>
<p>Dictionaries are extremely versatile and extensively used in the Python language. Dictionaries are implemented as hash maps and are very good at element insertion, deletion, and access; all these operations have an average <em>O</em>(1) time complexity.&nbsp;</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>In Python versions up to 3.5, dictionaries are unordered collections. Since Python 3.6, dictionaries are capable of maintaining their elements by order of insertion.</p>
</div>
</div>
<p>A hash map is a data structure that associates a set of key-value pairs. The principle behind hash maps is to assign a specific index to each key so that its associated value can be stored in an array. The index can be obtained through the use of a <kbd>hash</kbd> function; Python implements hash functions for several data types. As a demonstration, the generic function to obtain hash codes is <kbd>hash</kbd>. In the following example, we show you how to obtain the hash code given the <kbd>"hello"</kbd>&nbsp;string:</p>
<pre><code class="lang-python">    hash("hello")
    # Result: -1182655621190490452

    # To restrict the number to be a certain range you can use
    # the modulo (%) operator
    hash("hello") % 10
    # Result: 8
</code></pre>
<p>Hash maps can be tricky to implement because they need to handle collisions that happen when two different objects have the same hash code. However, all the complexity is elegantly hidden behind the implementation and the default collision resolution works well in most real-world scenarios.</p>
<p>Access, insertion, and removal of an item in a dictionary scales as <em>O</em>(1) with the size of the dictionary. However, note that the computation of the hash function still needs to happen and, for strings, the computation scales with the length of the string. As string keys are usually relatively small, this doesn't constitute a problem in practice.</p>
<p>A dictionary can be used to efficiently count unique elements in a list. In this example, we define the <kbd>counter_dict</kbd> function that takes a list and returns a dictionary containing the number of occurrences of each value in the list:</p>
<pre><code class="lang-python">    def counter_dict(items): 
        counter = {} 
        for item in items: 
            if item not in counter: 
                counter[item] = 0 
            else: 
                counter[item] += 1 
        return counter
</code></pre>
<p>The code can be somewhat simplified using&nbsp;<kbd>collections.defaultdict</kbd>, which can be used to produce dictionaries where each new key is automatically assigned a default value. In the following code, the <kbd>defaultdict(int)</kbd> call produces a dictionary where every new element is automatically assigned a zero value, and can be used to streamline the counting:</p>
<pre><code class="lang-python">    from collections import defaultdict
    def counter_defaultdict(items):
        counter = defaultdict(int)
        for item in items:
            counter[item] += 1
        return counter
</code></pre>
<p>The <kbd>collections</kbd> module also includes a <kbd>Counter</kbd> class that can be used for the same purpose with&nbsp;a single line of code:</p>
<pre><code class="lang-python">    from collections import Counter
    counter = Counter(items)
</code></pre>
<p>Speed-wise, all these ways of counting have the same time complexity, but the <kbd>Counter</kbd> implementation is the most efficient, as shown in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong>N=1000 (</strong><strong>&micro;s)</strong></td>
<td><strong>N=2000 (</strong><strong>&micro;s)</strong></td>
<td><strong>N=3000 (</strong><strong>&micro;s)</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td><kbd>Counter(items)</kbd></td>
<td>51.48</td>
<td>96.63</td>
<td>140.26</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td><kbd>counter_dict(items)</kbd></td>
<td>111.96</td>
<td>197.13</td>
<td>282.79</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td><kbd>counter_defaultdict(items)</kbd></td>
<td>120.90</td>
<td>238.27</td>
<td>359.60</td>
<td><em>O</em>(N)</td>
</tr>
</tbody>
</table>

<h4>Building an in-memory search index using a hash map</h4>
<p>Dictionaries can be used to quickly search for a word in a list of documents, similar to a search engine. In this subsection, we will learn how to build an inverted index based on a dictionary of lists. Let's say we have a collection of four documents:</p>
<pre><code class="lang-python">    docs = ["the cat is under the table",
            "the dog is under the table",
            "cats and dogs smell roses",
            "Carla eats an apple"]
</code></pre>
<p>A simple way to retrieve all the documents that match a query is to scan each document and test for the presence of a word. For example, if we want to look up the documents where the word <kbd>table</kbd>&nbsp;appears, we can&nbsp;employ the following filtering operation:</p>
<pre><code class="lang-python">    matches = [doc for doc in docs if "table" in doc]
</code></pre>
<p>This approach is simple and works well when we have one-off queries; however, if we need to query the collection very often, it can be beneficial to optimize querying time. Since the per-query cost of the linear scan is <em>O</em>(<em>N</em>), you can imagine that a better scaling will allow us to handle much larger document collections.</p>
<p>A better strategy is to spend some time preprocessing the documents so that they are easier to find at query time. We can build a structure, called the&nbsp;<strong>inverted index</strong>, that associates each word in our collection with the list of documents where that word is present. In our earlier&nbsp;example, the word <kbd>"table"</kbd> will be associated to the <kbd>"the cat is under the table"</kbd> and <kbd>"the dog is under the table"</kbd>&nbsp;documents; they correspond&nbsp;to indices <kbd>0</kbd> and <kbd>1</kbd>.</p>
<p>Such a mapping can be implemented by going over our collection of documents and storing in a dictionary the index of the documents where that term appears. The implementation is similar to the <kbd>counter_dict</kbd>&nbsp;function, except that, instead of accumulating a counter, we are growing the list of documents that match the current term:</p>
<pre><code class="lang-python">    # Building an index
    index = {}
    for i, doc in enumerate(docs):
        # We iterate over each term in the document
        for word in doc.split():
            # We build a list containing the indices 
            # where the term appears
            if word not in index:
                index[word] = [i]
            else:
                index[word].append(i)
</code></pre>
<p>Once we have built our index, doing a query involves a simple dictionary lookup. For example, if we want to return all the documents containing the term table, we can simply query the index, and retrieve the corresponding documents:</p>
<pre><code class="lang-python">    results = index["table"]
    result_documents = [docs[i] for i in results]
</code></pre>
<p>Since all it takes to query our collection is a dictionary access, the index can handle queries with time complexity <em>O</em>(1)! Thanks to the inverted index, we are now able to query any number of documents (as long as they fit in memory) in constant time. Needless to say, indexing is a technique widely used to quickly retrieve data not only in search engines, but also in databases and any system that requires fast searches.</p>
<p>Note that building an inverted index is an expensive operation and requires you to encode every possible query. This is a substantial drawback, but the benefits are great and it may be worthwhile to pay the price in terms of decreased flexibility.</p>

<h3>Sets</h3>
<p>Sets are unordered collections of elements, with the additional restriction that the elements must be unique.&nbsp; The main use-cases where sets are a good choice are membership tests (testing&nbsp;if an element is present in the collection) and, unsurprisingly, set operations such as union, difference, and intersection.</p>
<p>In Python, sets are implemented using a hash-based algorithm just like dictionaries; therefore, the time complexities for addition, deletion, and test for membership scale as <em>O</em>(1) with the size of the collection.</p>
<p>Sets contain only unique elements. An immediate use case of sets&nbsp;is the removal of duplicates from a collection, which can be accomplished by simply passing the collection through the&nbsp;<kbd>set</kbd>&nbsp;constructor, as follows:</p>
<pre><code class="lang-python">    # create a list that contains duplicates
    x = list(range(1000)) + list(range(500))
    # the set *x_unique* will contain only 
    # the unique elements in x
    x_unique = set(x)
</code></pre>
<p>The time complexity&nbsp;for removing duplicates is <em>O</em>(<em>N</em>), as it requires to read the input and put each element in the set.</p>
<p>Sets expose a number of operations like union, intersection, and difference. The union&nbsp;of two sets is a new set containing all the elements of both the sets; the intersection is a new set that contains only the elements in common between the two sets, and the difference is a new set containing the element of the first set that are not contained in the second set. The time complexities for these operations are shown in the following table. Note that since we have two different input sizes, we will use&nbsp;the letter S to indicate the size of the first set (called <kbd>s</kbd>), and T to indicate the size of the second set (called <kbd>t</kbd>):</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td><kbd>s.union(t)</kbd></td>
<td><em>O</em>(<em>S</em> + <em>T</em>)</td>
</tr>
<tr>
<td><kbd>s.intersection(t)</kbd></td>
<td><em>O</em>(<em>min</em>(<em>S</em>, <em>T</em>))</td>
</tr>
<tr>
<td><kbd>s.difference(t)</kbd></td>
<td><em>O</em>(<em>S</em>)</td>
</tr>
</tbody>
</table>
<p>An application of set operations are, for example, Boolean queries. Going back to&nbsp;the inverted index example of the previous subsection, we may want to support queries that include multiple terms. For example, we may want to search for all the documents that contain the words&nbsp;<kbd>cat</kbd>&nbsp;and <kbd>table</kbd>. This kind of a query can be efficiently computed by taking the intersection between the set of documents containing <kbd>cat</kbd>&nbsp;and the set of documents containing <kbd>table</kbd>.</p>
<p>In order to efficiently support those operations, we can change our indexing code so that each term is associated to a set of documents (rather than a list). After applying this change, calculating more advanced queries is a matter of applying the right set operation. In the following code, we show the inverted index based on sets and the query using set operations:</p>
<pre><code class="lang-python">    # Building an index using sets
    index = {}
    for i, doc in enumerate(docs):
        # We iterate over each term in the document
        for word in doc.split():
            # We build a set containing the indices 
            # where the term appears
            if word not in index:
                index[word] = {i}
            else:
                index[word].add(i)
    
    # Querying the documents containing both "cat" and "table"
    index['cat'].intersection(index['table'])
</code></pre>

<h3>Heaps</h3>
<p>Heaps are data structures designed&nbsp;to quickly find and extract the maximum&nbsp;(or minimum) value in a collection. A typical use-case for heaps is to process a series of incoming tasks in order of maximum priority.</p>
<p>One can theoretically use a sorted&nbsp;list using the tools in the <kbd>bisect</kbd> module; however, while extracting the maximum value will take <em>O</em>(1) time (using <kbd>list.pop</kbd>), insertion will still take <em>O</em>(N) time (remember that, even if finding the insertion point takes <em>O</em>(<em>log</em>(<em>N</em>)) time, inserting an element in the middle of a list is still a <em>O</em>(<em>N</em>) operation). A heap is a more efficient data structure that allows for insertion and extraction of maximum values with <em>O</em>(<em>log</em>(<em>N</em>)) time complexity.</p>
<p>In Python, heaps are built using the procedures contained in the <kbd>heapq</kbd> module on an&nbsp;underlying list. For example, if we have&nbsp;a list of 10 elements, we can reorganize&nbsp;it into a heap with the <kbd>heapq.heapify</kbd> function:</p>
<pre><code class="lang-python">    import heapq

    collection = [10, 3, 3, 4, 5, 6]
    heapq.heapify(collection)
</code></pre>
<p>To perform the insertion and extraction operations on the heap, we can use the&nbsp;<kbd>heapq.heappush</kbd> and <kbd>heapq.heappop</kbd>&nbsp;functions. The <kbd>heapq.heappop</kbd> function will extract the minimum value in the collection in <em>O</em>(<em>log</em>(<em>N</em>)) time and can be used in the following way:</p>
<pre><code class="lang-python">    heapq.heappop(collection)
    # Returns: 3
</code></pre>
<p>Similarly, you can push the integer <kbd>1</kbd>, with the <kbd>heapq.heappush</kbd> function, as follows:</p>
<pre><code class="lang-python">    heapq.heappush(collection, 1)
</code></pre>
<p>Another easy-to-use option is the <kbd>queue.PriorityQueue</kbd> class that, as a bonus, is thread and process-safe. The <kbd>PriorityQueue</kbd> class can be filled up with elements using the&nbsp;<kbd>PriorityQueue.put</kbd>&nbsp;method, while&nbsp;<kbd>PriorityQueue.get</kbd> can be used to extract the minimum value in the collection:</p>
<pre><code class="lang-python">    from queue import PriorityQueue

    queue = PriorityQueue()
    for element in collection:
        queue.put(element)

    queue.get()
    # Returns: 3
</code></pre>
<p>If the maximum element is required, a simple trick is to multiply each element of the list by&nbsp;<kbd>-1</kbd>. In this way, the order of the elements will be inverted. Also, if you want to associate an object (for example, a task to run) to each number (which can represent the priority), one can insert tuples of the <kbd>(number, object)</kbd>&nbsp;form; the comparison operator for the tuple will be ordered with respect to its first element, as shown in the following example:</p>
<pre><code class="lang-python">    queue = PriorityQueue()
    queue.put((3, "priority 3"))
    queue.put((2, "priority 2"))
    queue.put((1, "priority 1"))

    queue.get()
    # Returns: (1, "priority 1")
</code></pre>

<h3>Tries</h3>
<p>A perhaps less popular data structure, very useful in practice, is the trie (sometimes called prefix tree). Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search-as-you type and autocompletion, where the list of available completions is very large and short response times are required.</p>
<p>Unfortunately, Python does not include a trie implementation in its standard library; however, many efficient implementations are readily available through PyPI. The one we will use in this subsection is <kbd>patricia-trie</kbd>, a single-file, pure Python implementation of trie.&nbsp;As an example, we will use <kbd>patricia-trie</kbd> to perform the task of finding the longest prefix in a set of strings (just like autocompletion).</p>
<p>As&nbsp;an example, we can demonstrate how fast a trie is able to search through a list of strings. In order to generate a large amount of unique random strings, we can define a function,&nbsp;<kbd>random_string</kbd>. The <kbd>random_string</kbd>&nbsp;function will return a string composed of random uppercase characters and, while there is a chance to get duplicates, we can greatly reduce the probability of duplicates to the point of being negligible if we make the string long enough. The implementation of the <kbd>random_string</kbd> function is shown as follows:</p>
<pre><code class="lang-python">    from random import choice
    from string import ascii_uppercase

    def random_string(length):
     """Produce a random string made of *length* uppercase ascii 
     characters"""
     return ''.join(choice(ascii_uppercase) for i in range(length))
</code></pre>
<p>We can build a list of random strings&nbsp;and time how fast it searches for a prefix (in our case, the <kbd>"AA"</kbd>&nbsp;string) using the <kbd>str.startswith</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    strings = [random_string(32) for i in range(10000)]
    matches = [s for s in strings if s.startswith('AA')]
</code></pre>
<p>List comprehension and <kbd>str.startwith</kbd> are already very optimized operations and, on this small dataset, the search takes only a millisecond or so:</p>
<pre><code class="lang-python">    %timeit [s for s in strings if s.startswith('AA')]

    1000 loops, best of 3: 1.76 ms per loop
</code></pre>
<p>Now, let's try using a trie for the same operation. In this example, we will use the <kbd>patricia-trie</kbd> library that is installable through <kbd>pip</kbd>. The&nbsp;<kbd>patricia.trie</kbd>&nbsp;class implements a variant of the trie data structure with an interface similar to a dictionary. We can initialize our trie by creating a dictionary from our list of strings, as follows:</p>
<pre><code class="lang-python">    from patricia import trie
    strings_dict = {s:0 for s in strings} 
    # A dictionary where all values are 0
    strings_trie = trie(**strings_dict)
</code></pre>
<p>To query <kbd>patricia-trie</kbd> for a matching prefix, we can use the <kbd>trie.iter</kbd>&nbsp;method, which returns an iterator&nbsp;over the matching strings:</p>
<pre><code class="lang-python">    matches = list(strings_trie.iter('AA'))
</code></pre>
<p>Now that we know how to initialize and query a trie, we can time the operation:</p>
<pre><code class="lang-python">    %timeit list(strings_trie.iter('AA'))
    10000 loops, best of 3: 60.1 &micro;s per loop
</code></pre>

<p>If you look closely, the timing for this input size is <strong>60.1 &micro;s</strong>, which is about 30 times faster&nbsp;(1.76 ms = 1760 &micro;s) than linear search! The speed up is so impressive because of the better computational complexity of the trie prefix search. Querying&nbsp;a trie has a time complexity&nbsp;<em>O</em>(<em>S</em>), where S is the length of the longest string&nbsp;in the collection, while the time complexity of&nbsp;a simple linear scan is <em>O</em>(<em>N</em>), where <em>N</em> is the size of the collection.&nbsp;</p>
<p>Note that if we want to return all the prefixes that match, the running time will be proportional to the number of results that match the prefix. Therefore, when designing timing benchmarks, care must be taken to ensure that we are always returning the same number of results.</p>
<p>The scaling properties of a trie versus a linear scan for datasets of different sizes that contains ten&nbsp;prefix matches&nbsp;are&nbsp;shown in the following table:</p>

<table>
<tbody>
<tr>
<td><strong>Algorithm</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>&micro;s)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td>Trie</td>
<td>17.12</td>
<td>17.27</td>
<td>17.47</td>
<td><em>O</em>(<em>S</em>)</td>
</tr>
<tr>
<td>Linear scan</td>
<td>1978.44</td>
<td>4075.72</td>
<td>6398.06</td>
<td><em>O</em>(<em>N</em>)</td>
</tr>
</tbody>
</table>
<p>An interesting fact is that the implementation of <kbd>patricia-trie</kbd> is actually a single Python file; this clearly shows how simple and powerful a clever algorithm can be. For extra features and performance, other C-optimized trie libraries are also available, such as <kbd>datrie</kbd> and <kbd>marisa-trie</kbd>.</p>

<h2>Caching and memoization</h2>
<p>Caching is a great technique used to improve the performance of a wide range of applications. The idea behind caching is to store expensive results&nbsp;in a temporary location, called cache, that can be located in memory, on-disk, or in a remote location.</p>
<p>Web applications make extensive use of caching. In a web application, it often happens that users request a certain page at the same time. In this case, instead of recomputing the page for each user, the web application can compute it once and serve the user the already rendered page. Ideally, caching also needs a mechanism for invalidation so that if the page needs to be updated, we can recompute it before serving it again. Intelligent caching allows web applications to handle increasing number of users with less resources.&nbsp;Caching can also be done preemptively, such as the later sections of the video get buffered when watching a video online.</p>
<p>Caching is also used to improve the performance of certain algorithms. A great example is computing the Fibonacci sequence. Since computing the next number in the Fibonacci sequence requires the previous number in the sequence, one can store and reuse previous results, dramatically improving the running time. Storing and reusing the results of the previous function calls in an application is usually termed as&nbsp;<strong>memoization</strong>, and is one of the forms of caching. Several other algorithms can take advantage of memoization to gain impressive performance improvements, and this programming technique is commonly referred to as&nbsp;<strong>dynamic programming</strong>.</p>
<p>The benefits of caching, however, do not come for free. What we are actually doing is sacrificing some space to improve the speed of the application. Additionally, if the cache is stored in a location on the network, we may incur transfer costs and general time needed for communication. One should evaluate when it is convenient to use a cache and how much space we are willing to trade&nbsp;for an increase in speed.</p>
<p>Given the&nbsp;usefulness of this technique, the Python standard library includes a simple in-memory cache out of the box in the <kbd>functools</kbd>&nbsp;module. The <kbd>functools.lru_cache</kbd> decorator can be used to easily cache&nbsp;the results of a function. In the following example, we create a function,&nbsp;<kbd>sum2</kbd>,&nbsp;that prints a statement and returns the sum of two numbers. By running the function twice, you can see that the first time the <kbd>sum2</kbd>&nbsp;function is executed the <kbd>"Calculating ..."</kbd> string is produced, while the second time the result is returned without running the function:</p>
<pre><code class="lang-python">    from functools import lru_cache

    @lru_cache()
    def sum2(a, b):
        print("Calculating {} + {}".format(a, b))
        return a + b

    print(sum2(1, 2))
    # Output: 
    # Calculating 1 + 2
    # 3
    print(sum2(1, 2))
    # Output: 
    # 3
</code></pre>
<p>The <kbd>lru_cache</kbd> decorator also provides other basic features. To restrict the size of the cache, one can set&nbsp;the number of elements that we intend to maintain through the <kbd>max_size</kbd>&nbsp;argument. If we want our cache size to be unbounded, we can specify a value of <kbd>None</kbd>. An example usage of <kbd>max_size</kbd> is shown here:</p>
<pre><code class="lang-python">    @lru_cache(max_size=16)
    def sum2(a, b):
        ...
</code></pre>
<p>In this way, as we execute <kbd>sum2</kbd> with different arguments, the cache will reach a maximum size of <kbd>16</kbd> and, as we keep requesting more calculations, new values&nbsp;will replace older values in the cache. The <kbd>lru</kbd> prefix originates from&nbsp;this strategy, which means least recently used.</p>
<p>The <kbd>lru_cache</kbd> decorator also adds extra functionalities to the decorated function. For example, it is possible to examine the cache performance using the <kbd>cache_info</kbd> method, and it is possible to reset the cache using the <kbd>cache_clear</kbd> method, as follows:</p>
<pre><code class="lang-python">    sum2.cache_info()
    # Output: CacheInfo(hits=0, misses=1, maxsize=128, currsize=1)
    sum2.cache_clear()
</code></pre>
<p>As an example, we can see how a problem, such as computing the fibonacci series, may benefit from caching. We can define a <kbd>fibonacci</kbd> function and time its execution:</p>
<pre><code class="lang-python">    def fibonacci(n):
        if n &lt; 1:
            return 1
        else:
            return fibonacci(n - 1) + fibonacci(n - 2)

    # Non-memoized version
    %timeit fibonacci(20)
    100 loops, best of 3: 5.57 ms per loop
</code></pre>
<p>The execution takes 5.57 ms, which is very high. The scaling of the function written in this way has poor performance; the previously computed fibonacci sequences&nbsp;are not reused, causing this algorithm to have an exponential scaling of roughly <em>O</em>(<em>2<sup>N</sup></em>).</p>
<p>Caching can improve this algorithm by storing and reusing the already-computed fibonacci numbers. To implement the cached version, it is sufficient to apply the <kbd>lru_cache</kbd> decorator to the original <kbd>fibonacci</kbd> function. Also, to design a proper benchmark, we need to ensure that a new cache is instantiated for every run; to do this, we can use the <kbd>timeit.repeat</kbd> function, as shown in the following example:</p>
<pre><code class="lang-python">    import timeit
    setup_code = '''
    from functools import lru_cache
    from __main__ import fibonacci
    fibonacci_memoized = lru_cache(maxsize=None)(fibonacci)
    '''

    results = timeit.repeat('fibonacci_memoized(20)',
                            setup=setup_code,
                            repeat=1000,
                            number=1)
    print("Fibonacci took {:.2f} us".format(min(results)))
    # Output: Fibonacci took 0.01 us
</code></pre>
<p>Even though we changed the algorithm by adding&nbsp;a simple decorator, the running time now is much less than a microsecond. The reason is that, thanks to caching, we now have a linear time algorithm instead of&nbsp;an exponential one.</p>
<p>The&nbsp;<kbd>lru_cache</kbd>&nbsp;decorator&nbsp;can be used to implement simple in-memory caching in your application. For more advanced use cases,&nbsp;third-party modules can be used for more powerful implementation and on-disk caching.</p>

<h4>Joblib</h4>
<p>A simple library that, among other things, provides a simple on-disk cache is <kbd>joblib</kbd>. The package can be used in a similar way as <kbd>lru_cache</kbd>, except that the results will be stored on disk and will persist between runs.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The <kbd>joblib</kbd> module can be installed from PyPI using the <kbd>pip install joblib</kbd>&nbsp;command.</p>
</div>
</div>
<p>The <kbd>joblib</kbd> module provides the&nbsp;<kbd>Memory</kbd>&nbsp;class that can be used to memoize functions using the <kbd>Memory.cache</kbd> decorator:</p>
<pre><code class="lang-python">    from joblib import Memory
    memory = Memory(cachedir='/path/to/cachedir')

    @memory.cache
    def sum2(a, b):
        return a + b
</code></pre>
<p>The function will behave similar to <kbd>lru_cache</kbd>, with the exception that the results will be stored on-disk in the directory specified by the <kbd>cachedir</kbd> argument during <kbd>Memory</kbd> initialization. Additionally, the cached results will persist&nbsp;over subsequent&nbsp;runs!</p>
<p>The <kbd>Memory.cache</kbd> method also allows to limit recomputation only when certain arguments change, and the resulting decorated function supports basic functionalities to clear and analyze the cache.&nbsp;</p>
<p>Perhaps the best&nbsp;<kbd>joblib</kbd>&nbsp;feature is that, thanks to intelligent hashing algorithms, it provides efficient memoization of functions that operate on <kbd>numpy</kbd> arrays, and is particularly useful in scientific and engineering applications.</p>

<h2>Comprehensions and generators</h2>
<p>In this section, we will explore a few simple strategies to speed up Python loops using comprehension and generators. In Python, comprehension and generator expressions are fairly optimized operations and should be preferred in place of explicit for-loops. Another reason to use this construct is readability; even if the speedup over a standard loop is modest, the comprehension and generator syntax is more compact and (most of the times) more intuitive.</p>
<p>In the following example, we can see that both the list comprehension and generator expressions are faster than an explicit loop when combined with the <kbd>sum</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    def loop(): 
        res = [] 
        for i in range(100000): 
            res.append(i * i) 
        return sum(res) 

    def comprehension(): 
        return sum([i * i for i in range(100000)]) 

    def generator(): 
        return sum(i * i for i in range(100000)) 

    %timeit loop() 
    100 loops, best of 3: 16.1 ms per loop 
    %timeit comprehension() 
    100 loops, best of 3: 10.1 ms per loop 
    %timeit generator() 
    100 loops, best of 3: 12.4 ms per loop 
</code></pre>
<p>Just like lists, it is possible to use <kbd>dict</kbd> comprehension&nbsp;to build dictionaries slightly more efficiently and compactly, as shown in the following code:</p>
<pre><code class="lang-python">    def loop(): 
        res = {} 
        for i in range(100000): 
            res[i] = i
        return res

    def comprehension(): 
        return {i: i for i in range(100000)}
    %timeit loop() 
    100 loops, best of 3: 13.2 ms per loop 
    %timeit comprehension() 
    100 loops, best of 3: 12.8 ms per loop
</code></pre>
<p>Efficient looping (especially in terms of memory) can be implemented using iterators and functions such as <kbd>filter</kbd> and <kbd>map</kbd>. As an example, consider the problem of applying a series of operations to a list using list comprehension and then taking the maximum value:</p>
<pre><code class="lang-python">    def map_comprehension(numbers):
        a = [n * 2 for n in numbers]
        b = [n ** 2 for n in a]
        c = [n ** 0.33 for n in b]
        return max(c)
</code></pre>
<p>The problem with this approach is that for every list comprehension, we are allocating a new list, increasing memory usage. Instead of using list comprehension, we can employ generators. Generators are objects that, when iterated upon, compute a value on the fly and return the result.</p>
<p>For example, the <kbd>map</kbd>&nbsp;function takes two arguments--a function and an iterator--and returns a generator that applies the function to every element of the collection. The important point is that the operation happens only <em>while we are iterating</em>, and not when <kbd>map</kbd> is invoked!</p>
<p>We can rewrite the previous function using <kbd>map</kbd> and by creating intermediate generators, rather than lists, thus saving memory by computing the values on the fly:</p>
<pre><code class="lang-python">    def map_normal(numbers):
        a = map(lambda n: n * 2, numbers)
        b = map(lambda n: n ** 2, a)
        c = map(lambda n: n ** 0.33, b)
        return max(c)
</code></pre>
<p>We can profile the memory of the two solutions using the <kbd>memory_profiler</kbd> extension from an IPython session. The extension provides a small utility,&nbsp;<kbd>%memit</kbd>, that will help us evaluate the memory usage of a Python statement in a way similar to <kbd>%timeit</kbd>, as illustrated in the following snippet:</p>
<pre><code class="lang-python">    %load_ext memory_profiler
    numbers = range(1000000)
    %memit map_comprehension(numbers)
    peak memory: 166.33 MiB, increment: 102.54 MiB
    %memit map_normal(numbers)
    peak memory: 71.04 MiB, increment: 0.00 MiB
</code></pre>
<p>As you can see, the memory used by the first version is&nbsp;<kbd>102.54 MiB</kbd>, while the second version consumes <kbd>0.00 MiB</kbd>! For the interested reader, more functions that return generators can be found in the <kbd>itertools</kbd> module, which provides a set of utilities designed to handle common iteration patterns.</p>

<h2>Summary</h2>
<p>Algorithmic optimization can improve how your application scales as we process increasingly large data. In this chapter, we demonstrated&nbsp;use-cases and running times of the most common data structures available in Python, such as lists, deques, dictionaries, heaps, and tries. We also covered caching, a technique that can be used&nbsp;to trade&nbsp;some space, in memory or on-disk,&nbsp;in exchange for increased&nbsp;responsiveness of an application. We also demonstrated how&nbsp;to get modest speed gains by replacing for-loops with fast constructs, such as list comprehensions and generator expressions.</p>
<p>In the subsequent&nbsp;chapters, we will learn how to improve performance&nbsp;further using numerical libraries such as <kbd>numpy</kbd>, and how to write extension modules in a lower-level language with the help of <em>Cython</em>.</p>

</div>





<!--Chapter 3-->

<div class="chapter" data-chapter-number="3">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 3 </span></div>
<h1 class="chaptertitle">Fast Array Operations with NumPy and Pandas</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>



<p>NumPy is the <em>de facto</em> standard for scientific computing in Python. It extends Python with a flexible multidimensional array that allows fast and concise mathematical calculations.</p>
<p>NumPy provides common data structures and algorithms designed to express&nbsp;complex mathematical operations using a concise syntax. The multidimensional array,&nbsp;<kbd>numpy.ndarray</kbd>,&nbsp;is internally based on C arrays. Apart from the performance benefits, this choice allows NumPy code to easily interface with the existing C and FORTRAN routines; NumPy is helpful in bridging the gap between Python and the legacy code written using those languages.</p>
<p>In this chapter, we will learn how to create and manipulate NumPy arrays. We will also explore the NumPy broadcasting feature used to rewrite complex mathematical expressions in an efficient and succinct manner.</p>
<p>Pandas is a tool that relies heavily on NumPy and provides additional&nbsp;data structures and algorithms targeted toward data analysis. We will introduce the main Pandas features and its usage. &nbsp;We will also learn how to achieve high performance from Pandas data structures and vectorized operations.&nbsp;</p>
<p>The topics covered&nbsp;in this chapter are as follows:</p>
<ul>
<li>Creating and manipulating NumPy arrays</li>
<li>Mastering&nbsp;NumPy's broadcasting feature for fast and succinct vectorized operations</li>
<li>Improving our particle simulator with NumPy</li>
<li>Reaching optimal performance with <kbd>numexpr</kbd></li>
<li>Pandas fundamentals</li>
<li>Database-style operations with Pandas</li>
</ul>

<h2>Getting started with NumPy</h2>
<p>The NumPy library revolves around its multidimensional array object, <kbd>numpy.ndarray</kbd>. NumPy arrays are collections of elements of the same data type; this fundamental restriction allows NumPy to pack the data in a way that allows for high-performance mathematical operations.</p>

<h3>Creating arrays</h3>
<p>You can create NumPy arrays using the <kbd>numpy.array</kbd> function. It takes a list-like object (or another array) as input and, optionally, a string expressing its data type. You can interactively test array creation using an IPython shell, as follows:</p>
<pre><code class="lang-python">    import numpy as np 
    a = np.array([0, 1, 2]) 
</code></pre>
<p>Every NumPy array has&nbsp;an associated data type that can be accessed using the&nbsp;<kbd>dtype</kbd> attribute. If we inspect the <kbd>a</kbd> array,&nbsp;we find that its &nbsp;<kbd>dtype</kbd> is <kbd>int64</kbd>, which stands for&nbsp;64-bit integer:</p>
<pre><code class="lang-python">    a.dtype 
    # Result: 
    # dtype('int64') 
</code></pre>
<p>We may decide to convert those integer numbers to&nbsp;<kbd>float</kbd> type. To do this,&nbsp;we can either pass the <kbd>dtype</kbd> argument at array initialization&nbsp;or cast the array to another data type using the <kbd>astype</kbd> method. The two ways to select a data type are shown in the following code:</p>
<pre><code class="lang-python">    a = np.array([1, 2, 3], dtype='float32') 
    a.astype('float32') 
    # Result:
    # array([ 0.,  1.,  2.], dtype=float32) 
</code></pre>
<p>To create an array with two dimensions (an array of arrays), we can perform the initialization using a nested sequence, as follows:</p>
<pre><code class="lang-python">    a = np.array([[0, 1, 2], [3, 4, 5]]) 
    print(a) 
    # Output:
    # [[0 1 2]
    #  [3 4 5]] 
</code></pre>
<p>The array created in this way has two dimensions, which are called&nbsp;<strong>axes</strong>&nbsp;in NumPy's jargon. An&nbsp;array formed in this way is like a table that contains two rows and three columns. We can access the axes&nbsp;using the <kbd>ndarray.shape</kbd> attribute:</p>
<pre><code class="lang-python">    a.shape 
    # Result:
    # (2, 3) 
</code></pre>
<p>Arrays can also be reshaped as long as the product of the shape dimensions is equal to the total number of elements in the array (that is,&nbsp;the total number of elements is conserved). For example, we can reshape an array containing 16 elements in the following ways: <kbd>(2, 8)</kbd>, <kbd>(4, 4)</kbd>, or <kbd>(2, 2, 4)</kbd>. To reshape an array, we can either use the <kbd>ndarray.reshape</kbd> method or assign a new value to&nbsp;the <kbd>ndarray.shape</kbd>&nbsp;tuple. The following code illustrates the use of the <kbd>ndarray.reshape</kbd> method:</p>
<pre><code class="lang-python">    a = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 
                  9, 10, 11, 12, 13, 14, 15]) 
    a.shape 
    # Output:
    # (16,)

    a.reshape(4, 4) # Equivalent: a.shape = (4, 4) 
    # Output: 
    # array([[ 0,  1,  2,  3],
    #        [ 4,  5,  6,  7],
    #        [ 8,  9, 10, 11],
    #        [12, 13, 14, 15]]) 
</code></pre>
<p>Thanks to this property, you can freely add dimensions of size one. You can reshape an array with 16 elements to <kbd>(16, 1)</kbd>, <kbd>(1, 16)</kbd>, <kbd>(16, 1, 1)</kbd>, and so on. In the next section, we will extensively use this feature to implement complex operations through&nbsp;<em>broadcasting</em>.&nbsp;</p>
<p>NumPy provides convenience functions, shown in the following code, to create arrays filled with zeros, ones, or with no&nbsp;initial value (in this case, their actual value is meaningless and depends on the memory state). Those functions take the array shape as a tuple and, optionally, its <kbd>dtype</kbd>:</p>
<pre><code class="lang-python">    np.zeros((3, 3)) 
    np.empty((3, 3)) 
    np.ones((3, 3), dtype='float32') 
</code></pre>
<p>In our examples, we will use the <kbd>numpy.random</kbd> module to generate random floating point numbers in the <kbd>(0, 1)</kbd> interval. The <kbd>numpy.random.rand</kbd>&nbsp;will take a shape and return an array of random numbers with that shape:</p>
<pre><code class="lang-python">    np.random.rand(3, 3) 
</code></pre>
<p>Sometimes it is convenient to initialize arrays that have the same shape as that of some other array. For that purpose, NumPy provides some handy functions, such as <kbd>zeros_like</kbd>, <kbd>empty_like</kbd>, and <kbd>ones_like</kbd>. These functions can be used as follows:</p>
<pre><code class="lang-python">    np.zeros_like(a) 
    np.empty_like(a) 
    np.ones_like(a) 
</code></pre>

<h3>Accessing arrays</h3>
<p>The NumPy array interface is, on a shallow level, similar to that of Python lists. NumPy arrays can be indexed using integers and iterated using a <kbd>for</kbd> loop:</p>
<pre><code class="lang-python">    A = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8]) 
    A[0] 
    # Result:
    # 0 

    [a for a in A] 
    # Result:
    # [0, 1, 2, 3, 4, 5, 6, 7, 8] 
</code></pre>
<p>In NumPy, array elements and sub-arrays can be conveniently accessed by using multiple values separated by commas inside&nbsp;the subscript operator, <kbd>[]</kbd>. If we take a <kbd>(3,3)</kbd> array (an array containing three triplets), and we access&nbsp;the element with index <kbd>0</kbd>, we obtain the first row, as follows:</p>
<pre><code class="lang-python">    A = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) 
    A[0] 
    # Result:
    # array([0, 1, 2]) 
</code></pre>
<p>We can index the row&nbsp;again by adding another index separated by a comma. To get the second element of the first row, we can use the&nbsp;<kbd>(0, 1)</kbd> index. An important observation is that the <kbd>A[0, 1]</kbd> notation is actually a shorthand for <kbd>A[(0, 1)]</kbd>, that is, we are actually indexing using a <em>tuple</em>! Both the versions are shown in the following snippet:</p>
<pre><code class="lang-python">    A[0, 1] 
    # Result:
    # 1

    # Equivalent version using tuple
    A[(0, 1)]
</code></pre>
<p>NumPy allows you to slice arrays into multiple dimensions. If we slice&nbsp;on the first dimension, we can obtain a collection of triplets, shown as follows:</p>
<pre><code class="lang-python">    A[0:2] 
    # Result:
    # array([[0, 1, 2], 
    #        [3, 4, 5]]) 
</code></pre>
<p>If we slice the array again on the second dimension with <kbd>0:2</kbd>, we are basically extracting&nbsp;the first two elements from the collection of triplets shown earlier. This results in an array of shape <kbd>(2, 2)</kbd>, shown in the following code:</p>
<pre><code class="lang-python">    A[0:2, 0:2] 
    # Result:
    # array([[0, 1], 
    #        [3, 4]]) 
</code></pre>
<p>Intuitively, you can update the values in the array using both numerical indexes and slices. An example is illustrated in the following code snippet:</p>
<pre><code class="lang-python">    A[0, 1] = 8 
    A[0:2, 0:2] = [[1, 1], [1, 1]]
</code></pre>
<p>Indexing with the slicing syntax is very fast because, unlike lists, it doesn't produce&nbsp;a copy of the array. In NumPy's terminology, it returns a <em>view</em> of the same memory area. If we take a slice of the original array, and then we change one of its values, the original array will be updated as well. The following code illustrates an example of this feature:</p>
<pre><code class="lang-python">    a= np.array([1, 1, 1, 1]) 
    a_view = a[0:2] 
    a_view[0] = 2 
    print(a) 
    # Output:
    # [2 1 1 1] 
</code></pre>
<p>It is important to be extra&nbsp;careful when mutating NumPy arrays. Since views share data, changing the values of a view&nbsp;can result in hard-to-find bugs. To prevent side effects, you can set the <kbd>a.flags.writeable = False</kbd> flag,&nbsp;which will prevent accidental mutation of the array or any of its views.</p>
<p>We can take a look at another example that shows how the slicing syntax can be used in a real-world setting. We define an <kbd>r_i</kbd> array, shown in the following line of code, which contains a set of 10 coordinates (<em>x</em>, <em>y</em>). Its shape will be <kbd>(10, 2)</kbd>:</p>
<pre><code class="lang-python">    r_i = np.random.rand(10, 2)
</code></pre>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>If you have a hard time distinguishing arrays that differ in the axes order, for example between an a array of shape&nbsp;<kbd>(10, 2)</kbd>&nbsp;and&nbsp;<kbd>(2, 10)</kbd>, it is useful to think that every time you say the word <em>of</em>, you should&nbsp;introduce a new dimension. An array with ten elements <em>of</em> size two will be <kbd>(10, 2)</kbd>. Conversely, an array with two elements <em>of</em> size ten will be <kbd>(2, 10)</kbd>.</p>
</div>
</div>
<p>A typical operation we may be interested in is the extraction of the <em>x</em> component from each coordinate. In other words, you want to extract the <kbd>(0, 0)</kbd>, <kbd>(1, 0)</kbd>, <kbd>(2, 0)</kbd>, and so on items, resulting in an array with shape <kbd>(10,)</kbd>. It is helpful to think that the first index is <em>moving</em> while the second one is <em>fixed</em> (at <kbd>0</kbd>). With this in mind, we will slice every index on the first axis (the moving one) and take the first element (the fixed one) on the second axis, as shown in the following line of code:</p>
<pre><code class="lang-python">    x_i = r_i[:, 0] 
</code></pre>
<p>On the other hand, the following expression will keep the first index fixed and the second index moving, returning the first (<em>x</em>, <em>y</em>) coordinate:</p>
<pre><code class="lang-python">    r_0 = r_i[0, :] 
</code></pre>
<p>Slicing all the indexes over the last axis is optional; using <kbd>r_i[0]</kbd> has the same effect as <kbd>r_i[0, :]</kbd>.</p>
<p>NumPy allows you to index an array using another NumPy array made of either integer or Boolean values--a feature called <em>fancy indexing</em>.</p>
<p>If you index an array (say, <kbd>a</kbd>) with another array of integers (say, <kbd>idx</kbd>), NumPy will interpret the integers as indexes and will return an array containing their corresponding values. If we index an array containing 10 elements with <kbd>np.array([0, 2, 3])</kbd>, we obtain an array of shape&nbsp;<kbd>(3,)</kbd> containing the elements at positions <kbd>0</kbd>, <kbd>2</kbd>, and <kbd>3</kbd>. The following code gives us an illustration of this concept:</p>
<pre><code class="lang-python">    a = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) 
    idx = np.array([0, 2, 3]) 
    a[idx] 
    # Result:
    # array([9, 7, 6]) 
</code></pre>
<p>You can use fancy indexing on multiple dimensions by passing an array for each dimension. If we want to extract the <kbd>(0, 2)</kbd> and <kbd>(1, 3)</kbd> elements, we have to pack all the indexes acting on the first axis in one array, and the ones acting on the second axis in another. This can be seen in the following code:</p>
<pre><code class="lang-python">    a = np.array([[0, 1, 2], [3, 4, 5], 
                  [6, 7, 8], [9, 10, 11]]) 
    idx1 = np.array([0, 1]) 
    idx2 = np.array([2, 3]) 
    a[idx1, idx2]
</code></pre>
<p>You can also use normal lists as index arrays, but not tuples. For example, the following two statements are equivalent:</p>
<pre><code class="lang-python">    a[np.array([0, 1])] # is equivalent to
    a[[0, 1]]
</code></pre>
<p>However, if you use a tuple, NumPy will interpret the following statement as an index on multiple dimensions:</p>
<pre><code class="lang-python">    a[(0, 1)] # is equivalent to
    a[0, 1] 
</code></pre>
<p>The index arrays are not required to be one-dimensional; we can extract elements from the original array in any shape. For example, we can select elements from the original array to form a <kbd>(2,2)</kbd> array, as shown:</p>
<pre><code class="lang-python">    idx1 = [[0, 1], [3, 2]] 
    idx2 = [[0, 2], [1, 1]] 
    a[idx1, idx2] 
    # Output: 
    # array([[ 0,  5],
    #        [10,  7]]) 
</code></pre>
<p>The array slicing and fancy-indexing features can be combined. This is useful, for instance, when we want to swap the <em>x</em> and <em>y</em> columns in a coordinate array. In the following code, the first index will be running over all the elements (a slice) and, for each of those, we extract the element in position <kbd>1</kbd> (the <em>y</em>) first and then the one in position <kbd>0</kbd> (the <em>x</em>):</p>
<pre><code class="lang-python">    r_i = np.random(10, 2) 
    r_i[:, [0, 1]] = r_i[:, [1, 0]] 
</code></pre>
<p>When the index array is of the <kbd>bool</kbd> type, the rules are slightly different. The <kbd>bool</kbd>&nbsp;array will act like a <em>mask</em>; every element corresponding to <kbd>True</kbd> will be extracted and put in the output array. This procedure is shown in the following code:</p>
<pre><code class="lang-python">    a = np.array([0, 1, 2, 3, 4, 5]) 
    mask = np.array([True, False, True, False, False, False]) 
    a[mask] 
    # Output:
    # array([0, 2]) 
</code></pre>
<p>The same rules apply when dealing with multiple dimensions. Furthermore, if the index array has the same shape as the original array, the elements corresponding to <kbd>True</kbd> will be selected and put in the resulting array.</p>
<p>Indexing in NumPy is a reasonably fast operation. Anyway, when speed is critical, you can use the slightly faster <kbd>numpy.take</kbd> and <kbd>numpy.compress</kbd> functions to squeeze out a little more performance. The first argument of <kbd>numpy.take</kbd> is the array we want to operate on, and the second is the list of indexes we want to extract. The last argument is <kbd>axis</kbd>; if not provided, the indexes will act on the flattened array; otherwise, they will act along the specified axis:</p>
<pre><code class="lang-python">    r_i = np.random(100, 2) 
    idx = np.arange(50) # integers 0 to 50 

    %timeit np.take(r_i, idx, axis=0) 
    1000000 loops, best of 3: 962 ns per loop 

    %timeit r_i[idx] 
    100000 loops, best of 3: 3.09 us per loop 
</code></pre>
<p>The similar, but faster version for Boolean arrays is <kbd>numpy.compress</kbd>, which works in the same way. The use of <kbd>numpy.compress</kbd> is shown as follows:</p>
<pre><code class="lang-python">    In [51]: idx = np.ones(100, dtype='bool') # all True values 
    In [52]: %timeit np.compress(idx, r_i, axis=0) 
    1000000 loops, best of 3: 1.65 us per loop 
    In [53]: %timeit r_i[idx] 
    100000 loops, best of 3: 5.47 us per loop 
</code></pre>

<h3>Broadcasting</h3>
<p>The true power of NumPy lies in its fast mathematical operations. The approach used by NumPy is to avoid stepping into the Python interpreter by performing element-wise calculation using optimized C code. <strong>Broadcasting</strong> is a clever set of rules that enables&nbsp;fast array calculations for arrays of similar (but not equal!) shape.</p>
<p>Whenever you do an arithmetic operation on two arrays (like a product), if the two operands have the same shape, the operation will be applied in an element-wise fashion. For example, upon multiplying two shape <kbd>(2,2)</kbd> arrays, the operation will be done between pairs of corresponding elements, producing another <kbd>(2, 2)</kbd> array,&nbsp;as shown in the following code:</p>
<pre><code class="lang-python">    A = np.array([[1, 2], [3, 4]]) 
    B = np.array([[5, 6], [7, 8]]) 
    A * B 
    # Output:
    # array([[ 5, 12],           
    #        [21, 32]]) 
</code></pre>
<p>If the shapes of the operands don't match, NumPy will attempt to match them using broadcasting rules. If one of the operands is a <em>scalar</em> (for example, a number), it will be applied to every element of the array, as the following code illustrates:</p>
<pre><code class="lang-python">    A * 2 
    # Output: 
    # array([[2, 4], 
    #        [6, 8]]) 
</code></pre>
<p>If the operand is another array, NumPy will try to match the shapes starting from the last axis. For example, if we want to combine an array of shape <kbd>(3, 2)</kbd> with one of shape <kbd>(2,)</kbd>, the second array will be&nbsp;repeated three&nbsp;times to generate a <kbd>(3, 2)</kbd> array. In other words, the array is <em>broadcasted</em>&nbsp;along a dimension to match the shape of the other operand, as shown in the following figure:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000014.png" class="lazyload" /></p>
</div>
<p>If the shapes mismatch, for example, when combining a <kbd>(3, 2)</kbd> array with a <kbd>(2, 2)</kbd> array, NumPy will throw an exception.</p>
<p>If one of the axis's size is 1, the array will be repeated over this axis until the shapes match. To illustrate that point,&nbsp;consider that&nbsp;we have an array of the following shape:</p>
<pre><code class="lang-python">    5, 10, 2 
</code></pre>
<p>Now, consider that we want to broadcast it with an array of shape <kbd>(5, 1, 2)</kbd>; the array will be repeated on the second axis 10 times, which is shown as follows:</p>
<pre><code class="lang-python">    5, 10, 2 
    5,  1, 2 &rarr; repeated 
    - - - - 
    5, 10, 2 
</code></pre>
<p>Earlier, we saw that it is possible to freely reshape arrays to add axes of size 1. Using the <kbd>numpy.newaxis</kbd> constant while indexing will introduce an extra dimension. For instance, if we have a <kbd>(5, 2)</kbd> array and we want to combine it with one of shape <kbd>(5, 10, 2)</kbd>, we can add an extra axis in the middle, as shown in the following code, to obtain a compatible <kbd>(5, 1, 2)</kbd> array:</p>
<pre><code class="lang-python">    A = np.random.rand(5, 10, 2) 
    B = np.random.rand(5, 2) 
    A * B[:, np.newaxis, :] 
</code></pre>
<p>This feature can be used, for example, to operate on all possible combinations of the two arrays. One of these applications is the <em>outer product</em>.&nbsp;Consider that&nbsp;we have the following two arrays:</p>
<pre><code class="lang-python">    a = [a1, a2, a3] 
    b = [b1, b2, b3] 
</code></pre>
<p>The outer product is a matrix containing the product of all the possible combinations (i, j) of the two array elements, as shown in the following snippet:</p>
<pre><code class="lang-python">    a x b = a1*b1, a1*b2, a1*b3 
            a2*b1, a2*b2, a2*b3 
            a3*b1, a3*b2, a3*b3 
</code></pre>
<p>To calculate this using NumPy, we will repeat the <kbd>[a1, a2, a3]</kbd> elements in one dimension, the <kbd>[b1, b2, b3]</kbd> elements in another dimension, and then take their element-wise product, as shown in the following figure:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000008.png" class="lazyload" /></p>
</div>
<p>Using code, our strategy will be to transform the <kbd>a</kbd> array from shape <kbd>(3,)</kbd> to shape <kbd>(3, 1)</kbd>, and the <kbd>b</kbd> array from shape <kbd>(3,)</kbd> to shape <kbd>(1, 3)</kbd>. The two arrays are broadcasted in the two dimensions and get multiplied together using the following code:</p>
<pre><code class="lang-python">    AB = a[:, np.newaxis] * b[np.newaxis, :] 
</code></pre>
<p>This operation is very fast and extremely effective as it avoids Python loops and is able to process a high number of elements at speeds comparable with pure C&nbsp;or FORTRAN code.</p>

<h3>Mathematical operations</h3>
<p>NumPy includes the most common mathematical operations available for broadcasting, by default, ranging from simple algebra to trigonometry, rounding, and logic. For instance, to take the square root of every element in the array, we can use&nbsp;<kbd>numpy.sqrt</kbd>, as shown in the following code:</p>
<pre><code class="lang-python">    np.sqrt(np.array([4, 9, 16])) 
    # Result:
    # array([2., 3., 4.]) 
</code></pre>
<p>The comparison operators are useful when trying to filter certain elements based on a condition. Imagine that we have an array of random numbers from <kbd>0</kbd> to <kbd>1</kbd>, and we want to extract all the numbers greater than <kbd>0.5</kbd>. We can use the <kbd>&gt;</kbd> operator on the array&nbsp;to obtain a <kbd>bool</kbd>&nbsp;array, as follows:</p>
<pre><code class="lang-python">    a = np.random.rand(5, 3) 
    a &gt; 0.3 
    # Result:
    # array([[ True, False,  True],
    #        [ True,  True,  True],
    #        [False,  True,  True],
    #        [ True,  True, False],
    #        [ True,  True, False]], dtype=bool) 
</code></pre>
<p>The resulting <kbd>bool</kbd>&nbsp;array can then be reused as an index to retrieve the elements greater than <kbd>0.5</kbd>:</p>
<pre><code class="lang-python">    a[a &gt; 0.5] 
    print(a[a&gt;0.5]) 
    # Output:
    # [ 0.9755  0.5977  0.8287  0.6214  0.5669  0.9553  0.5894   
    0.7196  0.9200  0.5781  0.8281 ] 
</code></pre>
<p>NumPy also implements methods such as <kbd>ndarray.sum</kbd>, which takes the sum of all the elements on an axis. If we have an array of shape <kbd>(5, 3)</kbd>, we can use the <kbd>ndarray.sum</kbd> method to sum the elements on the first axis, the second axis, or over all the elements of the array, as illustrated in the following snippet:</p>
<pre><code class="lang-python">    a = np.random.rand(5, 3) 
    a.sum(axis=0) 
    # Result:
    # array([ 2.7454,  2.5517,  2.0303]) 

    a.sum(axis=1) 
    # Result:
    # array([ 1.7498,  1.2491,  1.8151,  1.9320,  0.5814]) 

    a.sum() # With no argument operates on flattened array 
    # Result:
    # 7.3275 
</code></pre>
<p>Note that by summing the elements over an axis, we eliminate that axis. From the&nbsp;preceding example, the sum on the axis <kbd>0</kbd> produces an array of shape&nbsp;<kbd>(3,)</kbd>, while the sum on the axis <kbd>1</kbd> produces an array of shape&nbsp;<kbd>(5,)</kbd>.</p>

<h3>Calculating the norm</h3>
<p>We can review the basic concepts illustrated in this section by calculating the <em>norm</em> of a set of coordinates. For a two-dimensional vector, the norm is defined as follows:</p>
<pre><code class="lang-python">    norm = sqrt(x**2 + y**2) 
</code></pre>
<p>Given an array of 10 coordinates (<em>x</em>, <em>y</em>), we want to find the norm of each coordinate. We can calculate the norm by taking these steps:</p>
<ol>
<li>Square the coordinates, obtaining an array&nbsp;that contains <kbd>(x**2, y**2)</kbd> elements.</li>
<li>Sum those with <kbd>numpy.sum</kbd> over the last axis.</li>
<li>Take the square root, element-wise, with <kbd>numpy.sqrt</kbd>.</li>
</ol>
<p>The final expression can be compressed in a single line:</p>
<pre><code class="lang-python">    r_i = np.random.rand(10, 2) 
    norm = np.sqrt((r_i ** 2).sum(axis=1)) 
    print(norm)
    # Output:
    # [ 0.7314  0.9050  0.5063  0.2553  0.0778   0.9143   1.3245  
    0.9486  1.010   1.0212] 
</code></pre>

<h2>Rewriting the particle simulator in NumPy</h2>
<p>In this section, we will optimize our particle simulator by rewriting some parts of it in NumPy. We found, from the profiling we did in Chapter 1, <em>Benchmarking and Profiling</em>, that the slowest part of our program is the following loop contained in the <kbd>ParticleSimulator.evolve</kbd> method:</p>
<pre><code class="lang-python">    for i in range(nsteps): 
      for p in self.particles: 

        norm = (p.x**2 + p.y**2)**0.5 
        v_x = (-p.y)/norm 
        v_y = p.x/norm 

        d_x = timestep * p.ang_vel * v_x 
        d_y = timestep * p.ang_vel * v_y 

        p.x += d_x 
        p.y += d_y 
</code></pre>
<p>You&nbsp;may have noticed that the body of the loop acts solely on the current particle. If we had an array containing the particle positions and angular speed, we could rewrite the loop using a broadcasted operation. In contrast, the loop's steps depend on the previous step and cannot be parallelized in this way.</p>
<p>It is natural then, to store all the array coordinates in an array of shape <kbd>(nparticles, 2)</kbd> and the angular speed in an array of shape <kbd>(nparticles,)</kbd>, where <kbd>nparticles</kbd> is the number of particles. We'll call those arrays <kbd>r_i</kbd> and <kbd>ang_vel_i</kbd>:</p>
<pre><code class="lang-python">    r_i = np.array([[p.x, p.y] for p in self.particles]) 
    ang_vel_i = np.array([p.ang_vel for p in self.particles]) 
</code></pre>
<p>The velocity direction, perpendicular to the vector (<em>x</em>, <em>y</em>), was defined as follows:</p>
<pre><code class="lang-python">    v_x = -y / norm 
    v_y = x / norm 
</code></pre>
<p>The norm can be calculated using the strategy illustrated in the <em>Calculating the norm</em> section under the <em>Getting started with NumPy</em> heading:</p>
<pre><code class="lang-python">    norm_i = ((r_i ** 2).sum(axis=1))**0.5 
</code></pre>
<p>For the (<em>-y</em>, <em>x</em>) components, we first need to swap the x and y columns in <kbd>r_i</kbd> and then multiply the first column by -1, as shown in the following code:</p>
<pre><code class="lang-python">    v_i = r_i[:, [1, 0]] / norm_i 
    v_i[:, 0] *= -1 
</code></pre>
<p>To calculate the displacement, we need to compute the product of <kbd>v_i</kbd>, <kbd>ang_vel_i</kbd>, and <kbd>timestep</kbd>. Since <kbd>ang_vel_i</kbd> is of shape <kbd>(nparticles,)</kbd>, it needs a new axis in order to operate with <kbd>v_i</kbd> of shape <kbd>(nparticles, 2)</kbd>. We will do that using <kbd>numpy.newaxis</kbd>, as follows:</p>
<pre><code class="lang-python">    d_i = timestep * ang_vel_i[:, np.newaxis] * v_i 
    r_i += d_i 
</code></pre>
<p>Outside the loop, we have to update the particle instances with the new coordinates, <em>x</em> and <em>y</em>, as follows:</p>
<pre><code class="lang-python">    for i, p in enumerate(self.particles): 
      p.x, p.y = r_i[i] 
</code></pre>
<p>To summarize, we will implement a method called <kbd>ParticleSimulator.evolve_numpy</kbd> and benchmark it against the pure Python version, renamed as <kbd>ParticleSimulator.evolve_python</kbd>:</p>
<pre><code class="lang-python">    def evolve_numpy(self, dt): 
      timestep = 0.00001 
      nsteps = int(dt/timestep) 

      r_i = np.array([[p.x, p.y] for p in self.particles]) 
      ang_vel_i = np.array([p.ang_vel for p in self.particles]) 

      for i in range(nsteps): 

        norm_i = np.sqrt((r_i ** 2).sum(axis=1)) 
        v_i = r_i[:, [1, 0]] 
        v_i[:, 0] *= -1 
        v_i /= norm_i[:, np.newaxis] 
        d_i = timestep * ang_vel_i[:, np.newaxis] * v_i 
        r_i += d_i 

        for i, p in enumerate(self.particles): 
          p.x, p.y = r_i[i] 
</code></pre>
<p>We also update the benchmark to conveniently change the number of particles and the simulation method, as follows:</p>
<pre><code class="lang-python">    def benchmark(npart=100, method='python'): 
      particles = [Particle(uniform(-1.0, 1.0),     
                            uniform(-1.0, 1.0),
                            uniform(-1.0, 1.0))  
                            for i in range(npart)] 

      simulator = ParticleSimulator(particles) 

      if method=='python': 
        simulator.evolve_python(0.1) 

      elif method == 'numpy': 
        simulator.evolve_numpy(0.1) 
</code></pre>
<p>Let's run the benchmark in an IPython session:</p>
<pre><code class="lang-python">    from simul import benchmark 
    %timeit benchmark(100, 'python') 
    1 loops, best of 3: 614 ms per loop 
    %timeit benchmark(100, 'numpy') 
    1 loops, best of 3: 415 ms per loop 
</code></pre>
<p>We have some improvement, but it doesn't look like a huge speed boost. The power of NumPy is revealed when handling big arrays. If we increase the number of particles, we will note a more significant performance boost:</p>
<pre><code class="lang-python">    %timeit benchmark(1000, 'python') 
    1 loops, best of 3: 6.13 s per loop 
    %timeit benchmark(1000, 'numpy') 
    1 loops, best of 3: 852 ms per loop 
</code></pre>
<p>The plot in the following figure was produced by running the benchmark with different particle numbers:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000034.png" class="lazyload" /></p>
</div>
<p>The plot shows that both the implementations scale linearly with particle size, but the runtime in the pure Python version grows much faster than the NumPy version; at greater sizes, we have a greater NumPy advantage. In general, when using NumPy, you should try to pack things into large arrays and group the calculations using the broadcasting feature.</p>

<h2>Reaching optimal performance with numexpr</h2>
<p>When handling complex expressions, NumPy stores intermediate results in memory. David M. Cooke wrote a package called <kbd>numexpr</kbd>, which optimizes and compiles array expressions on the fly. It works by optimizing the usage of the CPU cache and by taking advantage of multiple processors.</p>
<p>Its usage is generally straightforward and is based on a single function--<kbd>numexpr.evaluate</kbd>. The function takes a string containing an array expression as its first argument. The syntax is basically identical to that of NumPy. For example, we can calculate a simple <kbd>a + b * c</kbd> expression in the following way:</p>
<pre><code class="lang-python">    a = np.random.rand(10000) 
    b = np.random.rand(10000) 
    c = np.random.rand(10000) 
    d = ne.evaluate('a + b * c') 
</code></pre>
<p>The <kbd>numexpr</kbd> package increases the performances in almost all cases, but to get&nbsp;a substantial advantage, you should use it with large arrays. An application that involves a large array is the calculation of a <em>distance matrix</em>. In a particle system, a distance matrix contains all the possible distances between the particles. To calculate it, we should first calculate all the vectors connecting any two particles <kbd>(i,j)</kbd>, as follows:</p>
<pre><code class="lang-python">    x_ij = x_j - x_i 
    y_ij = y_j - y_i. 
</code></pre>
<p>Then, we calculate the length of this vector by taking its norm, as in the following code:</p>
<pre><code class="lang-python">    d_ij = sqrt(x_ij**2 + y_ij**2) 
</code></pre>
<p>We can write this in NumPy by employing the usual broadcasting rules (the operation is similar to the outer product):</p>
<pre><code class="lang-python">    r = np.random.rand(10000, 2) 
    r_i = r[:, np.newaxis] 
    r_j = r[np.newaxis, :] 
    d_ij = r_j - r_i 
</code></pre>
<p>Finally, we calculate the norm over the last axis using the following line of code:</p>
<pre><code class="lang-python">    d_ij = np.sqrt((d_ij ** 2).sum(axis=2)) 
</code></pre>
<p>Rewriting the same expression using the <kbd>numexpr</kbd> syntax is extremely easy. The <kbd>numexpr</kbd> package doesn't support slicing in its array expression; therefore, we first need to prepare the operands for broadcasting by adding an extra dimension, as follows:</p>
<pre><code class="lang-python">    r = np.random(10000, 2) 
    r_i = r[:, np.newaxis] 
    r_j = r[np.newaxis, :] 
</code></pre>
<p>At that point, we should try to pack as many operations as possible in a single expression to allow a significant optimization.</p>
<p>Most of the NumPy mathematical functions are also available in <kbd>numexpr</kbd>.&nbsp;However, there is a limitation--the reduction operations (the ones&nbsp;that reduce an axis, such as sum) have to happen last. Therefore,&nbsp;we have to first calculate the sum, then step out of <kbd>numexpr</kbd>, and finally calculate the square root in another expression:</p>
<pre><code class="lang-python">    d_ij = ne.evaluate('sum((r_j - r_i)**2, 2)') 
    d_ij = ne.evaluate('sqrt(d_ij)') 
</code></pre>
<p>The <kbd>numexpr</kbd> compiler will avoid redundant memory allocation&nbsp;by not storing&nbsp;intermediate results. When possible, it&nbsp;will also distribute&nbsp;the operations over multiple processors. In the <kbd>distance_matrix.py</kbd> file, you will find two functions that implement the two versions: <kbd>distance_matrix_numpy</kbd> and <kbd>distance_matrix_numexpr</kbd>:</p>
<pre><code class="lang-python">    from distance_matrix import (distance_matrix_numpy, 
                                 distance_matrix_numexpr) 
    %timeit distance_matrix_numpy(10000) 
    1 loops, best of 3: 3.56 s per loop 
    %timeit distance_matrix_numexpr(10000) 
    1 loops, best of 3: 858 ms per loop 
</code></pre>
<p>By simply converting&nbsp;the expressions to use&nbsp;<kbd>numexpr</kbd>, we were able to obtain a 4.5x increase in performance over standard NumPy. The <kbd>numexpr</kbd> package can be used every time you need to optimize a NumPy expression that involves large arrays and complex operations, and you can do so with minimal changes in the code.</p>

<h2>Pandas</h2>
<p>Pandas is a library originally developed by Wes McKinney, which was designed to analyze datasets in a seamless and performant way. In recent years, this powerful library has seen&nbsp;an incredible growth and huge adoption by the Python community. In this section, we will introduce the main concepts and tools provided in this library, and we will use it to increase performance of various usecases that can't otherwise be addressed with&nbsp;NumPy's vectorized operations and broadcasting.</p>

<h3>Pandas fundamentals</h3>
<p>While NumPy deals mostly with arrays, Pandas main data structures are <kbd>pandas.Series</kbd>, <kbd>pandas.DataFrame</kbd>, and <kbd>pandas.Panel</kbd>. In the rest of this chapter, we will abbreviate <kbd>pandas</kbd> with <kbd>pd</kbd>.</p>
<p>The main difference between a <kbd>pd.Series</kbd> object and an <kbd>np.array</kbd> is that a&nbsp;<kbd>pd.Series</kbd> object associates a specific <em>key</em> to each element of an array. Let&rsquo;s see how this works in practice with an example.</p>
<p>Let's assume that we are trying to test a new blood pressure drug, and we want to store, for each patient,&nbsp;whether the patient's blood pressure improved after administering the drug. We can encode this information by associating to each subject ID (represented by&nbsp;an integer), &nbsp;<kbd>True</kbd>&nbsp;if the drug was effective, and <kbd>False</kbd> otherwise.</p>
<p>We can create a <kbd>pd.Series</kbd> object by associating an&nbsp;array of keys, the patients, to the array of values that represent the drug effectiveness. The array of keys can be passed to the <kbd>Series</kbd> constructor using the <kbd>index</kbd> argument, as shown in the following snippet:</p>
<pre><code class="lang-python">    import pandas as pd
    patients = [0, 1, 2, 3]
    effective = [True, True, False, False]

    effective_series = pd.Series(effective, index=patients)
</code></pre>
<p>Associating a set of integers from 0 to <em>N</em> to a set of values can technically be implemented with <kbd>np.array</kbd>, since, in this case, the key will simply be the position of the element in the array. In Pandas, keys are not limited to integers but can also be strings, floating point numbers, and also generic (hashable) Python objects. For example, we can easily turn our IDs into strings with little effort, as shown in the following code:</p>
<pre><code class="lang-python">    patients = ["a", "b", "c", "d"]
    effective = [True, True, False, False]

    effective_series = pd.Series(effective, index=patients)
</code></pre>
<p>An interesting observation is that, while NumPy arrays can be thought of as a contiguous collection of values similar to Python lists, the Pandas <kbd>pd.Series</kbd> object can be thought of as a structure that maps keys to values, similar to Python dictionaries.</p>
<p>What if you want to store the initial and final blood pressure for each patient? In Pandas, one can use a <kbd>pd.DataFrame</kbd> object to associate&nbsp;multiple data to each key.</p>
<p><kbd>pd.DataFrame</kbd> can be initialized, similarly to a <kbd>pd.Series</kbd> object, by passing a dictionary of columns and an index. In the following example, we will&nbsp;see how to create a <kbd>pd.DataFrame</kbd> containing four columns that represent the initial and final measurements of systolic and dyastolic blood pressure for our patients:</p>
<pre><code class="lang-python">    patients = ["a", "b", "c", "d"]

    columns = {
      "sys_initial": [120, 126, 130, 115],
      "dia_initial": [75, 85, 90, 87],
      "sys_final": [115, 123, 130, 118],
      "dia_final": [70, 82, 92, 87]
    }
    
    df = pd.DataFrame(columns, index=patients)
</code></pre>
<p>Equivalently, you can think of a <kbd>pd.DataFrame</kbd> as a collection of <kbd>pd.Series</kbd>. In fact, it is possible to directly initialize a <kbd>pd.DataFrame</kbd>, using a dictionary of <kbd>pd.Series</kbd> instances:</p>
<pre><code class="lang-python">    columns = {
      "sys_initial": pd.Series([120, 126, 130, 115], index=patients),
      "dia_initial": pd.Series([75, 85, 90, 87], index=patients),
      "sys_final": pd.Series([115, 123, 130, 118], index=patients),
      "dia_final": pd.Series([70, 82, 92, 87], index=patients)
    }
    df = pd.DataFrame(columns)
</code></pre>
<p>To inspect the content of a <kbd>pd.DataFrame</kbd> or <kbd>pd.Series</kbd> object, you can use the <kbd>pd.Series.head</kbd> and <kbd>pd.DataFrame.head</kbd> methods, which print&nbsp;the first few rows of the dataset:</p>
<pre><code class="lang-python">    effective_series.head()
    # Output:
    # a True
    # b True
    # c False
    # d False
    # dtype: bool

    df.head()
    # Output:
    #    dia_final  dia_initial  sys_final  sys_initial
    # a         70           75        115          120
    # b         82           85        123          126
    # c         92           90        130          130
    # d         87           87        118          115
</code></pre>
<p>Just like a <kbd>pd.DataFrame</kbd>&nbsp;can be used to store a collection of <kbd>pd.Series</kbd>, you can use a <kbd>pd.Panel</kbd> to store a collection of <kbd>pd.DataFrames</kbd>. We will not cover the usage of <kbd>pd.Panel</kbd> as&nbsp;it is&nbsp;not used as often as <kbd>pd.Series</kbd> and <kbd>pd.DataFrame</kbd>. To learn more about <kbd>pd.Panel</kbd>, ensure that you refer to the excellent documentation at <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel</a>.</p>

<h3>Indexing Series and DataFrame objects</h3>
<p>Retrieving data from a <kbd>pd.Series</kbd>, given its <em>key</em>, can be done intuitively by indexing&nbsp;the <kbd>pd.Series.loc</kbd> attribute:</p>
<pre><code class="lang-python">    effective_series.loc["a"]
    # Result:
    # True
</code></pre>
<p>It is also possible to access the elements, given its <em>position</em> in the underlying array, using the <kbd>pd.Series.iloc</kbd> attribute:</p>
<pre><code class="lang-python">    effective_series.iloc[0]
    # Result:
    # True
</code></pre>
<p>You can also use the <kbd>pd.Series.ix</kbd> attribute for mixed access.&nbsp;If the key is not an integer, it will try to match by key, otherwise it will extract the element at the position indicated by the integer. A similar behavior will&nbsp;take place&nbsp;when you access the <kbd>pd.Series</kbd> directly. The following example demonstrates these concepts:</p>
<pre><code class="lang-python">    effective_series.ix["a"] # By key
    effective_series.ix[0]   # By position

    # Equivalent
    effective_series["a"] # By key
    effective_series[0]   # By position
</code></pre>
<p>Note that if the index is made of integers, this method will fall back to the key-only method (like <kbd>loc</kbd>). To index by position in this scenario, the <kbd>iloc</kbd> method is your only option.</p>
<p>Indexing <kbd>pd.DataFrame</kbd>&nbsp;works in a similar way. For example, you can use <kbd>pd.DataFrame.loc</kbd> to extract a row by key, and you can use <kbd>pd.DataFrame.iloc</kbd> to extract a row by position:</p>
<pre><code class="lang-python">    df.loc["a"]
    df.iloc[0]
    # Result:
    # dia_final 70
    # dia_initial 75
    # sys_final 115
    # sys_initial 120
    # Name: a, dtype: int64
</code></pre>
<p>An important aspect is that the return type in this case is a <kbd>pd.Series</kbd>, where each column is a new key. In order to retrieve a specific row and column, you can use the following code. The <kbd>loc</kbd> attribute will index both row and column by key, while the <kbd>iloc</kbd> version will index row and column by an integer:</p>
<pre><code class="lang-python">    df.loc["a", "sys_initial"] # is equivalent to
    df.loc["a"].loc["sys_initial"]

    df.iloc[0, 1] # is equivalent to
    df.iloc[0].iloc[1]
</code></pre>
<p>Indexing a <kbd>pd.DataFrame</kbd> using the <kbd>ix</kbd> attribute is convenient&nbsp;to mix and match index and location-based indexing. For example, retrieving the <kbd>"sys_initial"</kbd> column for the row at position 0 can be accomplished&nbsp;as follows:</p>
<pre><code class="lang-python">    df.ix[0, "sys_initial"] 
</code></pre>
<p>Retrieving&nbsp;a column from a <kbd>pd.DataFrame</kbd> by name can be achieved by regular indexing or attribute access. &nbsp;To retrieve a column by position, you can either use <kbd>iloc</kbd> or use the <kbd>pd.DataFrame.column</kbd>&nbsp;attribute to retrieve the name of the column:</p>
<pre><code class="lang-python">    # Retrieve column by name
    df["sys_initial"] # Equivalent to
    df.sys_initial

    # Retrieve column by position
    df[df.columns[2]] # Equivalent to
    df.iloc[:, 2]
</code></pre>
<p>The mentioned methods&nbsp;also support more advanced indexing similar to those of NumPy, such as <kbd>bool</kbd>, lists, and <kbd>int</kbd> arrays.</p>
<p>Now it's time for some performance considerations. There are some differences between an index in Pandas and a dictionary. For example, while the keys of a dictionary cannot contain duplicates, Pandas indexes can contain repeated&nbsp;elements. This flexibility, however, comes at a cost--if we try to access an element in a non-unique index, we may incur substantial&nbsp;performance loss--the access will be <em>O</em>(<em>N</em>), like a linear search, rather than <em>O</em>(1), like a dictionary.</p>
<p>A way to mitigate this effect is to sort the index; this will allow Pandas to use a binary search algorithm with a&nbsp;computational&nbsp;complexity of <em>O</em>(<em>log</em>(<em>N</em>)), which is much better. This can be accomplished using the <kbd>pd.Series.sort_index</kbd> function, as in the following code (the same applies for <kbd>pd.DataFrame</kbd>):</p>
<pre><code class="lang-python">    # Create a series with duplicate index
    index = list(range(1000)) + list(range(1000))

    # Accessing a normal series is a O(N) operation
    series = pd.Series(range(2000), index=index)

    # Sorting the will improve look-up scaling to O(log(N))
    series.sort_index(inplace=True)
</code></pre>
<p>The timings for the different versions are summarized in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Index type</strong></td>
<td><strong>N=10000</strong></td>
<td><strong>N=20000</strong></td>
<td><strong>N=30000</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td>Unique</td>
<td>12.30</td>
<td>12.58</td>
<td>13.30</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td>Non unique</td>
<td>494.95</td>
<td>814.10</td>
<td>1129.95</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td>Non unique (sorted)</td>
<td>145.93</td>
<td>145.81</td>
<td>145.66</td>
<td><em>O</em>(<em>log</em>(<em>N</em>))</td>
</tr>
</tbody>
</table>

<h3>Database-style operations with Pandas</h3>
<p>You may have noted that the &ldquo;tabular&rdquo; data is similar to what is usually stored in a database. A database is usually indexed using a primary key, and the various columns can have different data types, just like in a <kbd>pd.DataFrame</kbd>.&nbsp;</p>
<p>The efficiency of the index operations in&nbsp;Pandas makes it suitable for database style manipulations, such as counting, joining, grouping, and aggregations.</p>

<h3>Mapping</h3>
<p>Pandas supports&nbsp;element-wise operations just like NumPy (after all, <kbd>pd.Series</kbd> stores their data using&nbsp;<kbd>np.array</kbd>). For example, it is possible to apply transformation very easily on both <kbd>pd.Series</kbd> and <kbd>pd.DataFrame</kbd>:</p>
<pre><code class="lang-python">    np.log(df.sys_initial) # Logarithm of a series
    df.sys_initial ** 2    # Square a series
    np.log(df)             # Logarithm of a dataframe
    df ** 2                # Square of a dataframe
</code></pre>
<p>You can also perform element-wise operations between two <kbd>pd.Series</kbd> in a way similar to NumPy. An important difference is&nbsp;that&nbsp;the operands will be matched by key, rather than by position; if there is a mismatch in the index, the resulting value will be set to&nbsp;<kbd>NaN</kbd>. Both the scenarios are&nbsp;exemplified in the following example:</p>
<pre><code class="lang-python">    # Matching index
    a = pd.Series([1, 2, 3], index=["a", "b", "c"])
    b = pd.Series([4, 5, 6], index=["a", "b", "c"])
    a + b
    # Result: 
    # a 5
    # b 7
    # c 9
    # dtype: int64

    # Mismatching index
    b = pd.Series([4, 5, 6], index=["a", "b", "d"])
    # Result:
    # a 5.0
    # b 7.0
    # c NaN
    # d NaN
    # dtype: float64
</code></pre>
<p>For added flexibility, Pandas exposes the <kbd>map</kbd>, <kbd>apply</kbd>, and <kbd>applymap</kbd> methods&nbsp;that can be used to apply specific transformations.</p>
<p>The <kbd>pd.Series.map</kbd> method can be used to execute a function to each value and return a <kbd>pd.Series</kbd> containing each result. In the following example, we show how to apply the <kbd>superstar</kbd>&nbsp;function to each element of a&nbsp;<kbd>pd.Series</kbd>:</p>
<pre><code class="lang-python">    a = pd.Series([1, 2, 3], index=["a", "b", "c"])
    def superstar(x):
        return '*' + str(x) + '*'
    a.map(superstar)

    # Result:
    # a *1*
    # b *2*
    # c *3*
    # dtype: object
</code></pre>
<p>The <kbd>pd.DataFrame.applymap</kbd> function is the equivalent of <kbd>pd.Series.map</kbd>, but for <kbd>DataFrames</kbd>:</p>
<pre><code class="lang-python">    df.applymap(superstar)
    # Result:
    #    dia_final  dia_initial  sys_final  sys_initial
    # a       *70*         *75*      *115*        *120*
    # b       *82*         *85*      *123*        *126*
    # c       *92*         *90*      *130*        *130*
    # d       *87*         *87*      *118*        *115*
</code></pre>
<p>Finally, the <kbd>pd.DataFrame.apply</kbd> function can apply the passed function to each column or&nbsp;each row, rather than element-wise. The selection can be performed with the argument axis, where a value of <kbd>0</kbd> (the default) corresponds to columns, and <kbd>1</kbd> corresponds to rows. Also, note&nbsp;that the return value of <kbd>apply</kbd> is a <kbd>pd.Series</kbd>:</p>
<pre><code class="lang-python">    df.apply(superstar, axis=0)
    # Result:
    # dia_final *a 70nb 82nc 92nd 87nName: dia...
    # dia_initial *a 75nb 85nc 90nd 87nName: dia...
    # sys_final *a 115nb 123nc 130nd 118nName:...
    # sys_initial *a 120nb 126nc 130nd 115nName:...
    # dtype: object

    df.apply(superstar, axis=1)
    # Result:
    # a *dia_final 70ndia_initial 75nsys_f...
    # b *dia_final 82ndia_initial 85nsys_f...
    # c *dia_final 92ndia_initial 90nsys_f...
    # d *dia_final 87ndia_initial 87nsys_f...
    # dtype: object
</code></pre>
<p>Pandas also supports efficient&nbsp;<kbd>numexpr</kbd>-style expressions with the convenient&nbsp;<kbd>eval</kbd> method. For example, if we want to calculate the difference in the final and initial blood pressure, we can write the expression as a string, as shown in the following code:</p>
<pre><code class="lang-python">    df.eval("sys_final - sys_initial")
    # Result:
    # a -5
    # b -3
    # c 0
    # d 3
    # dtype: int64


</code></pre>
<p>It is also possible to create new columns&nbsp;using the assignment operator in the <kbd>pd.DataFrame.eval</kbd>&nbsp;expression. Note that, if the&nbsp;<kbd>inplace=True</kbd> argument is used, the operation will be applied directly on the original <kbd>pd.DataFrame</kbd>; otherwise, the function will return a new dataframe. In the next example, we compute the difference between <kbd>sys_final</kbd> and <kbd>sys_initial</kbd>, and we store it in the <kbd>sys_delta</kbd> column:</p>
<pre><code class="lang-python">df.eval("sys_delta = sys_final - sys_initial", inplace=False)
# Result:
#     dia_final   dia_initial   sys_final   sys_initial   sys_delta
# a          70            75         115           120          -5
# b          82            85         123           126          -3
# c          92            90         130           130           0
# d          87            87         118           115           3
</code></pre>

<h3>Grouping, aggregations, and transforms</h3>
<p>One of the most appreciated&nbsp;features of Pandas is the simple and concise expression of data analysis pipelines that requires grouping, transforming, and aggregating the data. To demonstrate this concept, let's extend our dataset by adding two new patients to whom we didn't administer the treatment (this is usually called a&nbsp;<em>control group</em>). We also include a column, <kbd>drug_admst</kbd>, which&nbsp;records whether&nbsp;the patient was administered the treatment:</p>
<pre><code class="lang-python">    patients = ["a", "b", "c", "d", "e", "f"]

    columns = {
      "sys_initial": [120, 126, 130, 115, 150, 117],
      "dia_initial": [75, 85, 90, 87, 90, 74],
      "sys_final": [115, 123, 130, 118, 130, 121],
      "dia_final": [70, 82, 92, 87, 85, 74],
      "drug_admst": [True, True, True, True, False, False]
    }

    df = pd.DataFrame(columns, index=patients)
</code></pre>
<p>At this point, we may be interested to know how the blood pressure changed between the&nbsp;two groups. You can group the patients according to <kbd>drug_amst</kbd>&nbsp;using the <kbd>pd.DataFrame.groupby</kbd> function. The return value will be the <kbd>DataFrameGroupBy</kbd> object, which can be iterated to obtain a new <kbd>pd.DataFrame</kbd> for each value of the <kbd>drug_admst</kbd> column:</p>
<pre><code class="lang-python">    df.groupby('drug_admst')
    for value, group in df.groupby('drug_admst'):
        print("Value: {}".format(value))
        print("Group DataFrame:")
        print(group)
# Output:
# Value: False
# Group DataFrame:
#    dia_final   dia_initial   drug_admst   sys_final   sys_initial
# e         85            90        False         130           150
# f         74            74        False         121           117
# Value: True
# Group DataFrame:
#    dia_final   dia_initial   drug_admst   sys_final   sys_initial
# a         70            75         True         115           120
# b         82            85         True         123           126
# c         92            90         True         130           130
# d         87            87         True         118           115
</code></pre>
<p>Iterating on the&nbsp;<kbd>DataFrameGroupBy</kbd> object is almost never necessary, because, thanks to method chaining, it is possible to calculate group-related properties&nbsp;directly. For example, we may want to calculate mean, max, or standard deviation for each group. All those operations that summarize the data in some way&nbsp;are called aggregations and can be performed using the <kbd>agg</kbd> method. The result of <kbd>agg</kbd> is another <kbd>pd.DataFrame</kbd>&nbsp;that relates the grouping variables and the result of the aggregation, as illustrated in the following code:</p>
<pre><code class="lang-python">df.groupby('drug_admst').agg(np.mean)
#              dia_final   dia_initial   sys_final   sys_initial
# drug_admst 
# False            79.50         82.00       125.5        133.50
# True             82.75         84.25       121.5        122.75
</code></pre>
<p><strong>It&nbsp;is also possible to perform processing on the DataFrame groups that do not represent a summarization. One common example of such an operation is filling in missing values. Those intermediate steps are called <em>transforms</em>.</strong></p>
<p>We can illustrate this concept with an example. Let's assume that we have a few missing values in our dataset, and we want to replace those values with the average of the other values in the same group. This can be accomplished using a transform, as follows:</p>
<pre><code class="lang-python">df.loc['a','sys_initial'] = None
df.groupby('drug_admst').transform(lambda df: df.fillna(df.mean())) 
#     dia_final    dia_initial   sys_final   sys_initial
# a          70             75         115    123.666667
# b          82             85         123    126.000000
# c          92             90         130    130.000000
# d          87             87         118    115.000000
# e          85             90         130    150.000000
# f          74             74         121    117.000000
</code></pre>

<h3>Joining</h3>
<p>Joins are useful to aggregate&nbsp;data that is scattered among different tables. Let&rsquo;s say that we want to include the location of the&nbsp;hospital in which patient measurements were taken in our dataset. We can reference&nbsp;the location for each patient using the <kbd>H1</kbd>, <kbd>H2</kbd>, and <kbd>H3</kbd> labels,&nbsp;and we can store&nbsp;the address and identifier of the hospital in a <kbd>hospital</kbd> table:</p>
<pre><code class="lang-python">    hospitals = pd.DataFrame(
      { "name" : ["City 1", "City 2", "City 3"],
        "address" : ["Address 1", "Address 2", "Address 3"],
        "city": ["City 1", "City 2", "City 3"] },
      index=["H1", "H2", "H3"])

    hospital_id = ["H1", "H2", "H2", "H3", "H3", "H3"]
    df['hospital_id'] = hospital_id
</code></pre>
<p>Now, we want to find&nbsp;the city where the measure was taken for each patient. We need to <em>map</em> the keys&nbsp;from the&nbsp;<kbd>hospital_id</kbd>&nbsp;column to the city stored in the <kbd>hospitals</kbd> table.</p>
<p>This can surely&nbsp;be implemented in Python using dictionaries:</p>
<pre><code class="lang-python">    hospital_dict = {
     "H1": ("City 1", "Name 1", "Address 1"),
     "H2": ("City 2", "Name 2", "Address 2"),
     "H3": ("City 3", "Name 3", "Address 3")
    }
    cities = [hospital_dict[key][0] 
               for key in hospital_id]
</code></pre>
<p>This algorithm runs efficiently with an&nbsp;<em>O</em>(<em>N</em>) time complexity, where <em>N</em> is the size of <kbd>hospital_id</kbd>.&nbsp;Pandas allows you to encode the same operation using simple indexing; the advantage is that the join will be performed in heavily optimized Cython and with efficient hashing algorithms. The preceding simple Python expression can be easily converted to Pandas in this way:</p>
<pre><code class="lang-python">    cities = hospitals.loc[hospital_id, "city"]
</code></pre>
<p>More advanced joins can also be performed with the <kbd>pd.DataFrame.join</kbd> method, which&nbsp;will produce a new <kbd>pd.DataFrame</kbd>&nbsp;that will attach the hospital information for each patient:</p>
<pre><code class="lang-python">    result = df.join(hospitals, on='hospital_id')
    result.columns
    # Result:
    # Index(['dia_final', 'dia_initial', 'drug_admst', 
    # 'sys_final', 'sys_initial',
    # 'hospital_id', 'address', 'city', 'name'],
    # dtype='object')
</code></pre>

<h2>Summary</h2>
<p>In this chapter, we learned how to manipulate NumPy arrays and how to write fast mathematical expressions using array broadcasting. This knowledge will help you write more concise, expressive code and, at the same time, to obtain&nbsp;substantial performance gains. We also introduced the <kbd>numexpr</kbd> library to further speed up NumPy&nbsp;calculations with minimal effort.</p>
<p>Pandas implements efficient data structures that are useful when analyzing large datasets. In particular, Pandas shines when the data is indexed by non-integer keys and provides very fast hashing algorithms.</p>
<p>NumPy and Pandas work well when handling large, homogenous inputs, but they are not suitable when the expressions grow complex and the operations cannot be expressed using the tools provided by these libraries.&nbsp;In such cases, we can leverage Python capabilities as a glue language by interfacing it with C using the Cython package.</p>

</div>



<!--Chapter 4-->


<div class="chapter" data-chapter-number="4">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 4 </span></div>
<h1 class="chaptertitle">C Performance with Cython</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>Cython is a language that extends Python by supporting the&nbsp;declaration of types for functions, variables, and classes.&nbsp;These typed declarations enable Cython to compile Python scripts to efficient C code. Cython can also&nbsp;act as a bridge between Python and C as it provides easy-to-use constructs to write interfaces to external C and C++ routines.</p>
<p>In this chapter, we will learn the following things:</p>
<ul>
<li>Cython syntax basics</li>
<li>How to compile Cython programs</li>
<li>How to use <strong>static typing</strong> to generate fast code</li>
<li>How to efficiently manipulate arrays using&nbsp;typed <strong>memoryviews</strong></li>
<li>Optimizing a sample particle simulator</li>
<li>Tips on using Cython in the Jupyter notebook</li>
<li>The profiling tools available for Cython</li>
</ul>
<p>While a minimum knowledge of C is helpful, this chapter focuses only on Cython in the context of Python optimization. Therefore, it doesn't require any C background.</p>

<h2>Compiling Cython extensions</h2>
<p>The Cython syntax is, by design, a superset of Python.&nbsp;Cython can compile, with a few exceptions, most Python modules without requiring any change. Cython source files have the&nbsp;<kbd>.pyx</kbd> extension and can be compiled to produce a C file using the <kbd>cython</kbd> command.</p>
<p>Our first Cython script will contain a simple function that prints <kbd>Hello, World!</kbd>&nbsp;as the output. Create a new <kbd>hello.pyx</kbd> file containing the following code:</p>
<pre><code class="lang-python">    def hello(): 
      print('Hello, World!') 
</code></pre>
<p>The <kbd>cython</kbd> command will read <kbd>hello.pyx</kbd> and generate the <kbd>hello.c</kbd> file:</p>
<pre><code class="lang-python">$ cython hello.pyx
</code></pre>
<p>To compile <kbd>hello.c</kbd> to a Python extension module, we will use the GCC&nbsp;compiler. We need to add some Python-specific compilation options that depend on the operating system. It's important to specify the directory that contains the header files; in the following example, the directory is <kbd>/usr/include/python3.5/</kbd>:</p>
<pre><code class="lang-python">$ gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -fno-strict-aliasing -lm -I/usr/include/python3.5/ -o hello.so hello.c
</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>To find your Python include directory, you can use the <kbd>distutils</kbd> utility:&nbsp; <kbd>sysconfig.get_python_inc</kbd>. To execute it, you can simply issue the following&nbsp;<kbd>python -c "from distutils import sysconfig; print(sysconfig.get_python_inc())"</kbd>&nbsp;command.</p>
</div>
</div>
<p>This will produce a file called <kbd>hello.so</kbd>, a C extension module that is directly importable into a Python session:</p>
<pre><code class="lang-python">    &gt;&gt;&gt; import hello 
    &gt;&gt;&gt; hello.hello() 
    Hello, World!
</code></pre>
<p>Cython accepts both Python 2 and Python 3 as input and output languages. In other words, you can compile a Python 3 script&nbsp;<kbd>hello.pyx</kbd> file using the <kbd>-3</kbd> option:</p>
<pre><code class="lang-python">$ cython -3 hello.pyx
</code></pre>
<p>The generated <kbd>hello.c</kbd> can be compiled without any changes to Python 2 and Python 3 by including the corresponding headers with the <kbd>-I</kbd> option, as follows:</p>
<pre><code class="lang-python">$ gcc -I/usr/include/python3.5 # ... other options
$ gcc -I/usr/include/python2.7 # ... other options
</code></pre>
<p>A Cython program can be compiled in a more straightforward way using <kbd>distutils</kbd>, the standard Python packaging tool. By writing a <kbd>setup.py</kbd> script, we can compile the <kbd>.pyx</kbd> file directly to an extension module. To compile our <kbd>hello.pyx</kbd> example, we can&nbsp;write a minimal&nbsp;<kbd>setup.py</kbd> containing the following code:</p>
<pre><code class="lang-python">    from distutils.core import setup 
    from Cython.Build import cythonize 

    setup( 
      name='Hello',
      ext_modules = cythonize('hello.pyx')
    ) 
</code></pre>
<p>In the first two lines of the preceding&nbsp;code, we import the <kbd>setup</kbd> function and the <kbd>cythonize</kbd> helper. The <kbd>setup</kbd> function contains a few key-value pairs that specify&nbsp;the name of the application and the&nbsp;extensions that need to be built.</p>
<p>The <kbd>cythonize</kbd> helper takes either a string or a list of strings containing the Cython modules we want to compile. You can also use glob patterns using the following code:</p>
<pre><code class="lang-python">    cythonize(['hello.pyx', 'world.pyx', '*.pyx']) 
</code></pre>
<p>To compile our extension module using <kbd>distutils</kbd>, you can execute the <kbd>setup.py</kbd> script using the following code:</p>
<pre><code class="lang-python">$ python setup.py build_ext --inplace
</code></pre>
<p>The <kbd>build_ext</kbd> option tells the script to build the extension modules indicated in <kbd>ext_modules</kbd>, while&nbsp;the <kbd>--inplace</kbd> option tells the script to place&nbsp;the <kbd>hello.so</kbd> output file in the same location as the source file (instead of a build directory).</p>
<p>Cython modules can also be automatically compiled using <kbd>pyximport</kbd>. All that's needed is a call to <kbd>pyximport.install()</kbd> at the beginning of your script (or you need to issue the command in your interpreter). After doing that, you can import <kbd>.pyx</kbd> files directly and&nbsp;<kbd>pyximport</kbd> will transparently compile the corresponding Cython modules:</p>
<pre><code class="lang-python">    &gt;&gt;&gt; import pyximport 
    &gt;&gt;&gt; pyximport.install() 
    &gt;&gt;&gt; import hello # This will compile hello.pyx 
</code></pre>
<p>Unfortunately, <kbd>pyximport</kbd> will not work for all kinds of configurations (for example, when they involve a combination of C and Cython files), but it comes handy for testing simple scripts.</p>
<p>Since version 0.13, IPython includes the <kbd>cythonmagic</kbd> extension to interactively write and test a series of Cython statements. You can load the extensions in an IPython shell using <kbd>load_ext</kbd>:</p>
<pre><code class="lang-python">    %load_ext cythonmagic 
</code></pre>
<p>Once the extension is loaded, you can use the <kbd>%%cython</kbd>&nbsp;<em>cell magic</em> to write a multiline Cython snippet. In the following example, we define a <kbd>hello_snippet</kbd> function that will be compiled and added to the IPython session&nbsp;namespace:</p>
<pre><code class="lang-python">    %%cython 
    def hello_snippet(): 
        print("Hello, Cython!") 

    hello_snippet()
    Hello,  Cython! 
</code></pre>

<h2>Adding static types</h2>
<p>In Python, a variable&nbsp;can be associated to objects of different types&nbsp;during the execution of the program. While this feature is desirable as it makes the language flexible and dynamic, it also adds a significant overhead to the interpreter as it needs to look up&nbsp;type and methods of the variables at runtime, making it difficult&nbsp;to perform various optimizations. Cython extends the Python language with explicit type declarations so that it can generate efficient C extensions through compilation.</p>
<p>The main way to declare data types in Cython is through&nbsp;<kbd>cdef</kbd> statements. The <kbd>cdef</kbd> keyword can be used in multiple contexts, such as variables, functions, and extension types (statically-typed classes).</p>

<h3>Variables</h3>
<p>In Cython, you can declare the type of a variable by prepending the variable with <kbd>cdef</kbd> and its respective type. For example, we can declare the <kbd>i</kbd> variable as a 16 bit integer in the following way:</p>
<pre><code class="lang-python">    cdef int i 
</code></pre>
<p>The <kbd>cdef</kbd> statement supports multiple variable names on the same line along with optional initialization, as seen in the following line:</p>
<pre><code class="lang-python">    cdef double a, b = 2.0, c = 3.0 
</code></pre>
<p>Typed variables are treated differently in comparison to regular&nbsp;variables. In Python, variables are often described as&nbsp;<em>labels</em>&nbsp;that refer to objects in memory. For example, we could assign the value&nbsp;<kbd>'hello'</kbd> to the <kbd>a</kbd>&nbsp;variable at any point in the program&nbsp;without restriction:</p>
<pre><code class="lang-python">    a = 'hello' 
</code></pre>
<p>The <kbd>a</kbd>&nbsp;variable holds a reference to the <kbd>'hello'</kbd>&nbsp;string. We can also freely assign another value (for example, the integer <kbd>1</kbd>) to the same variable later in the code:</p>
<pre><code class="lang-python">    a = 1 
</code></pre>
<p>Python will assign the integer <kbd>1</kbd>&nbsp;to the <kbd>a</kbd> variable without any problem.</p>
<p>Typed variables behave quite differently and are usually described as&nbsp;<em>data containers:</em>&nbsp;we can only&nbsp;store&nbsp;values that fit into the container that is determined by its data type. For example, if we declare the <kbd>a</kbd> variable as&nbsp;<kbd>int</kbd>, and then we try to assign it to a <kbd>double</kbd>, Cython will trigger an error, as shown in the following code:</p>
<pre><code class="lang-python">    %%cython 
    cdef int i 
    i = 3.0 

    # Output has been cut 
    ...cf4b.pyx:2:4 Cannot assign type 'double' to 'int' 
</code></pre>
<p>Static typing makes it easy for the compiler to perform&nbsp;useful optimizations. For example, if we declare a loop index as <kbd>int</kbd>, Cython will rewrite the loop in pure C without needing to step into the Python interpreter. The typing declaration guarantees that the type of the index will always be <kbd>int</kbd> and cannot be overwritten at runtime so that the compiler is free to perform the optimizations without compromising the program correctness.</p>
<p>We can assess the speed gain in this case with a small test case.&nbsp;In the following example, we implement a simple loop that increments a variable 100 times. With Cython, the <kbd>example</kbd> function can be coded as follows:</p>
<pre><code class="lang-python">    %%cython 
    def example(): 
       cdef int i, j=0 
       for i in range(100):
           j += 1 
       return j 

    example() 
    # Result:
    # 100 
</code></pre>
<p>We can compare the speed of an analogous, untyped, pure Python loop:</p>
<pre><code class="lang-python">    def example_python(): 
        j=0 
        for i in range(100):
            j += 1 
        return j 

    %timeit example() 
    10000000 loops, best of 3: 25 ns per loop 
    %timeit example_python() 
    100000 loops, best of 3: 2.74 us per loop 
</code></pre>
<p>The speedup obtained by implementing this simple type declaration&nbsp;is a whopping 100x! This works because the Cython loop has first been converted to pure C and then to efficient machine code, while the Python loop still relies on the slow interpreter.</p>
<p>In Cython, it is possible to declare a variable to be of any standard C type, and it is also possible&nbsp;to&nbsp;define custom types using classic C constructs, such as <kbd>struct</kbd>, <kbd>enum</kbd>, and <kbd>typedef</kbd>.</p>
<p>An interesting example is that if we declare a variable to be of the&nbsp;<kbd>object</kbd> type, the variable will accept any kind of Python object:</p>
<pre><code class="lang-python">    cdef object a_py 
    # both 'hello' and 1 are Python objects 
    a_py = 'hello' 
    a_py = 1 
</code></pre>
<p>Note that declaring a variable as <kbd>object</kbd> has no performance benefits as accessing and operating on the object will still require the interpreter to look up the underlying type of the variable and its attributes and methods.</p>
<p>Sometimes, certain data types (such as <kbd>float</kbd> and <kbd>int</kbd> numbers) are compatible in the sense that they can be converted into each other. In Cython, it is possible to convert (<em>cast</em>) between types by surrounding the destination type between pointy brackets, as shown in the following snippet:</p>
<pre><code class="lang-python">    cdef int a = 0 
    cdef double b 
    b = &lt;double&gt; a 
</code></pre>

<h3>Functions</h3>
<p>You can add type information to the arguments of a Python function by specifying the type in front of each of the argument names. Functions specified in this way will work and perform like regular Python functions, but their arguments will be type-checked. We can write a <kbd>max_python</kbd> function, which returns the greater value between two integers:</p>
<pre><code class="lang-python">    def max_python(int a, int b):
        return a if a &gt; b else b 
</code></pre>
<p>A function specified in this way will perform type-checking and treat the arguments as typed variables, just like in <kbd>cdef</kbd> definitions. However, the function will still be a Python function, and calling it multiple times will still need to switch back to the interpreter. To allow Cython for function call optimizations, we should declare the type of the return type using a <kbd>cdef</kbd> statement:</p>
<pre><code class="lang-python">    cdef int max_cython(int a, int b): 
        return a if a &gt; b else b 
</code></pre>
<p>Functions declared in this way are translated to native C functions and have much less overhead compared to Python functions. A substantial drawback is that they can't be used from Python, but only from Cython, and their scope is restricted to the same Cython file unless they're exposed in a definition file (refer to the <em>Sharing declarations</em> section).</p>
<p>Fortunately, Cython allows you to define functions that are both callable from Python and translatable to performant C functions. If you declare a function with a <kbd>cpdef</kbd> statement, Cython will generate two versions of the function: a Python version available to the interpreter, and a fast C function usable from Cython. The <kbd>cpdef</kbd> syntax is equivalent to <kbd>cdef</kbd>, shown as follows:</p>
<pre><code class="lang-python">    cpdef int max_hybrid(int a, int b): 
        return a if a &gt; b else b 
</code></pre>
<p>Sometimes, the call overhead can be a performance issue even with C functions, especially when the same function is called many times in a critical loop. When the function body is small, it is convenient to add the <kbd>inline</kbd> keyword in front of the function definition; the function call will be replaced by the function body itself. Our <kbd>max</kbd> function is a good candidate for <em>inlining</em>:</p>
<pre><code class="lang-python">    cdef inline int max_inline(int a, int b): 
        return a if a &gt; b else b 
</code></pre>

<h3>Classes</h3>
<p>We can define an extension type using the <kbd>cdef class</kbd> statement and declaring its attributes in the class body. For example, we can create an extension type--<kbd>Point</kbd>--as shown in the following code, which stores two coordinates (<em>x</em>, <em>y</em>) of the&nbsp;<kbd>double</kbd>&nbsp;type:</p>
<pre><code class="lang-python">    cdef class Point
        cdef double x 
        cdef double y
        def __init__(self, double x, double y): 
            self.x = x 
            self.y = y 
</code></pre>
<p>Accessing the declared attributes in the class methods allows Cython to bypass expensive Python attribute look-ups by direct access to the given fields in the underlying C <kbd>struct</kbd>. For this reason, attribute access in typed classes is an extremely fast operation.</p>
<p>To use the <kbd>cdef class</kbd> in your code, you need to explicitly declare the type of the variables you intend to use at compile time. You can use the extension type name (such as <kbd>Point</kbd>) in any context where you will&nbsp;use a standard type (such as <kbd>double</kbd>, <kbd>float</kbd>, and&nbsp;<kbd>int</kbd>). For example, if we want a Cython function that calculates the distance from the origin (in the example, the function is called <kbd>norm</kbd>) of a <kbd>Point</kbd>, we have to declare the input variable as <kbd>Point</kbd>, as shown in the following code:</p>
<pre><code class="lang-python">    cdef double norm(Point p): 
        return (p.x**2 + p.y**2)**0.5 
</code></pre>
<p>Just like typed functions, typed classes have some limitations. If you try to access an extension type attribute from Python, you will get an <kbd>AttributeError</kbd>, as follows:</p>
<pre><code class="lang-python">    &gt;&gt;&gt; a = Point(0.0, 0.0) 
    &gt;&gt;&gt; a.x 
    AttributeError: 'Point' object has no attribute 'x' 
</code></pre>
<p>In order to access attributes from Python code, you have to use the <kbd>public</kbd> (for read/write access) or <kbd>readonly</kbd> specifiers in the attribute declaration, as shown in the following code:</p>
<pre><code class="lang-python">    cdef class Point: 
        cdef public double x 
</code></pre>
<p>Additionally, methods can be declared with the <kbd>cpdef</kbd> statement, just like regular functions.</p>
<p>Extension types do not support the addition of extra attributes at runtime. In order to do that, a solution is defining a Python class that is a subclass of the typed class and extends its attributes and methods in pure Python.</p>

<h2>Sharing declarations</h2>
<p>When writing your Cython modules, you may want to reorganize your most used functions and classes declaration in a separate file so that they can be reused in different modules. Cython allows you to put these components in a <em>definition file</em> and access them with <kbd>cimport</kbd> statements<em>.</em></p>
<p>Let's say that we have a module with the <kbd>max</kbd> and <kbd>min</kbd>&nbsp;functions, and we want to reuse those functions in multiple Cython programs. If we simply write a bunch of functions in a <kbd>.pyx</kbd> file, the declarations will be confined to the same file.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Definition files are also used to interface Cython with external C code. The idea is to copy (or, more accurately, translate) the types and function prototypes in the definition file and leave the implementation in the external C code that will be compiled and linked in a separate step.</p>
</div>
</div>
<p>To share the&nbsp;<kbd>max</kbd> and <kbd>min</kbd> functions, we need to write a definition file with a <kbd>.pxd</kbd> extension. Such a file only contains the types and function prototypes that we want to share with&nbsp;other modules--a <em>public</em> interface. We can declare the prototypes of our <kbd>max</kbd> and <kbd>min</kbd> functions in a file named <kbd>mathlib.pxd</kbd>, as follows:</p>
<pre><code class="lang-python">    cdef int max(int a, int b) 
    cdef int min(int a, int b) 
</code></pre>
<p>As you can see, we only write the function name and arguments without implementing the function body.</p>
<p>The function implementation goes into the implementation file with the same base name but the&nbsp;<kbd>.pyx</kbd> extension--<kbd>mathlib.pyx</kbd>:</p>
<pre><code class="lang-python">    cdef int max(int a, int b): 
      return a if a &gt; b else b 

    cdef int min(int a, int b): 
      return a if a &lt; b else b 
</code></pre>
<p>The <kbd>mathlib</kbd> module is now importable from another Cython module.</p>
<p>To test our new Cython module, we will create a file named <kbd>distance.pyx</kbd> containing a function named <kbd>chebyshev</kbd>. The function will calculate the Chebyshev distance between two points, as shown in the following code. The Chebyshev distance between two coordinates--<kbd>(x1, y1)</kbd> and <kbd>(x2, y2)</kbd>--is defined as the maximum value of the difference between each coordinate:</p>
<pre><code class="lang-python">    max(abs(x1 - x2), abs(y1 - y2)) 
</code></pre>
<p>To implement the <kbd>chebyshev</kbd> function, we will use the <kbd>max</kbd> function declared in <kbd>mathlib.pxd</kbd> by importing it with the <kbd>cimport</kbd> statement, as shown in the following code snippet:</p>
<pre><code class="lang-python">    from mathlib cimport max 

    def chebyshev(int x1, int y1, int x2, int y2): 
        return max(abs(x1 - x2), abs(y1 - y2)) 
</code></pre>
<p>The <kbd>cimport</kbd> statement will read <kbd>hello.pxd</kbd> and the <kbd>max</kbd> definition will be used to generate the <kbd>distance.c</kbd> file.</p>

<h2>Working with arrays</h2>
<p>Numerical and high performance calculations often make use of arrays. Cython provides an easy way to interact with them, using directly low-level C arrays, or the more general <em>typed memoryviews</em>.</p>

<h3>C arrays and pointers</h3>
<p>C arrays are a collection of items of the same type, stored contiguously in memory. Before digging into the details, it is helpful to understand (or review) how memory is managed in C.</p>
<p>Variables in C are like containers. When creating a variable, a space in memory is reserved to store its value. For example, if we create a variable containing a 64 bit floating point number (<kbd>double</kbd>), the program will allocate 64 bit (16 bytes) of memory. This portion of memory can be accessed through an address to that memory location.</p>
<p>To obtain the address of a variable, we can use the <em>address operator</em>&nbsp;denoted by&nbsp;the <kbd>&amp;</kbd> symbol. We can also use the <kbd>printf</kbd> function, as follows, available in the <kbd>libc.stdio</kbd> Cython module to print the address of this variable:</p>
<pre><code class="lang-python">    %%cython 
    cdef double a 
    from libc.stdio cimport printf 
    printf("%p", &amp;a)
    # Output:
    # 0x7fc8bb611210 
</code></pre>
<p>Memory addresses can be stored in special variables, <em>pointers</em>, that can be declared by putting a <kbd>*</kbd> prefix in front of the variable name, as follows:</p>
<pre><code class="lang-python">    from libc.stdio cimport printf 
    cdef double a 
    cdef double *a_pointer 
    a_pointer = &amp;a # a_pointer and &amp;a are of the same type 
</code></pre>
<p>If we have a pointer, and we want to grab the value contained in the address it's pointing at, we can use the <em>dereference operator</em>&nbsp;denoted by&nbsp;the <kbd>*</kbd> symbol. Be careful, the <kbd>*</kbd> used in this context has a different meaning from the <kbd>*</kbd> used in the variable declaration:</p>
<pre><code class="lang-python">    cdef double a 
    cdef double *a_pointer 
    a_pointer = &amp;a 

    a = 3.0 
    print(*a_pointer) # prints 3.0 
</code></pre>
<p>When declaring a C array, the program allocates enough space to accommodate all the elements requested. For instance, to create an array that has 10 <kbd>double</kbd> values (16 bytes each), the program will reserve&nbsp;<em>16 * 10 = 160</em> bytes of contiguous space in memory. In Cython, we can declare such arrays using the following syntax:</p>
<pre><code class="lang-python">    cdef double arr[10]
</code></pre>
<p>We can also declare a multidimensional array, such as&nbsp;an array with <kbd>5</kbd> rows and <kbd>2</kbd> columns, using the following syntax:</p>
<pre><code class="lang-python">    cdef double arr[5][2] 
</code></pre>
<p>The memory will be allocated in a single block of memory, row after row. This order is commonly referred to as <em>row-major</em> and is depicted in the following figure. Arrays can also be ordered <em>column-major</em>, as is the case for the FORTRAN programming language:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000005.png" class="lazyload" /></p>
</div>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>Array ordering has important consequences. When iterating a C array over the last dimension, we access contiguous memory blocks (in our example, 0, 1, 2, 3 ...) while when we iterate on the first dimension, we skip a few positions (0, 2, 4, 6, 8, 1 ... ). You should always try to access memory sequentially as this optimizes cache and memory usage.</p>
</div>
</div>
<p>We can store and retrieve elements from the array using standard indexing; C arrays don't support fancy indexing or slices:</p>
<pre><code class="lang-python">    arr[0] = 1.0 
</code></pre>
<p>C arrays have many of the same behaviors as pointers. The <kbd>arr</kbd> variable, in fact, points to the memory location of the first element of the array. We can verify that the address of the first element of the array is the same as the address contained in the <kbd>arr</kbd>&nbsp;variable using the dereference operator, as follows:</p>
<pre><code class="lang-python">    %%cython 
    from libc.stdio cimport printf 
    cdef double arr[10] 
    printf("%pn", arr) 
    printf("%pn", &amp;arr[0]) 

    # Output
    # 0x7ff6de204220 
    # 0x7ff6de204220 
</code></pre>
<p>You should use C arrays and pointers when interfacing with the existing C libraries or when you need a fine control over the memory (also, they are very performant). This level of fine control is also prone to mistakes as it doesn't prevent you from accessing the wrong memory locations. For more common use cases and improved safety, you can use NumPy arrays or typed memoryviews.</p>

<h3>NumPy arrays</h3>
<p>NumPy arrays can be used as normal Python objects in Cython using their already optimized broadcasted operations. However, Cython provides a <kbd>numpy</kbd> module with better support for direct iteration.</p>
<p>When we normally access an element of a NumPy array, a few other operations take place at the interpreter level causing a major overhead. Cython can bypass those operations and checks by acting directly on the underlying memory area used by NumPy arrays, and thus obtaining impressive performance gains.</p>
<p>NumPy arrays can be declared as the <kbd>ndarray</kbd> data type. To use the data type in our code, we first need to <kbd>cimport</kbd> the <kbd>numpy</kbd> Cython module (which is not the same as the Python NumPy module). We will bind the module to the <kbd>c_np</kbd> variable to make the difference with the Python <kbd>numpy</kbd> module more explicit:</p>
<pre><code class="lang-python">    cimport numpy as c_np
    import numpy as np
</code></pre>
<p>We can now declare a NumPy array by specifying its&nbsp;type and the number of dimensions between square brackets (this is called <em>buffer syntax</em>). To declare a two-dimensional array of type <kbd>double</kbd>, we can use the following code:</p>
<pre><code class="lang-python">    cdef c_np.ndarray[double, ndim=2] arr 
</code></pre>
<p>Access to this array will be performed by directly operating on the&nbsp;underlying memory area; the operation will avoid stepping into the interpreter, giving us a tremendous speed boost.</p>
<p>In the next example, we will show the usage of typed numpy arrays and compare them&nbsp;with the normal Python version.</p>
<p>We first write the <kbd>numpy_bench_py</kbd> function that increments each element of <kbd>py_arr</kbd>.&nbsp;We declare the <kbd>i</kbd> index as an integer so that we avoid the for-loop overhead:</p>
<pre><code class="lang-python">    %%cython 
    import numpy as np 
    def numpy_bench_py(): 
        py_arr = np.random.rand(1000) 
        cdef int i 
        for i in range(1000): 
            py_arr[i] += 1 
</code></pre>
<p>Then, we write the same function using the <kbd>ndarray</kbd> type. Note that after we define the <kbd>c_arr</kbd> variable using <kbd>c_np.ndarray</kbd>, we can assign to it an array from the <kbd>numpy</kbd> Python module:</p>
<pre><code class="lang-python">    %%cython 
    import numpy as np 
    cimport numpy as c_np 

    def numpy_bench_c(): 
        cdef c_np.ndarray[double, ndim=1] c_arr 
        c_arr = np.random.rand(1000) 
        cdef int i

        for i in range(1000): 
           c_arr[i] += 1 
</code></pre>
<p>We can time the results using <kbd>timeit</kbd>, and we can see how the typed version is&nbsp;50x faster:</p>
<pre><code class="lang-python">    %timeit numpy_bench_c() 
    100000 loops, best of 3: 11.5 us per loop 
    %timeit numpy_bench_py() 
    1000 loops, best of 3: 603 us per loop 
</code></pre>

<h3>Typed memoryviews</h3>
<p>C and NumPy arrays as well as the built-in <kbd>bytes</kbd>, <kbd>bytearray</kbd>, and <kbd>array.array</kbd> objects are similar in the sense that they all operate on a contiguous memory area (also called memory <em>buffer</em>). Cython provides a universal interface--the <em>typed memoryview--</em>that unifies and simplifies the access to all these data types.</p>
<p>A <strong>memoryview</strong> is an object that maintains a reference on a specific memory area. It doesn't actually own the memory, but it can read and change its contents; in other words, it is a <em>view</em>&nbsp;on the underlying data. Memoryviews can be defined using a special syntax. For example, we can define a memoryview of <kbd>int</kbd> and a &nbsp;two-dimensional memoryview of <kbd>double</kbd> in the following way:</p>
<pre><code class="lang-python">    cdef int[:] a 
    cdef double[:, :] b 
</code></pre>
<p>The same syntax applies to the declaration of any type in variables, function definitions, class attributes, and so on. Any object that exposes a buffer interface (for example, NumPy arrays, <kbd>bytes</kbd>, and <kbd>array.array</kbd> objects) will be bound to the memoryview automatically. For example, we can bind the memoryview to a NumPy array using&nbsp;a&nbsp;simple variable assignment:</p>
<pre><code class="lang-python">    import numpy as np 

    cdef int[:] arr 
    arr_np = np.zeros(10, dtype='int32') 
    arr = arr_np # We bind the array to the memoryview 
</code></pre>
<p>It is important to note that the memoryview does not <em>own</em> the data, but it only provides a way to <em>access</em> and <em>change</em> the data it is bound to; the ownership, in this case, is left to the NumPy array. As you can see in the following example, changes made through the memoryview will act on the underlying memory area and will be reflected in the original NumPy structure (and vice versa):</p>
<pre><code class="lang-python">    arr[2] = 1 # Changing memoryview 
    print(arr_np) 
    # [0 0 1 0 0 0 0 0 0 0] 
</code></pre>
<p>In a certain sense, the mechanism behind&nbsp;memoryviews is similar to what&nbsp;NumPy produces when we slice an array. As we have seen in Chapter 3, <em>Fast Array Operations with NumPy and Pandas</em>, slicing a NumPy array does not copy the data but returns a view on the same memory area, and changes to the view will reflect on the original array.</p>
<p>Memoryviews also support array slicing with the standard NumPy syntax:</p>
<pre><code class="lang-python">    cdef int[:, :, :] a 
    arr[0, :, :] # Is a 2-dimensional memoryview 
    arr[0, 0, :] # Is a 1-dimensional memoryview 
    arr[0, 0, 0] # Is an int 
</code></pre>
<p>To copy data between one memoryview and another, you can use syntax similar to slice assignment, as shown in the following code:</p>
<pre><code class="lang-python">    import numpy as np 

    cdef double[:, :] b 
    cdef double[:] r 
    b = np.random.rand(10, 3) 
    r = np.zeros(3, dtype='float64') 

    b[0, :] = r # Copy the value of r in the first row of b 
</code></pre>
<p>In the next section, we will use the typed memoryviews to declare types for&nbsp;the arrays in our particle simulator.</p>

<h2>Particle simulator in Cython</h2>
<p>Now that we have a basic understanding of how Cython works, we can rewrite the <kbd>ParticleSimulator.evolve</kbd> method. Thanks to Cython, we can convert our loops in C, thus removing the overhead introduced by the Python interpreter.</p>
<p>In Chapter 3, <em>Fast Array Operations with NumPy and Pandas</em>, we wrote a fairly efficient version of the <kbd>evolve</kbd> method using NumPy. We can rename the old version as <kbd>evolve_numpy</kbd> to differentiate it from the new version:</p>
<pre><code class="lang-python">    def evolve_numpy(self, dt): 
        timestep = 0.00001 
        nsteps = int(dt/timestep) 

        r_i = np.array([[p.x, p.y] for p in self.particles])     
        ang_speed_i = np.array([p.ang_speed for p in self.particles]) 
        v_i = np.empty_like(r_i) 

        for i in range(nsteps): 
            norm_i = np.sqrt((r_i ** 2).sum(axis=1)) 

            v_i = r_i[:, [1, 0]] 
            v_i[:, 0] *= -1 
            v_i /= norm_i[:, np.newaxis]         

            d_i = timestep * ang_speed_i[:, np.newaxis] * v_i 

            r_i += d_i 

        for i, p in enumerate(self.particles): 
            p.x, p.y = r_i[i] 
</code></pre>
<p>We want to convert this code to Cython. Our strategy will be to take advantage of the fast indexing operations by removing the NumPy array broadcasting, thus reverting to an indexing-based algorithm. Since Cython generates efficient C code, we are free to use as many loops as we like without any performance penalty.</p>
<p>As a design choice, we can decide to encapsulate the loop in a function that we will rewrite in a Cython module called <kbd>cevolve.pyx</kbd>. The module will contain a single Python function,&nbsp;<kbd>c_evolve</kbd>, that will take the particle positions, angular velocities, timestep, and number of steps as input.</p>
<p>At first, we are not adding typing information; we just want to isolate the function and ensure that we can compile our module without errors:</p>
<pre><code class="lang-python">    # file: simul.py 
    def evolve_cython(self, dt): 
        timestep = 0.00001 
        nsteps = int(dt/timestep) 

        r_i = np.array([[p.x, p.y] for p in self.particles])     
        ang_speed_i = np.array([p.ang_speed for p in self.particles]) 

        c_evolve(r_i, ang_speed_i, timestep, nsteps) 

        for i, p in enumerate(self.particles): 
            p.x, p.y = r_i[i] 

    # file: cevolve.pyx 
    import numpy as np 

    def c_evolve(r_i, ang_speed_i, timestep, nsteps): 
        v_i = np.empty_like(r_i) 

        for i in range(nsteps): 
            norm_i = np.sqrt((r_i ** 2).sum(axis=1)) 

            v_i = r_i[:, [1, 0]] 
            v_i[:, 0] *= -1 
            v_i /= norm_i[:, np.newaxis]         
     
            d_i = timestep * ang_speed_i[:, np.newaxis] * v_i 

            r_i += d_i 
</code></pre>
<p>Note that we don't need a return value for <kbd>c_evolve</kbd>&nbsp;as values are updated in the <kbd>r_i</kbd> array in-place. We can benchmark the untyped Cython version against the old NumPy version by slightly changing our benchmark function, as follows:</p>
<pre><code class="lang-python">    def benchmark(npart=100, method='python'): 
        particles = [Particle(uniform(-1.0, 1.0),
                              uniform(-1.0, 1.0),
                              uniform(-1.0, 1.0)) 
                              for i in range(npart)] 
        simulator = ParticleSimulator(particles) 
        if method=='python': 
            simulator.evolve_python(0.1)
        elif method == 'cython': 
            simulator.evolve_cython(0.1) 
        elif method == 'numpy': 
            simulator.evolve_numpy(0.1) 
</code></pre>
<p>We can time the different versions in an IPython shell:</p>
<pre><code class="lang-python">    %timeit benchmark(100, 'cython') 
    1 loops, best of 3: 401 ms per loop 
    %timeit benchmark(100, 'numpy') 
    1 loops, best of 3: 413 ms per loop 
</code></pre>
<p>The two versions have the same speed. Compiling the Cython module without static typing doesn't have any advantage over pure Python. The next step is to declare the type of all the important variables so that Cython can perform its optimizations.</p>
<p>We can start adding types to the function arguments and see how the performance changes. We can declare the arrays as typed memoryviews containing <kbd>double</kbd> values. It's worth mentioning that if we pass an array of the&nbsp;<kbd>int</kbd> or <kbd>float32</kbd> type, the casting won't happen automatically and we will get an error:</p>
<pre><code class="lang-python">    def c_evolve(double[:, :] r_i,
                 double[:] ang_speed_i,
                 double timestep,
                 int nsteps): 
</code></pre>
<p>At this point, we can rewrite the loops over the particles and timesteps. We can declare the <kbd>i</kbd>&nbsp;and&nbsp;<kbd>j</kbd> iteration indices and the <kbd>nparticles</kbd> particle number as <kbd>int</kbd>:</p>
<pre><code class="lang-python">    cdef int i, j 
    cdef int nparticles = r_i.shape[0] 
</code></pre>
<p>The algorithm is very similar to the pure Python version; we iterate over the particles and timesteps, and we compute the velocity and displacement vectors for each particle coordinate using the following code:</p>
<pre><code class="lang-python">      for i in range(nsteps): 
          for j in range(nparticles): 
              x = r_i[j, 0] 
              y = r_i[j, 1] 
              ang_speed = ang_speed_i[j] 

              norm = sqrt(x ** 2 + y ** 2) 

              vx = (-y)/norm 
              vy = x/norm 

              dx = timestep * ang_speed * vx 
              dy = timestep * ang_speed * vy 

              r_i[j, 0] += dx 
              r_i[j, 1] += dy 
</code></pre>
<p>In the preceding&nbsp;code, we added the <kbd>x</kbd>, <kbd>y</kbd>, <kbd>ang_speed</kbd>, <kbd>norm</kbd>, <kbd>vx</kbd>, <kbd>vy</kbd>, <kbd>dx</kbd>, and <kbd>dy</kbd> variables. To avoid the Python interpreter overhead, we have to declare them with their corresponding types at the beginning of the function, as follows:</p>
<pre><code class="lang-python">    cdef double norm, x, y, vx, vy, dx, dy, ang_speed 
</code></pre>
<p>We also used a function called <kbd>sqrt</kbd> to calculate the norm. If we use the <kbd>sqrt</kbd> present in the <kbd>math</kbd> module or the one in <kbd>numpy</kbd>, we will&nbsp;again include a slow Python function in our critical loop, thus killing our performance. A fast <kbd>sqrt</kbd> is available in the standard C library, already wrapped in the <kbd>libc.math</kbd> Cython module:</p>
<pre><code class="lang-python">    from libc.math cimport sqrt 
</code></pre>
<p>We can rerun our benchmark to assess our improvements, as follows:</p>
<pre><code class="lang-python">    In [4]: %timeit benchmark(100, 'cython') 
    100 loops, best of 3: 13.4 ms per loop 
    In [5]: %timeit benchmark(100, 'numpy') 
    1 loops, best of 3: 429 ms per loop 
</code></pre>
<p>For small particle numbers, the speed-up is massive as we obtained a 40x performance over the previous version. However, we should also try to test the performance scaling with a larger number of particles:</p>
<pre><code class="lang-python">    In [2]: %timeit benchmark(1000, 'cython') 
    10 loops, best of 3: 134 ms per loop 
    In [3]: %timeit benchmark(1000, 'numpy') 
    1 loops, best of 3: 877 ms per loop
</code></pre>
<p>As we increase the number of particles, the two versions get closer in speed. By increasing the particle size to 1000, we already decreased our speed-up to a more modest 6x. This is likely due to the fact that, as we increase the number of particles, the Python for-loop overhead becomes less and less significant compared to the speed of other operations.</p>

<h2>Profiling Cython</h2>
<p>Cython provides a feature, called <em>annotated view</em>, that helps&nbsp;find which lines are executed in the Python interpreter and which are good candidates for ulterior optimizations.&nbsp;We can turn this feature on by compiling a Cython file with the <kbd>-a</kbd> option. In this way, Cython will generate an HTML file containing our code annotated with some useful information. The usage of the&nbsp;<kbd>-a</kbd> option is as follows:</p>
<pre><code class="lang-python">$ cython -a cevolve.pyx
$ firefox cevolve.html
</code></pre>
<p>The HTML file displayed in the following screenshot shows our Cython file line by line:</p>

<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000035.png" class="lazyload" /></p>
</div>

<p>Each line in the source code can appear&nbsp;in different shades of yellow. A more intense&nbsp;color corresponds to more&nbsp;interpreter-related calls, while white lines are translated to regular&nbsp;C code. Since interpreter calls substantially slow down execution, the objective is to make the function body as white as possible. By clicking on any of the lines, we can inspect the code generated by the Cython compiler. For example, the <kbd>v_y = x/norm</kbd> line checks that the norm is not <kbd>0</kbd>&nbsp;and raises a <kbd>ZeroDivisionError</kbd>&nbsp;if the condition is not verified. The <kbd>x = r_i[j, 0]</kbd> line shows that Cython checks whether&nbsp;the indexes are within the bounds of the array. You may note that the last line is of a very intense color; by inspecting the code, we can see that this is actually a glitch; the code refers to a boilerplate related to the end of the function.</p>
<p>Cython can shut down checks, such as division by zero, so that it can remove those extra interpreter related calls; this is usually accomplished through compiler directives. There are a few different ways to add compiler directives:</p>
<ul>
<li>Using a decorator or a context manager</li>
<li>Using a comment at the beginning of the file</li>
<li>Using the Cython command-line options</li>
</ul>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>For a complete list of the Cython compiler directives, you can refer to the official documentation at <a href="http://docs.cython.org/src/reference/compilation.html#compiler-directives">http://docs.cython.org/src/reference/compilation.html#compiler-directives</a>.</p>
</div>
</div>
<p>For example, to disable bounds&nbsp;checking for arrays, it is sufficient to decorate a function with <kbd>cython.boundscheck</kbd>, in the following way:</p>
<pre><code class="lang-python">    cimport cython 

    @cython.boundscheck(False) 
    def myfunction(): 
        # Code here 
</code></pre>
<p>Alternatively, we can use <kbd>cython.boundscheck</kbd> to wrap a block of code into a context manager, as follows:</p>
<pre><code class="lang-python">    with cython.boundscheck(False): 
        # Code here 
</code></pre>
<p>If we want to disable bounds checking for a whole module, we can add the following line of code at the beginning of the file:</p>
<pre><code class="lang-python">    # cython: boundscheck=False 
</code></pre>
<p>To alter the directives with the command-line options, you can use the&nbsp;<kbd>-X</kbd>&nbsp;option as follows:</p>
<pre><code class="lang-python">$ cython -X boundscheck=True
</code></pre>
<p>To disable&nbsp;the extra checks in our <kbd>c_evolve</kbd> function, we can disable the <kbd>boundscheck</kbd> directive and enable&nbsp;<kbd>cdivision</kbd> (this prevents checks for&nbsp;<kbd>ZeroDivisionError</kbd>), as in the following code:</p>
<pre><code class="lang-python">    cimport cython 

    @cython.boundscheck(False) 
    @cython.cdivision(True) 
    def c_evolve(double[:, :] r_i,
                 double[:] ang_speed_i,
                 double timestep,
                 int nsteps): 
</code></pre>
<p>If we look at the annotated view again, the loop body has become&nbsp;completely white--we removed all traces of the interpreter from the inner loop. In order to recompile, just type <kbd>python setup.py build_ext --inplace</kbd> again. By running the benchmark, however, we note that we didn't obtain a performance improvement, suggesting that those checks are not part of the bottleneck:</p>
<pre><code class="lang-python">    In [3]: %timeit benchmark(100, 'cython') 
    100 loops, best of 3: 13.4 ms per loop 
</code></pre>
<p>Another way to profile Cython code is through the use of&nbsp;the&nbsp;<kbd>cProfile</kbd>&nbsp;module.&nbsp;As an example,&nbsp;we can write a simple function that calculates&nbsp;the Chebyshev distance between coordinate arrays. Create a <kbd>cheb.py</kbd>&nbsp;file:</p>
<pre><code class="lang-python">    import numpy as np 
    from distance import chebyshev 

    def benchmark(): 
        a = np.random.rand(100, 2) 
        b = np.random.rand(100, 2) 
        for x1, y1 in a: 
            for x2, y2 in b: 
                chebyshev(x1, x2, y1, y2) 
</code></pre>
<p>If we try profiling this script as-is, we won't get any statistics regarding the functions that we implemented in Cython. If we want to collect&nbsp;profiling information&nbsp;for the <kbd>max</kbd> and <kbd>min</kbd> functions, we need&nbsp;to add the <kbd>profile=True</kbd> option to the <kbd>mathlib.pyx</kbd> file, as shown in the following code:</p>
<pre><code class="lang-python">    # cython: profile=True 

    cdef int max(int a, int b): 
        # Code here 
</code></pre>
<p>We can now profile our script with <kbd>%prun</kbd> using IPython, as follows:</p>
<pre><code class="lang-python">    import cheb 
    %prun cheb.benchmark() 

# Output:
2000005 function calls in 2.066 seconds 

  Ordered by: internal time 

  ncalls tottime percall cumtime percall filename:lineno(function) 
       1   1.664   1.664   2.066   2.066 cheb.py:4(benchmark) 
 1000000   0.351   0.000   0.401   0.000 {distance.chebyshev} 
 1000000   0.050   0.000   0.050   0.000 mathlib.pyx:2(max) 
       2   0.000   0.000   0.000   0.000 {method 'rand' of        'mtrand.RandomState' objects} 
       1   0.000   0.000   2.066   2.066 &lt;string&gt;:1(&lt;module&gt;) 
       1   0.000   0.000   0.000   0.000 {method 'disable' of        '_lsprof.Profiler' objects} 
</code></pre>
<p>From the output, we can see that the <kbd>max</kbd> function is present and is not a bottleneck. Most of the time&nbsp;seems to be&nbsp;spent&nbsp;in the <kbd>benchmark</kbd> function, meaning that the bottleneck is likely the pure Python for-loop. In this case, the best strategy will&nbsp;be rewriting the loop in NumPy or porting the code to Cython.</p>

<h2>Using Cython with Jupyter</h2>
<p>Optimizing Cython code requires substantial trial and error. Fortunately, Cython tools can be conveniently accessed through the Jupyter notebook for a more streamlined and integrated experience.</p>
<p>You can launch a notebook session by typing <kbd>jupyter notebook</kbd> in the command line and you can load the Cython magic by typing <kbd>%load_ext cython</kbd> in a cell.</p>
<p>As already mentioned earlier, the <kbd>%%cython</kbd> magic can be used to compile and load the Cython code inside the current session. As an example, we may copy the contents of <kbd>cheb.py</kbd> into a notebook cell:</p>
<pre><code class="lang-python">    %%cython
    import numpy as np

    cdef int max(int a, int b):
        return a if a &gt; b else b

    cdef int chebyshev(int x1, int y1, int x2, int y2):
        return max(abs(x1 - x2), abs(y1 - y2))

    def c_benchmark():
        a = np.random.rand(1000, 2)
        b = np.random.rand(1000, 2)

        for x1, y1 in a:
           for x2, y2 in b:
               chebyshev(x1, x2, y1, y2)
</code></pre>
<p>A useful feature of the <kbd>%%cython</kbd> magic is the <kbd>-a</kbd> option that will compile the code and produce an annotated view (just like the command line <kbd>-a</kbd> option) of the source directly in the notebook, as shown in the following screenshot:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000004.png" class="lazyload" /></p>
<p>This allows you to quickly test different versions of your code and also use the other integrated tools available in Jupyter. For example, we can time and profile the code (provided that we activate the profile directive in the cell) in the same session using tools such as <kbd>%prun</kbd> and <kbd>%timeit</kbd>. For example, we can inspect the profiling results by taking advantage of the <kbd>%prun</kbd> magic, as shown in the following screenshot:</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000023.png" class="lazyload" /></p>
<p>It is also possible to use the <kbd>line_profiler</kbd> tool we discussed in Chapter 1,&nbsp;<em>Benchmarking and Profiling</em>, directly in the notebook. In order to support line annotations, it is necessary to do the following things:</p>
<ul>
<li>Enable the <kbd>linetrace=True</kbd> and <kbd>binding=True</kbd>&nbsp;compiler directives</li>
<li>Enable the <kbd>CYTHON_TRACE=1</kbd> flag at compile time</li>
</ul>
<p>This can be easily accomplished by adding the respective arguments to the <kbd>%%cython</kbd> magic, and by setting the compiler directives, as shown in the following code:</p>
<pre><code class="lang-python">    %%cython -a -f -c=-DCYTHON_TRACE=1
    # cython: linetrace=True
    # cython: binding=True

    import numpy as np

    cdef int max(int a, int b):
        return a if a &gt; b else b

    def chebyshev(int x1, int y1, int x2, int y2):
        return max(abs(x1 - x2), abs(y1 - y2))

    def c_benchmark():
        a = np.random.rand(1000, 2)
        b = np.random.rand(1000, 2)
    
        for x1, y1 in a:
            for x2, y2 in b:
                chebyshev(x1, x2, y1, y2)
</code></pre>
<p>Once the code is instrumented, we can profile using the <kbd>%lprun</kbd> magic:</p>
<pre><code class="lang-python">%lprun -f c_benchmark c_benchmark()
# Output:
Timer unit: 1e-06 s

Total time: 2.322 s
File: /home/gabriele/.cache/ipython/cython/_cython_magic_18ad8204e9d29650f3b09feb48ab0f44.pyx
Function: c_benchmark at line 11

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    11                                           def c_benchmark():
    12         1          226    226.0      0.0      a = np.random.rand...
    13         1           67     67.0      0.0      b = np.random.rand...    
    14                                               
    15      1001         1715      1.7      0.1      for x1, y1 in a:
    16   1001000      1299792      1.3     56.0          for x2, y2 in b:
    17   1000000      1020203      1.0     43.9              chebyshev...
</code></pre>
<p>As you can see, a good chunk of time is actually spent in line 16, which is a pure Python loop and a good candidate for further optimization.</p>
<p>The tools available in Jupyter notebook allow for a fast edit-compile-test cycle so that you can quickly prototype and save time when testing different solutions.</p>

<h2>Summary</h2>
<p>Cython is a tool that bridges the convenience of Python with the speed of C. Compared to&nbsp;C bindings, Cython programs are much easier to maintain and debug, thanks to the tight integration and compatibility with Python and the availability of excellent tools.</p>
<p>In this chapter, we introduced the basics of the Cython language and how to make our programs faster by adding static types to our variables and functions. We also learned how to work with C arrays, NumPy arrays, and memoryviews.</p>
<p>We optimized our particle simulator by rewriting the critical <kbd>evolve</kbd> function, obtaining a tremendous speed gain. Finally, we learned how to use the annotated view to spot hard-to-find interpreter related calls and how to enable <kbd>cProfile</kbd>&nbsp;support in Cython. Also, we learned how to take advantage of the Jupyter notebook&nbsp;for integrated profiling and analysis of Cython codes.</p>
<p>In the next chapter, we will explore other tools&nbsp;that can generate fast machine code on the fly, without requiring compilation of our code to C ahead of time.</p>

</div>




<!--Chapter 5-->

<div class="chapter" data-chapter-number="5">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 5 </span></div>
<h1 class="chaptertitle">Exploring Compilers</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>Python is a mature and widely used language and there is a large interest in improving its performance by compiling functions and methods directly to machine code rather than executing instructions in the interpreter. We have already seen a compiler example in Chapter 4, <em>C Performance with Cython</em>, where Python code is enhanced with types, compiled to efficient C code, and the interpreter calls are side-stepped.</p>
<p>In this chapter, we will explore two projects--Numba and PyPy--that approach compilation in a slightly different way. <strong>Numba</strong> is a library designed to compile small functions on the fly. Instead of transforming Python code to C, Numba analyzes and compiles Python functions directly to machine code. <strong>PyPy</strong> is a replacement interpreter that works by analyzing the code at runtime and optimizing the slow loops automatically.</p>
<p>These tools are called <strong>Just</strong>-<strong>In</strong>-<strong>Time</strong> (<strong>JIT</strong>) compilers because the compilation is performed at runtime rather than before running the code (in other cases, the compiler is called ahead-of-time or AOT).</p>
<p>The list of topics to be covered in this chapter is as follows:</p>
<ul>
<li>Getting started with Numba</li>
<li>Implementing fast functions with native mode compilation</li>
<li>Understanding and implementing universal functions</li>
<li>JIT classes</li>
<li>Setting up PyPy</li>
<li>Running the particle simulator with PyPy</li>
<li>Other interesting compilers</li>
</ul>

<h2>Numba</h2>
<p>Numba was started in 2012 by Travis Oliphant, the original author of NumPy, as a library for compiling individual Python functions at runtime using the <strong>Low-Level Virtual Machine</strong>&nbsp;(<strong>LLVM</strong>) toolchain.</p>
<p>LLVM is a set of tools designed to write compilers. LLVM is language agnostic and is used to write compilers for a wide range of languages (an important example is the clang&nbsp;compiler). One of the core aspects of LLVM is the intermediate representation (the LLVM IR), a very low-level platform-agnostic language similar to assembly, that can be compiled to machine code for the specific target platform.</p>
<p>Numba works by inspecting Python functions and by compiling them, using LLVM, to the IR. As we have already seen in the last&nbsp;chapter, the speed gains can be obtained when we introduce types for variables and functions. Numba implements clever algorithms to guess the types (this is called type inference) and compiles type-aware versions of the functions for fast execution.</p>
<p>Note that Numba was developed to improve the performance of numerical code. The development efforts often prioritize the optimization of applications that intensively use NumPy arrays.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Numba is evolving really fast and can have substantial improvements between releases and, sometimes, backward incompatible changes.&nbsp; To keep up, ensure that you refer to the release notes for each version. In the rest of this chapter, we will use Numba version 0.30.1; ensure that you install the correct version to avoid any error. The complete code examples in this chapter can be found in the&nbsp;<kbd>Numba.ipynb</kbd>&nbsp;notebook.</p>
</div>
</div>

<h3>First steps with Numba</h3>
<p>Getting started with Numba is fairly straightforward. As a first example, we will implement a function that calculates the sum of squares of an array. The function definition is as follows:</p>
<pre><code class="lang-python">    def sum_sq(a):
        result = 0
        N = len(a)
        for i in range(N):
            result += a[i]
        return result
</code></pre>
<p>To set up this function with Numba, it is sufficient to apply the <kbd>nb.jit</kbd> decorator:</p>
<pre><code class="lang-python">    from numba import nb

    @nb.jit
    def sum_sq(a):
        ...
</code></pre>
<p>The <kbd>nb.jit</kbd> decorator won't do much when applied. However, when the function will be invoked for the first time, Numba will detect the type of the input argument,&nbsp;<kbd>a</kbd> , and compile a specialized, performant version of the original function.</p>
<p>To measure the performance gain obtained by the Numba compiler, we can compare the timings of the original and the specialized functions. The original, undecorated function can be easily accessed through the <kbd>py_func</kbd>&nbsp;attribute. The timings for the two functions are as follows:</p>
<pre><code class="lang-python">    import numpy as np

    x = np.random.rand(10000)

    # Original
    %timeit sum_sq.py_func(x)
    100 loops, best of 3: 6.11 ms per loop

    # Numba
    %timeit sum_sq(x)
    100000 loops, best of 3: 11.7 &micro;s per loop
</code></pre>
<p>From the previous code, you can see how the Numba version&nbsp;(11.7 &micro;s) is one order of magnitude faster than the Python version (6.11 ms). We can also compare how this implementation stacks up against NumPy standard operators:</p>
<pre><code class="lang-python">    %timeit (x**2).sum()
    10000 loops, best of 3: 14.8 &micro;s per loop
</code></pre>
<p>In this case, the Numba compiled function is marginally faster than NumPy vectorized operations. The reason for the extra speed of the Numba version is likely that the NumPy version allocates an extra array before performing the sum in comparison with the in-place operations performed by our <kbd>sum_sq</kbd> function.</p>
<p>As we didn't use array-specific methods in <kbd>sum_sq</kbd>, we can also try to apply the same function on a regular Python list of floating point numbers. Interestingly, Numba is able to obtain a substantial speed up even in this case, as compared to a list comprehension:</p>
<pre><code class="lang-python">    x_list = x.tolist()
    %timeit sum_sq(x_list)
    1000 loops, best of 3: 199 &micro;s per loop

    %timeit sum([x**2 for x in x_list])
    1000 loops, best of 3: 1.28 ms per loop
</code></pre>
<p>Considering that all we needed to do was apply a simple decorator to obtain an incredible speed up over different data types, it's no wonder that what Numba does looks like magic. In the following sections, we will dig deeper and understand how Numba works and evaluate the benefits and limitations of the Numba compiler.</p>

<h3>Type specializations</h3>
<p>As shown earlier, the <kbd>nb.jit</kbd> decorator works by compiling a specialized version of the function once it encounters a new argument type. To better understand how this works, we can inspect the decorated function in the <kbd>sum_sq</kbd> example.</p>
<p>Numba exposes the specialized types using the <kbd>signatures</kbd> attribute. Right after the <kbd>sum_sq</kbd> definition, we can inspect the available specialization by accessing the <kbd>sum_sq.signatures</kbd>, as follows:</p>
<pre><code class="lang-python">    sum_sq.signatures
    # Output:
    # []
</code></pre>
<p>If we call this function with a specific argument, for instance, an array of <kbd>float64</kbd> numbers, we can see how Numba compiles a specialized version on the fly. If we also apply the function on an array of <kbd>float32</kbd>, we can see how a new entry is added to the <kbd>sum_sq.signatures</kbd> list:</p>
<pre><code class="lang-python">    x = np.random.rand(1000).astype('float64')
    sum_sq(x)
    sum_sq.signatures
    # Result:
    # [(array(float64, 1d, C),)]

    x = np.random.rand(1000).astype('float32')
    sum_sq(x)
    sum_sq.signatures
    # Result:
    # [(array(float64, 1d, C),), (array(float32, 1d, C),)]
</code></pre>
<p>It is possible to explicitly compile the function for certain types by passing a signature to the <kbd>nb.jit</kbd> function.</p>
<p>An individual signature can be passed as a tuple that contains the type we would like to accept. Numba provides a great variety of types that can be found in the <kbd>nb.types</kbd> module, and they are also available in the top-level <kbd>nb</kbd> namespace. If we want to specify an array of a specific type, we can use the slicing operator,&nbsp;<kbd>[:]</kbd>, on the type itself. In the following example, we demonstrate how to declare a function that takes an array of <kbd>float64</kbd>&nbsp;as its only argument:</p>
<pre><code class="lang-python">    @nb.jit((nb.float64[:],))
    def sum_sq(a):
</code></pre>
<p>Note that when we explicitly declare a signature, we are prevented from using other types, as demonstrated in the following example. If we try to pass an array,&nbsp;<kbd>x</kbd>, as <kbd>float32</kbd>, Numba will raise a <kbd>TypeError</kbd>:</p>
<pre><code class="lang-python">    sum_sq(x.astype('float32'))
    # <span>TypeError: No matching definition for argument type(s) 
    array(float32, 1d, C)</span>
</code></pre>
<p>Another way to declare signatures is through type strings. For example, a function that takes a <kbd>float64</kbd> as input and returns a <kbd>float64</kbd> as output can be declared with the <kbd>float64(float64)</kbd>&nbsp;string. Array types can be declared using a <kbd>[:]</kbd> suffix. To put this together, we can declare a signature for our <kbd>sum_sq</kbd>&nbsp;function, as follows:</p>
<pre><code class="lang-python">    @nb.jit("float64(float64[:])")
    def sum_sq(a):
</code></pre>
<p>You can also pass multiple signatures by passing a list:</p>
<pre><code class="lang-python">    @nb.jit(["float64(float64[:])",
             "float64(float32[:])"])
    def sum_sq(a):
</code></pre>

<h3>Object mode versus native mode</h3>
<p>So far, we have shown how Numba behaves when handling a fairly simple function. In this case, Numba worked exceptionally well, and we obtained great performance on arrays and lists.</p>
<p>The degree of optimization obtainable from Numba depends on how well Numba is able to infer the variable types and how well it can translate those standard Python operations to fast type-specific versions. If this happens, the interpreter is side-stepped and we can get performance gains similar to those of Cython.</p>
<p>When Numba cannot infer variable types, it will still try and compile the code, reverting to the interpreter when the types can't be determined or when certain operations are unsupported. In Numba, this is called <strong>object mode</strong> and is in contrast to the interpreter-free scenario, called <strong>native mode</strong>.</p>
<p>Numba provides a function, called <kbd>inspect_types</kbd>, that helps understand how effective the type inference was and which operations were optimized. As an example, we can take a look at the types inferred for our <kbd>sum_sq</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    sum_sq.inspect_types()
</code></pre>
<p>When this function is called, Numba will print the type inferred for each specialized version of the function. The output consists of blocks that contain information about variables and types associated with them. For example, we can examine the&nbsp;<kbd>N = len(a)</kbd>&nbsp;line:</p>
<pre><code class="lang-python">    # --- LINE 4 --- 
    #   a = arg(0, name=a)  :: array(float64, 1d, A)
    #   $0.1 = global(len: &lt;built-in function len&gt;)  :: 
    Function(&lt;built-in function len&gt;)
    #   $0.3 = call $0.1(a)  :: (array(float64, 1d, A),) -&gt; int64
    #   N = $0.3  :: int64

    N = len(a)
</code></pre>
<p>For each line, Numba prints a thorough description of variables, functions, and intermediate results. In the preceding example, you can see (second line) that the argument <kbd>a</kbd> is correctly identified as an array of <kbd>float64</kbd> numbers. At <kbd>LINE&nbsp;4</kbd>, the input and return type of the <kbd>len</kbd> function is also correctly identified (and likely optimized) as taking an array of <kbd>float64</kbd> numbers and returning an <kbd>int64</kbd>.</p>
<p>If you scroll through the output, you can see how all the variables have a well-defined type. Therefore, we can be certain that Numba is able to compile the code quite efficiently. This form of compilation is called <strong>native mode</strong>.</p>
<p>As a counter example, we can see what happens if we write a function with unsupported operations. For example, as of version 0.30.1, Numba has limited support for string operations.</p>
<p>We can implement a function that concatenates a series of strings, and compiles it as follows:</p>
<pre><code class="lang-python">    @nb.jit
    def concatenate(strings):
        result = ''
        for s in strings:
            result += s
        return result
</code></pre>
<p>Now, we can invoke this function with a list of strings and inspect the types:</p>
<pre><code class="lang-python">    concatenate(['hello', 'world'])
    concatenate.signatures
    # Output: [(reflected list(str),)]
    concatenate.inspect_types()
</code></pre>
<p>Numba will return the output of the function for the <kbd>reflected list (str)</kbd>&nbsp;type. We can, for instance, examine how line 3 gets inferred. The output of <kbd>concatenate.inspect_types()</kbd> is reproduced here:</p>
<pre><code class="lang-python">    # --- LINE 3 --- 
    #   strings = arg(0, name=strings)  :: pyobject
    #   $const0.1 = const(str, )  :: pyobject
    #   result = $const0.1  :: pyobject
    #   jump 6
    # label 6

    result = ''
</code></pre>
<p>You can see how this time, each variable or function is of the generic <kbd>pyobject</kbd> type rather than a specific one. This means that, in this case, Numba is unable to compile this operation without the help of the Python interpreter. Most importantly, if we time the original and compiled function, we note that the compiled function is about three times <em>slower</em> than the pure Python counterpart:</p>
<pre><code class="lang-python">    x = ['hello'] * 1000
    %timeit concatenate.py_func(x)
    10000 loops, best of 3: 111 &micro;s per loop

    %timeit concatenate(x)
    1000 loops, best of 3: 317 &micro;s per loop
</code></pre>
<p>This is because the Numba compiler is not able to optimize the code and adds some extra overhead to the function call.</p>
<p>As you may have noted, Numba compiled the code without complaints even if it is inefficient. The main reason for this is that Numba can still compile other sections of the code in an efficient manner while falling back to the Python interpreter for other parts of the code. This compilation strategy is called <strong>object mode</strong>.</p>
<p>It is possible to force the use of native mode by passing the <kbd>nopython=True</kbd> option to the <kbd>nb.jit</kbd> decorator. If, for example, we apply this decorator to our concatenate function, we observe that Numba throws an error on first invocation:</p>
<pre><code class="lang-python">    @nb.jit(nopython=True)
    def concatenate(strings):
        result = ''
        for s in strings:
            result += s
        return result

    concatenate(x)
<span>    # Exception:
    # TypingError: Failed at nopython (nopython frontend)</span>
</code></pre>
<p>This feature is quite useful for debugging and ensuring that all the code is fast and correctly typed.</p>

<h3>Numba and NumPy</h3>
<p>Numba was originally developed to easily increase performance of code that uses NumPy arrays. Currently, many NumPy features are implemented efficiently by the compiler.</p>

<h4>Universal functions with Numba</h4>
<p>Universal functions are special functions defined in NumPy that are able to operate on arrays of different sizes and shapes according to the broadcasting rules. One of the best features of Numba is the implementation of fast <kbd>ufuncs</kbd>.</p>
<p>We have already seen some <kbd>ufunc</kbd> examples in Chapter 3<em>, Fast Array Operations with NumPy and Pandas</em>. For instance, the <kbd>np.log</kbd> function is a&nbsp;<kbd>ufunc</kbd> because it can accept scalars and arrays of different sizes and shapes. Also, universal functions that take multiple arguments still work according to the&nbsp; broadcasting rules. Examples of universal functions that take multiple arguments are <kbd>np.sum</kbd> or <kbd>np.difference</kbd>.</p>
<p>Universal functions can be defined in standard NumPy by implementing the scalar version and using the <kbd>np.vectorize</kbd> function to enhance the function with the broadcasting feature. As an example, we will see how to write the <em>Cantor pairing function</em>.</p>
<p>A pairing function is a function that encodes two natural numbers into a single natural number so that you can easily interconvert between the two representations. The Cantor pairing function can be written as follows:</p>
<pre><code class="lang-python">    import numpy as np

    def cantor(a, b):
        return  int(0.5 * (a + b)*(a + b + 1) + b)
</code></pre>
<p>As already mentioned, it is possible to create a ufunc in pure Python using the <kbd>np.vectorized</kbd> decorator:</p>
<pre><code class="lang-python">    @np.vectorize
    def cantor(a, b):
        return  int(0.5 * (a + b)*(a + b + 1) + b)

    cantor(np.array([1, 2]), 2)
    # Result:
    # array([ 8, 12])
</code></pre>
<p>Except for the convenience, defining universal functions in pure Python is not very useful as it requires a lot of function calls affected by interpreter overhead. For this reason, ufunc implementation is usually done in C or Cython, but Numba beats all these methods by its convenience.</p>
<p>All that is needed to do in order to perform the conversion is using the equivalent decorator,&nbsp;<kbd>nb.vectorize</kbd>. We can compare the speed of the standard <kbd>np.vectorized</kbd> version which, in the following code, is&nbsp;called <kbd>cantor_py</kbd>, and the same function is implemented using standard NumPy operations:</p>
<pre><code class="lang-python">    # Pure Python
    %timeit cantor_py(x1, x2)
    100 loops, best of 3: 6.06 ms per loop
    # Numba
    %timeit cantor(x1, x2)
    100000 loops, best of 3: 15 &micro;s per loop
    # NumPy
    %timeit (0.5 * (x1 + x2)*(x1 + x2 + 1) + x2).astype(int)
    10000 loops, best of 3: 57.1 &micro;s per loop
</code></pre>
<p>You can see how the Numba version beats all the other options by a large margin! Numba works extremely well because the function is simple and type inference is possible.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>An additional advantage of universal functions is that, since they depend on individual values, their evaluation can also be executed in parallel. Numba provides an easy way to parallelize such functions by passing the <kbd>target="cpu"</kbd> or <kbd>target="gpu"</kbd> keyword argument to the <kbd>nb.vectorize</kbd> decorator.</p>
</div>
</div>

<h4>Generalized universal functions</h4>
<p>One of the main limitations of universal functions is that they must be defined on scalar values. A generalized universal function, abbreviated <kbd>gufunc</kbd>, is an extension of universal functions to procedures that take arrays.</p>
<p>A&nbsp;classic example is the matrix multiplication. In NumPy, matrix multiplication can be applied using the <kbd>np.matmul</kbd> function, which takes two 2D arrays and returns another 2D array. An example usage of <kbd>np.matmul</kbd> is as follows:</p>
<pre><code class="lang-python">    a = np.random.rand(3, 3)
    b = np.random.rand(3, 3)

    c = np.matmul(a, b)
    c.shape
    # Result:
    # (3, 3)
</code></pre>
<p>As we saw in the previous subsection, a <kbd>ufunc</kbd> broadcasts the operation over arrays of <em>scalars</em>, its natural generalization will&nbsp;be to broadcast over an array of <em>arrays</em>. If, for instance, we take two arrays of 3 by 3 matrices, we will expect <kbd>np.matmul</kbd> to take to match the matrices and take their product. In the following example, we take two arrays containing 10 matrices of shape <kbd>(3, 3)</kbd>. If we apply <kbd>np.matmul</kbd>, the product will be applied&nbsp;<em>matrix-wise</em> to obtain a new array containing the 10 results (which are, again, <kbd>(3, 3)</kbd> matrices):</p>
<pre><code class="lang-python">    a = np.random.rand(10, 3, 3)
    b = np.random.rand(10, 3, 3)

    c = np.matmul(a, b)
    c.shape
    # Output
    # (10, 3, 3)
</code></pre>
<p>The usual rules for broadcasting will work in a similar way. For example, if we have an array of <kbd>(3, 3)</kbd> matrices, which will have a shape of <kbd>(10, 3, 3)</kbd>, we can use <kbd>np.matmul</kbd> to calculate the matrix multiplication of each element with a single <kbd>(3, 3)</kbd> matrix. According to the broadcasting rules, we obtain that the single matrix will be repeated to obtain a size of <kbd>(10, 3, 3)</kbd>:</p>
<pre><code class="lang-python">    a = np.random.rand(10, 3, 3)
    b = np.random.rand(3, 3) # Broadcasted to shape (10, 3, 3)
    c = np.matmul(a, b)
    c.shape
    # Result:
    # (10, 3, 3)
</code></pre>
<p>Numba supports the implementation of efficient generalized universal functions through the <kbd>nb.guvectorize</kbd>&nbsp;decorator. As an example, we will implement a function that computes the euclidean distance between two arrays as a <kbd>gufunc</kbd>. To create a <kbd>gufunc</kbd>, we have to define a function that takes the input arrays, plus an output array where we will store the result of our calculation.</p>
<p>The <kbd>nb.guvectorize</kbd> decorator requires two arguments:</p>
<ul>
<li>The types of the input and output: two 1D arrays as input and a scalar as output</li>
<li>The so called layout string, which is a representation of the input and output sizes; in our case, we take two arrays of the same size (denoted arbitrarily by <kbd>n</kbd>), and we output a scalar</li>
</ul>
<p>In the following example, we show the implementation of the <kbd>euclidean</kbd> function using the <kbd>nb.guvectorize</kbd> decorator:</p>
<pre><code class="lang-python">    @nb.guvectorize(['float64[:], float64[:], float64[:]'], '(n), (n) -
    &gt; ()')
    def euclidean(a, b, out):
        N = a.shape[0]
        out[0] = 0.0
        for i in range(N):
            out[0] += (a[i] - b[i])**2
</code></pre>
<p>There are a few very important points to be made. Predictably, we declared the types of the inputs <kbd>a</kbd> and <kbd>b</kbd> as <kbd>float64[:]</kbd>, because they are 1D arrays. However,&nbsp;what about the output argument? Wasn't it supposed to be a scalar? Yes, however, <strong>Numba treats scalar argument as arrays of size 1</strong>. That's why it was declared as <kbd>float64[:]</kbd>.</p>
<p>Similarly, the layout string indicates that we have two arrays of size <kbd>(n)</kbd> and the output is a scalar, denoted by empty brackets--<kbd>()</kbd>. However, the array out will be passed as an array of size 1.</p>
<p>Also, note that we don't return anything from the function; all the output has to be written in the <kbd>out</kbd> array.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>The letter <kbd>n</kbd> in the layout string is completely arbitrary; you may choose to use <kbd>k</kbd>&nbsp; or other letters of your liking. Also, if you want to combine arrays of uneven sizes, you can use layouts strings, such as <kbd>(n, m)</kbd>.</p>
</div>
</div>
<p>Our brand new <kbd>euclidean</kbd> function can be conveniently used on arrays of different shapes, as shown in the following example:</p>
<pre><code class="lang-python">    a = np.random.rand(2)
    b = np.random.rand(2)
    c = euclidean(a, b) # Shape: (1,)

    a = np.random.rand(10, 2)
    b = np.random.rand(10, 2)
    c = euclidean(a, b) # Shape: (10,)

    a = np.random.rand(10, 2)
    b = np.random.rand(2)
    c = euclidean(a, b) # Shape: (10,)
</code></pre>
<p>How does the speed of <kbd>euclidean</kbd> compare to standard NumPy? In the following code, we benchmark a NumPy vectorized version with our previously defined <kbd>euclidean</kbd> function:</p>
<pre><code class="lang-python">    a = np.random.rand(10000, 2)
    b = np.random.rand(10000, 2)

    %timeit ((a - b)**2).sum(axis=1)
    1000 loops, best of 3: 288 &micro;s per loop

    %timeit euclidean(a, b)
    10000 loops, best of 3: 35.6 &micro;s per loop
</code></pre>
<p>The Numba version, again, beats the NumPy version by a large margin!</p>

<h3>JIT classes</h3>
<p>As of today, Numba doesn't support optimization of generic Python objects. This limitation, however, doesn't have a huge impact on numerical codes as they usually involve arrays and math operations exclusively.</p>
<p>Nevertheless, certain data structures are much more naturally implemented using objects; therefore, Numba provides support for defining classes that can be used and compiled to fast, native code.</p>
<p>Bear in mind that this is one of the newest (almost experimental) features, and it is extremely useful as it allows us to extend Numba to support fast data structures that are not easily implemented with arrays.</p>
<p>As an example, we will show how to implement a simple linked list using JIT classes. A linked list can be implemented by defining a <kbd>Node</kbd> class that contains two fields: a value and the next item in the list. As you can see in the following figure, each <strong>Node</strong> connects to the next and holds a value, and the last node contains a broken link, to which we assign a value of <strong>None</strong>:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000015.png" class="lazyload" /></p>
</div>
<p>In Python, we can define the <kbd>Node</kbd> class as follows:</p>
<pre><code class="lang-python">    class Node:
        def __init__(self, value):
            self.next = None
            self.value = value
</code></pre>
<p>We can manage the collection of <kbd>Node</kbd> instances by creating another class, called <kbd>LinkedList</kbd>. This class will keep track of the head of the list (in the preceding figure, this corresponds to the <strong>Node</strong> with <strong>value</strong> <strong>3</strong>). To insert an element in the front of the list, we can simply create a new <strong>Node</strong> and link it to the current head.</p>
<p>In the following code, we develop the initialization function for <kbd>LinkedList</kbd> and the <kbd>LinkedList.push_back</kbd> method that inserts an element in the front of the list using the strategy outlined earlier:</p>
<pre><code class="lang-python">    class LinkedList:
    
        def __init__(self):
            self.head = None
    
        def push_front(self, value):
            if self.head == None:
                self.head = Node(value)
            else:
                # We replace the head
                new_head = Node(value)
                new_head.next = self.head
                self.head = new_head
</code></pre>
<p>For debugging purposes, we can also implement the <kbd>LinkedList.show</kbd> method that traverses and prints each element in the list. The method is shown in the following snippet:</p>
<pre><code class="lang-python">        def show(self):
            node = self.head
            while node is not None:
                print(node.value)
                node = node.next
</code></pre>
<p>At this point, we can test our <kbd>LinkedList</kbd> and see whether&nbsp;it behaves correctly. We can create an empty list, add a few elements, and print its content. Note that since we are pushing elements at the front of the list, the last elements inserted will be the first to be printed:</p>
<pre><code class="lang-python">    lst = LinkedList()
    lst.push_front(1)
    lst.push_front(2)
    lst.push_front(3)
    lst.show()
    # Output:
    # 3
    # 2
    # 1
</code></pre>
<p>Finally, we can implement a function,&nbsp;<kbd>sum_list</kbd>, that returns the sum of the elements in the linked list. We will use this method to time differences between the Numba and pure Python version:</p>
<pre><code class="lang-python">    @nb.jit
    def sum_list(lst):
        result = 0
        node = lst.head
        while node is not None:
            result += node.value
            node = node.next
        return result
</code></pre>
<p>If we measure the execution time of the original <kbd>sum_list</kbd> version and the <kbd>nb.jit</kbd> version, we see that there is not much difference. The reason is that Numba cannot infer the type of classes:</p>
<pre><code class="lang-python">    lst = LinkedList()
    [lst.push_front(i) for i in range(10000)]

    %timeit sum_list.py_func(lst)
    1000 loops, best of 3: 2.36 ms per loop

    %timeit sum_list(lst)
    100 loops, best of 3: 1.75 ms per loop
</code></pre>
<p>We can improve the performance of <kbd>sum_list</kbd> by compiling the <kbd>Node</kbd> and <kbd>LinkedList</kbd> classes using the <kbd>nb.jitclass</kbd> decorator.</p>
<p>The <kbd>nb.jitclass</kbd> decorator takes a single argument that contains the attribute types. In the <kbd>Node</kbd> class, the attribute types are <kbd>int64</kbd> for <kbd>value</kbd>&nbsp;and <kbd>Node</kbd> for <kbd>next</kbd>. The <kbd>nb.jitclass</kbd> decorator will also compile all the methods defined for the class. Before delving into the code, there are two observations that need to be made.</p>
<p>First, the attribute declaration has to be done before the class is defined, but how do we declare a type we haven't defined yet? Numba provides the&nbsp;<kbd>nb.deferred_type()</kbd> function, which can be used for this purpose.</p>
<p>Second, the <kbd>next</kbd> attribute can be either <kbd>None</kbd> or a <kbd>Node</kbd> instance. This is what is called an optional type, and Numba provides a utility, called <kbd>nb.optional</kbd>, that lets you declare variables that can be (optionally) <kbd>None</kbd>.</p>
<p>This&nbsp;<kbd>Node</kbd> class is illustrated in the following code sample. As you can see,&nbsp; <kbd>node_type</kbd> is predeclared using <kbd>nb.deferred_type()</kbd>. The attributes are declared as a list of pairs containing the attribute name and the type (also note the use of <kbd>nb.optional</kbd>). After the class declaration, we are required to declare the deferred type:</p>
<pre><code class="lang-python">    node_type = nb.deferred_type()

    node_spec = [
        ('next', nb.optional(node_type)),
        ('value', nb.int64)
    ]

    @nb.jitclass(node_spec)
    class Node:
        # Body of Node is unchanged

    node_type.define(Node.class_type.instance_type)
</code></pre>
<p>The <kbd>LinkedList</kbd> class can be easily compiled, as follows. All that's needed is to define the <kbd>head</kbd> attribute and to apply the <kbd>nb.jitclass</kbd> decorator:</p>
<pre><code class="lang-python">    ll_spec = [
        ('head', nb.optional(Node.class_type.instance_type))
    ]

    @nb.jitclass(ll_spec)
    class LinkedList:
        # Body of LinkedList is unchanged


</code></pre>
<p>We can now measure the execution time of the <kbd>sum_list</kbd> function when we pass a JIT <kbd>LinkedList</kbd>:</p>
<pre><code class="lang-python">    lst = LinkedList()
    [lst.push_front(i) for i in range(10000)]

    %timeit sum_list(lst)
    1000 loops, best of 3: 345 &micro;s per loop

    %timeit sum_list.py_func(lst)
    100 loops, best of 3: 3.36 ms per loop
</code></pre>
<p>Interestingly, when using a JIT class from a compiled function, we obtain a substantial performance improvement against the pure Python version. However, using the JIT class from the original <kbd>sum_list.py_func</kbd>&nbsp;actually results in worse performance. Ensure that you use JIT classes only inside compiled functions!</p>

<h3>Limitations in Numba</h3>
<p>There are some instances where Numba cannot properly infer the variable types and will refuse to compile. In the following example, we define a function that takes a nested list of integers and returns the sum of the element in every sublist. In this case, Numba will raise <kbd>ValueError</kbd> and refuse to compile:</p>
<pre><code class="lang-python">    a = [[0, 1, 2], 
         [3, 4], 
         [5, 6, 7, 8]]

    @nb.jit
    def sum_sublists(a):
        result = []
        for sublist in a:
            result.append(sum(sublist))
        return result

    sum_sublists(a)
    # ValueError: cannot compute fingerprint of empty list
</code></pre>
<p>The problem with this code is that Numba is not able to determine the type of the list and fails. A way to fix this problem is to help the compiler determine the right type by initializing the list with a sample element and removing it at the end:</p>
<pre><code class="lang-python">    @nb.jit
    def sum_sublists(a):
        result = [0]
        for sublist in a:
            result.append(sum(sublist))
        return result[1:]
</code></pre>
<p>Among other features that are not yet implemented in the Numba compiler are function and class definitions, list, set and dict comprehension, generators, the <kbd>with</kbd> statement, and <kbd>try</kbd> <kbd>except</kbd> blocks. Note, however, that many of these features may become supported in the future.</p>

<h2>The PyPy project</h2>
<p>PyPy is a very ambitious project at improving the performance of the Python interpreter. The way PyPy improves performance is by automatically compiling slow sections of the code at runtime.</p>
<p>PyPy is written in a special language called RPython (rather than C) that allows developers to quickly and reliably implement advanced features and improvements. RPython means <em>Restricted Python</em> because it implements a restricted subset of the Python language targeted to the compiler development.</p>
<p>As of today, PyPy version 5.6 supports a lot of Python features and is a possible choice for a large variety of applications.</p>
<p>PyPy compiles code using a very clever strategy, called <em>tracing JIT compilation</em>. At first, the code is executed normally using interpreter calls. PyPy then starts to profile the code and identifies the most intensive loops. After the identification takes place, the compiler then observes (<em>traces</em>) the operations and is able to compile its optimized, interpreter-free version.</p>
<p>Once an optimized version of the code is present, PyPy is able to run the slow loop much faster than the interpreted version.</p>
<p>This strategy can be contrasted with what Numba does. In Numba, the units of compilation are methods and functions, while the PyPy focus is&nbsp;just slow loops. Overall, the focus of the projects is also very different as Numba has a limited scope for numerical code and requires a lot of instrumentation while PyPy aims at replacing the CPython interpreter.</p>
<p>In this section, we will demonstrate and benchmark PyPy on our particle simulator application.</p>

<h3>Setting up PyPy</h3>
<p>PyPy is distributed as a precompiled binary that can be downloaded from <a href="http://pypy.org/download.html">http://pypy.org/download.html</a>, and it currently supports Python versions 2.7 (beta support in PyPy 5.6) and 3.3 (alpha support in PyPy 5.5). In this chapter, we will demonstrate the usage of the 2.7 version.</p>
<p>Once PyPy is downloaded and unpacked, you can locate the interpreter in the <kbd>bin/pypy</kbd>&nbsp;directory relative to the unpacked archive. You can initialize a new virtual environment where we can install additional packages using the following command:</p>
<pre><code class="lang-python">$ /path/to/bin/pypy -m ensurepip
$ /path/to/bin/pypy -m pip install virtualenv
$ /path/to/bin/virtualenv my-pypy-env
</code></pre>
<p>To activate the environment, we will use the following command:</p>
<pre><code class="lang-python">$ source my-pypy-env/bin/activate

</code></pre>
<p>At this point, you can verify that the binary Python is linked to the PyPy executable by typing <kbd>python -V</kbd>. At this point, we can go ahead and install some packages we may need. As of version 5.6, PyPy has limited support for software that uses the Python C API (most notably, packages such as <kbd>numpy</kbd> and <kbd>matplotlib</kbd>). We can go ahead and install them in the usual way:</p>
<pre><code class="lang-python">(my-pypy-env) $ pip install numpy matplotlib
</code></pre>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>On certain platforms, installation of <kbd>numpy</kbd> and <kbd>matplotlib</kbd> can be tricky. You can skip the installation step and remove any imports on these two packages from the scripts we will&nbsp;run.</p>
</div>
</div>

<h3>Running a particle simulator in PyPy</h3>
<p>Now that we have successfully set up the PyPy installation, we can go ahead and run our particle simulator. As a first step, we will&nbsp;time the particle simulator from Chapter 1,&nbsp;<em>Benchmarking and Profiling</em>, on the standard Python interpreter. If the virtual environment is still active, you can issue the command deactivate to exit the environment. We can confirm that the Python interpreter is the standard one by using the <kbd>python -V</kbd> command:</p>
<pre><code class="lang-python">(my-pypy-env) $ deactivate
$ python -V
Python 3.5.2 :: Continuum Analytics, Inc.
</code></pre>
<p>At this point, we can time our code using the <kbd>timeit</kbd> command-line interface:</p>
<pre><code class="lang-python">$ python -m timeit --setup "from simul import benchmark" "benchmark()"
10 loops, best of 3: 886 msec per loop
</code></pre>
<p>We can reactivate the environment and run the exact same code from PyPy. On Ubuntu, you may have problems importing the <kbd>matplotlib.pyplot</kbd> module. You can try issuing the following <kbd>export</kbd> command to fix the issue or removing the <kbd>matplotlib</kbd> imports from <kbd>simul.py</kbd>:</p>
<pre><code class="lang-python">$ export MPLBACKEND='agg'
</code></pre>
<p>Now, we can go ahead and time the code using PyPy:</p>
<pre><code class="lang-python">$ source my-pypy-env/bin/activate
Python 2.7.12 (aff251e54385, Nov 09 2016, 18:02:49)
[PyPy 5.6.0 with GCC 4.8.2]

(my-pypy-env) $ python -m timeit --setup "from simul import benchmark" "benchmark()"
WARNING: timeit is a very unreliable tool. use perf or something else for real measurements
10 loops, average of 7: 106 +- 0.383 msec per loop (using standard deviation)
</code></pre>
<p>Note that we obtained a large, more than eight times, speedup! PyPy, however, warns us that the <kbd>timeit</kbd> module can be unreliable. We can confirm our timings using the <kbd>perf</kbd> module, as suggested by PyPy:</p>
<pre><code class="lang-python">(my-pypy-env) $ pip install perf
(my-pypy-env) $ python -m perf timeit --setup 'from simul import benchmark' 'benchmark()'
.......
Median +- std dev: 97.8 ms +- 2.3 ms
</code></pre>

<h2>Other interesting projects</h2>
<p>Over the years, many projects attempted to improve Python performance through several strategies and, sadly, many of them failed. As of today, there are a few projects that survive and hold the promise for a faster Python.</p>
<p>Numba and PyPy are mature projects that are steadily improving over the years. Features are continuously being added and they hold great promise for the future of Python.</p>
<p><strong>Nuitka</strong> is a program developed by Kay Hayen that compiles Python code to C. As of right now (version 0.5.x), it provides extreme compatibility with the Python language and produces efficient code that results in moderate performance improvements over CPython.</p>
<p>Nuitka is quite different than Cython in the sense that it focuses on extreme compatibility with the Python language, and it doesn't extend the language with additional constructs.</p>
<p><strong>Pyston</strong> is a new interpreter developed by Dropbox that powers JIT compilers. It differs substantially from PyPy as it doesn't employ a tracing JIT, but rather a method-at-a-time JIT (similar to what Numba does). Pyston, like Numba, is also built on top of the LLVM compiler infrastructure.</p>
<p>Pyston is still in early development (alpha stage) and only supports Python 2.7. Benchmarks show that it is faster than CPython but slower than PyPy; that said, it is still an interesting project to follow as new features are added and compatibility is increased.</p>

<h2>Summary</h2>
<p>Numba is a tool that compiles fast, specialized versions of Python functions at runtime. In this chapter, we learned how to compile, inspect, and analyze functions compiled by Numba. We also learned how to implement fast NumPy universal functions that are useful in a wide array of numerical applications. Finally, we implemented more complex data structures using the <kbd>nb.jitclass</kbd> decorator.</p>
<p>Tools such as PyPy allow us to run Python programs unchanged to obtain significant speed improvements. We demonstrated how to set up PyPy, and we assessed the performance improvements on our particle simulator application.</p>
<p>We also, briefly, described the current ecosystem of the Python compilers and compared them with each other.&nbsp;</p>
<p>In the next chapter, we will learn about concurrency and asynchronous programming. Using these techniques, we will be able to improve the responsiveness&nbsp;and design&nbsp;of applications that spend a lot of time waiting for network and disk resources.&nbsp;</p>


</div>


<!--Chapter 6-->


<div class="chapter" data-chapter-number="6">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 6 </span></div>
<h1 class="chaptertitle">Implementing Concurrency</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>So far, we have explored how to measure and improve the performance of programs by reducing the number of operations performed by the CPU through clever algorithms and more efficient machine code. In this chapter, we will shift our focus to programs where most of the time is spent waiting for resources that are much slower than the CPU, such as persistent storage and network resources.</p>
<p>Asynchronous programming is a programming paradigm that helps to deal with slow and unpredictable resources (such as users) and is widely used to build responsive services and user interfaces. In this chapter, we will show you how to program asynchronously in Python using techniques such as coroutines and reactive programming.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>The memory hierarchy</li>
<li>Callbacks</li>
<li>Futures</li>
<li>Event loops</li>
<li>Writing coroutines with <kbd>asyncio</kbd></li>
<li>Converting synchronous code to asynchronous code</li>
<li>Reactive programming with RxPy</li>
<li>Working with observables</li>
<li>Building a memory monitor with RxPY</li>
</ul>

<h2>Asynchronous programming</h2>
<p>Asynchronous programming is a way of dealing with slow and unpredictable resources. Rather than waiting idle for resources to become available, asynchronous programs are able to handle multiple resources concurrently and efficiently. Programming in an asynchronous way can be challenging because it is necessary&nbsp;to deal with external requests that can arrive in any order, may take a variable amount of time, or may fail unpredictably. In this section, we will introduce the topic by explaining the main concepts and terminology as well as by giving an idea of how asynchronous programs work.</p>

<h3>Waiting for I/O</h3>
<p>A modern computer employs different kinds of memory to store data and perform operations. In general, a computer possesses a combination of expensive memory that is capable of operating at fast speeds and cheaper, and more abundant memory that operates at lower speeds and is used to store a larger amount of data.</p>
<p>The memory hierarchy is shown in the following diagram:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000027.png" class="lazyload" /></p>
</div>
<p>At the top of the memory hierarchy are the CPU registers. Those are integrated in the CPU and are used to store and execute machine instructions. Accessing data in a register generally takes one clock cycle. This means that if the CPU operates at 3 GHz, the time it takes to access one element in a CPU register is in the order of 0.3 nanoseconds.</p>
<p>At the layer just below the <strong>registers</strong>, you can find the CPU cache, which is comprised of multiple levels and is integrated in the processor. The <strong>cache</strong> operates at a slightly slower speed than the <strong>registers</strong> but within the same order of magnitude.</p>
<p>The next item in the hierarchy is the main memory (<strong>RAM</strong>), which holds much more data but is slower than the cache. Fetching an item from memory can take a few hundred clock cycles.</p>
<p>At the bottom layer, you can find persistent storage, such as a rotating disks (HDD) and <strong>Solid State Drives</strong> (<strong>SSD</strong>). These devices hold the most data and are orders of magnitude slower than the main memory. An HDD may take a few milliseconds to seek and retrieve an item, while an SSD is substantially faster and takes only a fraction of a millisecond.</p>
<p>To put the relative speed of each memory type into perspective, if you were to have the CPU with a clock speed of about one&nbsp;second, a register access would be equivalent to picking up a pen from the table. A cache access will be equivalent to picking up a book from the shelf. Moving higher in the hierarchy, a RAM access will&nbsp;be equivalent to&nbsp;loading up the laundry (about twenty x slower than the cache). When we move to persistent storage, things are quite a bit different. Retrieving an element from an SSD will&nbsp;be equivalent to doing a four&nbsp;day trip, while retrieving an element from an HDD can take up to six&nbsp;months! The times can stretch even further if we move on to access resources over the network.</p>
<p>From the preceding&nbsp;example, it should be clear that accessing data from storage and other I/O devices is much slower compared to the CPU; therefore, it is very important to handle those resources so that the CPU is never stuck waiting aimlessly. This can be accomplished by carefully designing software capable of managing multiple, ongoing requests at the same time.</p>

<h3>Concurrency</h3>
<p>Concurrency is a way to implement a&nbsp;system that is able to deal with multiple requests at the same time. The idea is that we can move on and start handling other resources while we wait for a resource to become available. Concurrency works by splitting a task into smaller subtasks that&nbsp;can be executed out of order so that multiple tasks can be partially advanced without waiting for the previous tasks to finish. &nbsp;</p>
<p>As a first example, we will describe how to implement concurrent&nbsp;access to a slow network resource. Let's say we have a web service that takes the square of a number, and the time between our request and the response will&nbsp;be approximately one second. &nbsp;We can implement the&nbsp;<kbd>network_request</kbd> function that takes a number and returns a dictionary that contains information about the success of the operation and the result.&nbsp;We can simulate such services using the <kbd>time.sleep</kbd> function, as follows:</p>
<pre><code class="lang-python">    import time

    def network_request(number):
        time.sleep(1.0)
        return {"success": True, "result": number ** 2}
</code></pre>
<p>We will also write some additional&nbsp;code that performs the request, verifies that the request was successful, and prints the result. In the following code, we define the <kbd>fetch_square</kbd> function and use it to calculate the square of the number two&nbsp;using a call to <kbd>network_request</kbd>:</p>
<pre><code class="lang-python">    def fetch_square(number):
        response = network_request(number)
        if response["success"]:
            print("Result is: {}".format(response["result"]))

    fetch_square(2)
    # Output:
    # Result is: 4
</code></pre>
<p>Fetching a number from the network will&nbsp;take one second because of the slow network. What if we want to calculate the square of multiple numbers? We can call <kbd>fetch_square</kbd>, which will start a network request as soon as the previous one is done:</p>
<pre><code class="lang-python">    fetch_square(2)
    fetch_square(3)
    fetch_square(4)
    # Output:
    # Result is: 4
    # Result is: 9
    # Result is: 16
</code></pre>
<p>The previous code will take three seconds to run, but it's not the best we can do. Waiting for the previous result to finish is unnecessary as we can technically submit multiple requests at and wait for them parallely.</p>
<p>In the following diagram, the three tasks are represented as boxes.&nbsp;The&nbsp;time spent by the CPU processing and submitting the request is in orange while the&nbsp;waiting times are in blue. You can see how most of the time is spent waiting for the resources while our machine sits idle without doing anything else:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000003.png" class="lazyload" /></p>
</div>
<p>Ideally, we would like to start other new task while we are waiting for the already submitted tasks to finish. In the following figure,&nbsp;you can see that as soon as we submit our request in <strong>fetch_square(2)</strong>, we can&nbsp;start preparing&nbsp;for <strong>fetch_square(3)</strong> and so on. This allows us to reduce the CPU waiting time and to start processing the results as soon as they become available:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000016.png" class="lazyload" /></p>
</div>
<p>This strategy is made possible by the fact that the three requests are completely independent, and we don't need to wait for the completion of a previous&nbsp;task to start the next one. Also, note how a single CPU can comfortably handle this scenario. While distributing the work on multiple CPUs can further speedup the execution, if the waiting time is large compared to the processing times, the speedup will be minimal.</p>
<p>To implement concurrency, it is necessary to think and code differently; in the following sections, we'll demonstrate techniques and best practices to implement robust concurrent applications.</p>

<h3>Callbacks</h3>
<p>The code we have seen so far blocks the execution of the program until the resource is available. The call responsible for the waiting is <kbd>time.sleep</kbd>. To make the code start working on other tasks, we need to find a way to avoid blocking the program flow so that the rest of the program can go on with the other tasks.</p>
<p>One of the simplest ways to accomplish this behavior is through callbacks. The strategy is quite similar to what we do when we request a cab.</p>
<p>Imagine that you are at a restaurant and you've had a few drinks. It's raining outside, and you'd rather not take the bus; therefore, you request a taxi and ask them to call when they're outside so that you can come out, and you don't have to wait in the rain.</p>
<p>What you did in this case is request a taxi (that is, the slow resource) but instead of waiting outside until the taxi arrives, you provide your number and instructions (callback) so that you can come outside when they're ready and go home.</p>
<p>We will now show how this mechanism can work in code.&nbsp;We will compare the blocking code of <kbd>time.sleep</kbd> with the equivalent non-blocking code of <kbd>threading.Timer</kbd>.</p>
<p>For this example, we will write a function,&nbsp;<kbd>wait_and_print</kbd>, that will block the program execution for one second and then print a message:</p>
<pre><code class="lang-python">    def wait_and_print(msg):
        time.sleep(1.0)
        print(msg)
</code></pre>
<p>If we want to write the same function in a non-blocking way, we can&nbsp;use the <kbd>threading.Timer</kbd> class. We can initialize a <kbd>threading.Timer</kbd>&nbsp;instance by passing the amount of time we want to wait and a callback. A <strong>callback</strong> is simply a function that will be called when the timer expires. Note that we have to also call the <kbd>Timer.start</kbd> method to activate the timer:</p>
<pre><code class="lang-python">    import threading

    def wait_and_print_async(msg):
        def callback():
            print(msg)

        timer = threading.Timer(1.0, callback)
        timer.start()
</code></pre>
<p>An important feature&nbsp;of the <kbd>wait_and_print_async</kbd> function is that none of the statements are blocking the execution flow of the program.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>How is <kbd>threading.Timer</kbd> capable of waiting without blocking?<br />The strategy used by <kbd>threading.Timer</kbd> involves starting a new thread that&nbsp;is able to execute code in parallel. If this is confusing, don't worry, we will explore threading and parallel programming in detail in the following chapters.</p>
</div>
</div>
<p>This technique of registering callbacks for execution in response to certain events is commonly called the <em>Hollywood principle</em>. This is because, after an audition for a role at Hollywood, you may be told "<q>Don't call us, we'll call you</q>", meaning that they won't tell you if they chose you for the role immediately, but they'll call you in case they do.</p>
<p>To highlight the&nbsp;difference between the blocking and non-blocking version of <kbd>wait_and_print</kbd>, we can test and compare the execution of the two versions. In the output comments, the waiting periods are indicated by <kbd>&lt;wait...&gt;</kbd>:</p>
<pre><code class="lang-python">    # Syncronous
    wait_and_print("First call")
    wait_and_print("Second call")
    print("After call")
    # Output:
    # &lt;wait...&gt;
    # First call  
    # &lt;wait...&gt;
    # Second call
    # After call
    # Async
    wait_and_print_async("First call async")
    wait_and_print_async("Second call async")
    print("After submission")
    # Output:
    # After submission 
    # &lt;wait...&gt;
    # First call
    # Second call
</code></pre>
<p>The synchronous version behaves in a very familiar way. The code waits for a second, prints <kbd>First call</kbd>, waits for another second, and then prints the&nbsp;<kbd>Second call</kbd>&nbsp;and <kbd>After call</kbd>&nbsp;messages.</p>
<p>In the asynchronous version, <kbd>wait_and_print_async</kbd>&nbsp;<em>submits&nbsp; (</em>rather than <em>execute</em>)<em>&nbsp;</em> those calls and moves on <em>immediately</em>. You can see this mechanism in action&nbsp;by acknowledging that the <kbd>"After submission"</kbd> message is printed immediately.</p>
<p>With this in mind, we can explore a slightly more complex situation by rewriting our <kbd>network_request</kbd> function using callbacks. In the following code, we define the <kbd>network_request_async</kbd>&nbsp;function. The biggest difference between <kbd>network_request_async</kbd>&nbsp;and its blocking counterpart is that <kbd>network_request_async</kbd> <em>doesn't return anything</em>. This is because we are merely submitting the request when <kbd>network_request_async</kbd> is called, but the value is available only when the request is completed.</p>
<p>If we can't return anything, how do we pass the result of the request? Rather than returning the value, we will pass the result as an argument to the <kbd>on_done</kbd> callback.</p>
<p>The rest of the function consists of submitting a callback (called <kbd>timer_done</kbd>) to the <kbd>timer.Timer</kbd> class that will call <kbd>on_done</kbd> when it's ready:</p>
<pre><code class="lang-python">    def network_request_async(number, on_done):

        def timer_done():
            on_done({"success": True, 
                     "result": number ** 2})

        timer = threading.Timer(1.0, timer_done)
        timer.start()
</code></pre>
<p>The usage of <kbd>network_request_async</kbd> is quite similar to <kbd>timer.Timer</kbd>; all we have to do is pass the number we want to square and a callback that will receive the result <em>when it's ready</em>. This is demonstrated in the following snippet:</p>
<pre><code class="lang-python">    def on_done(result):
        print(result)

    network_request_async(2, on_done)
</code></pre>
<p>Now, if we submit&nbsp;multiple network requests, we note that the calls get executed concurrently and do not block the code:</p>
<pre><code class="lang-python">    network_request_async(2, on_done)
    network_request_async(3, on_done)
    network_request_async(4, on_done)
    print("After submission")
</code></pre>
<p>In order to use <kbd>network_request_async</kbd> in <kbd>fetch_square</kbd>, we need to adapt the code to use asynchronous constructs. In the following code, we modify <kbd>fetch_square</kbd> by defining and passing the <kbd>on_done</kbd> callback to <kbd>network_request_async</kbd>:</p>
<pre><code class="lang-python">    def fetch_square(number):
        def on_done(response):
            if response["success"]:
                print("Result is: {}".format(response["result"]))

        network_request_async(number, on_done)
</code></pre>
<p>You may have noted that the asynchronous code is significantly more convoluted than its synchronous&nbsp;counterpart. This is due to the fact that we are required to write and pass a callback every time we need to retrieve a certain result, causing the code to become nested and hard to follow.</p>

<h3>Futures</h3>
<p>Futures are a more convenient pattern that can be used to keep track of the results of asynchronous calls. In the preceding code, we saw that rather than returning values, we accept callbacks and pass the results when they are ready. It is interesting to note that, so far, there is no easy way to track the status of the resource.</p>
<p>A <strong>future</strong> is an abstraction that helps us keep track of the requested resources and that we are waiting to become available. In Python, you can find a future implementation in the <kbd>concurrent.futures.Future</kbd> class. A <kbd>Future</kbd> instance can be&nbsp;created by calling its constructor with no arguments:</p>
<pre><code class="lang-python">    fut = Future()
    # Result:
    # &lt;Future at 0x7f03e41599e8 state=pending&gt;
</code></pre>
<p>A future represents a value that is not yet available. You can see that its string&nbsp;representation reports the current status of the result which, in our case, is still pending. In order to make a result available, we can use the <kbd>Future.set_result</kbd>&nbsp;method:</p>
<pre><code class="lang-python">    fut.set_result("Hello")
    # Result:
    # &lt;Future at 0x7f03e41599e8 state=finished returned str&gt;

    fut.result()
    # Result:
    # "Hello"
</code></pre>
<p>You can see that once we set the result, the <kbd>Future</kbd> will report that the task is finished and can be accessed using the <kbd>Future.result</kbd>&nbsp;method. It is also possible to subscribe a callback to a future so that, as soon as the result is available, the callback is executed. To attach a callback, it is sufficient to pass a function to the <kbd>Future.add_done_callback</kbd>&nbsp;method. When the task completes, the function&nbsp;will be called with the <kbd>Future</kbd> instance as its first argument and the result can be retrieved using the <kbd>Future.result()</kbd>&nbsp;method:</p>
<pre><code class="lang-python">    fut = Future()
    fut.add_done_callback(lambda future: print(future.result(), flush=True))
    fut.set_result("Hello")
    # Output:
    # Hello
</code></pre>
<p>To get a grasp on how futures can be used in practice, we will adapt the <kbd>network_request_async</kbd> function to use futures. The idea is that, this time, instead of returning nothing, we return a <kbd>Future</kbd> that will keep track of the result for us. Note two things:</p>
<ul>
<li>We don't need to accept an&nbsp;<kbd>on_done callback</kbd>&nbsp;as callbacks can be connected later using the <kbd>Future.add_done_callback</kbd> method. Also, we pass the generic <kbd>Future.set_result</kbd> method as the callback for&nbsp;<kbd>threading.Timer</kbd>.</li>
<li>This time we are able to return a value, thus making the code a bit more similar to the blocking version we saw in the preceding&nbsp;section:</li>
</ul>
<pre><code class="lang-python">    from concurrent.futures import Future

    def network_request_async(number):
        future = Future()
        result = {"success": True, "result": number ** 2}
        timer = threading.Timer(1.0, lambda: future.set_result(result))
        timer.start()
        return future

    fut = network_request_async(2)
</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Even though we instantiate and manage&nbsp;futures directly in these examples; in practical applications, the futures are handled by frameworks.&nbsp;</p>
</div>
</div>
<p>If you execute the preceding&nbsp;code, nothing will happen as the code only consists of&nbsp;preparing and returning a <kbd>Future</kbd> instance. To enable further operation of the future results, we need to use the <kbd>Future.add_done_callback</kbd>&nbsp;method. In the following code, we adapt the <kbd>fetch_square</kbd> function to use futures:</p>
<pre><code class="lang-python">    def fetch_square(number):
        fut = network_request_async(number)

        def on_done_future(future):
            response = future.result()
            if response["success"]:
                print("Result is: {}".format(response["result"]))
        
        fut.add_done_callback(on_done_future)
</code></pre>
<p>The code still looks quite similar to the callback version. Futures are a different and slightly more convenient way of working with callbacks. Futures are also advantageous, because they can&nbsp;keep track of the&nbsp;resource status, cancel (unschedule) scheduled tasks, and&nbsp;handle exceptions more naturally.</p>

<h3>Event loops</h3>
<p>So far, we have implemented parallelism&nbsp;using OS&nbsp;threads. However, in many asynchronous frameworks, the coordination of concurrent tasks is managed by an <strong>event loop</strong>.</p>
<p>The idea behind an event loop is to continuously monitor the status of the various resources (for example,&nbsp;network connections and database queries) and trigger the execution of callbacks when events take place (for example, when a resource is ready or when a timer expires).</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Why not just stick to&nbsp;threading?<br />Events loops are sometimes preferred as every unit of execution never runs at the same time as another and this can simplify dealing with shared variables, data structures,&nbsp;and resources.&nbsp;Read the next chapter for more details about parallel execution and its shortcomings.</p>
</div>
</div>
<p>As a first example, we will implement a thread-free version of <kbd>threading.Timer</kbd>. We can define a <kbd>Timer</kbd> class that will take a timeout and implement the <kbd>Timer.done</kbd>&nbsp;method that&nbsp;returns&nbsp;<kbd>True</kbd> if the timer has expired:</p>
<pre><code class="lang-python">    class Timer:
    
        def __init__(self, timeout):
            self.timeout = timeout
            self.start = time.time()
    
        def done(self):
            return time.time() - self.start &gt; self.timeout
</code></pre>
<p>To determine whether&nbsp;the timer has expired, we can write a loop that continuously checks the timer status by calling the <kbd>Timer.done</kbd> method. When the timer expires, we can print a message and exit&nbsp;the cycle:</p>
<pre><code class="lang-python">    timer = Timer(1.0)

    while True:
        if timer.done():
            print("Timer is done!")
            break
</code></pre>
<p>By implementing the timer in this way, the flow of execution is never blocked and we can, in principle, do other work inside the while loop.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Waiting for events to happen by continuously polling using a loop is commonly termed as&nbsp;<em>busy-waiting</em>.</p>
</div>
</div>
<p>Ideally, we would like to attach a custom function that executes when the timer goes off, just like we did in <kbd>threading.Timer</kbd>. To do this, we can implement a method, <kbd>Timer.on_timer_done</kbd>, that will accept a callback to&nbsp;be executed when the timer goes off:</p>
<pre><code class="lang-python">    class Timer:
       # ... previous code 
       def on_timer_done(self, callback):
            self.callback = callback
</code></pre>
<p>Note that <kbd>on_timer_done</kbd> merely stores&nbsp;a reference to the callback. The entity that monitors the event and executes the callback is the loop. This concept is demonstrated as follows. Rather than using the print function, the loop will call <kbd>timer.callback</kbd> when appropriate:</p>
<pre><code class="lang-python">    timer = Timer(1.0)
    timer.on_timer_done(lambda: print("Timer is done!"))

    while True:
        if timer.done():
            timer.callback()
            break
</code></pre>
<p>As you can see, an asynchronous framework is starting to take place. All we did outside the loop was define the timer and the callback, while the loop took care of monitoring the timer and executing the associated callback. We can further extend our code by implementing support for multiple timers.</p>
<p>A natural way to implement multiple timers is to add a few <kbd>Timer</kbd> instances to a list and modify our event loop to periodically check all the timers and dispatch the callbacks when required. In the following code, we define two timers and attach a callback to each of them. Those timers are added to a list, <kbd>timers</kbd>, that is continuously monitored by our event loop. As soon as a timer is done, we execute the callback and remove the event from the list:</p>
<pre><code class="lang-python">    timers = []

    timer1 = Timer(1.0)
    timer1.on_timer_done(lambda: print("First timer is done!"))

    timer2 = Timer(2.0)
    timer2.on_timer_done(lambda: print("Second timer is done!"))

    timers.append(timer1)
    timers.append(timer2)

    while True:
        for timer in timers:
            if timer.done():
                timer.callback()
                timers.remove(timer)
        # If no more timers are left, we exit the loop 
        if len(timers) == 0:
            break
</code></pre>
<p>The main restriction of&nbsp;an event loop is, since the flow of execution is managed by a continuously running loop, that it&nbsp;<strong>never uses blocking calls</strong>. If we use any blocking statement (such as <kbd>time.sleep</kbd>) inside the loop, you can imagine how the event monitoring and callback dispatching will stop until the blocking call is done.</p>
<p>To avoid this, rather than using a blocking call, such as <kbd>time.sleep</kbd>, we let the event loop detect and execute the callback when the resource is ready. By not blocking the execution flow, the event loop is free to monitor multiple resources in a concurrent way.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The notification for events is usually implemented through operating system calls (such as the <kbd>select</kbd>&nbsp;Unix tool) that will resume the execution of the program whenever an event is ready (in contrast to busy-waiting).</p>
</div>
</div>
<p>The Python standard libraries include a very convenient event loop-based concurrency framework, <kbd>asyncio</kbd>, which will be the topic of the next section.</p>

<h2>The asyncio framework</h2>
<p>By now, you should have a solid foundation of how concurrency works, and how to use callbacks and futures. We can now move on and learn how to use the <kbd>asyncio</kbd> package present in the standard library since version 3.4. We will also explore the brand new <kbd>async</kbd>/<kbd>await</kbd> syntax to deal with asynchronous programming in a very natural way.</p>
<p>As a first example, we will see how to retrieve and execute a simple callback using <kbd>asyncio</kbd>. The <kbd>asyncio</kbd> loop can be retrieved by calling the <kbd>asyncio.get_event_loop()</kbd>&nbsp;function. We can schedule a callback for execution using&nbsp; <kbd>loop.call_later</kbd> that takes a delay in seconds and a callback. We can also use the <kbd>loop.stop</kbd> method to halt the loop and exit the program.&nbsp; To start processing the scheduled call, it is necessary to start the loop, which&nbsp;can be done using <kbd>loop.run_forever</kbd>. The following example demonstrates the usage of these basic methods by scheduling a callback that will print a message and halt the loop:</p>
<pre><code class="lang-python">    import asyncio

    loop = asyncio.get_event_loop()

    def callback():
        print("Hello, asyncio")
        loop.stop()

    loop.call_later(1.0, callback)
    loop.run_forever()
</code></pre>

<h3>Coroutines</h3>
<p>One of the main problems with callbacks is that they require you to break the program execution into small functions that will be invoked when a certain event takes place. As we saw&nbsp;in the earlier&nbsp;sections, callbacks can quickly become cumbersome.</p>
<p>Coroutines are another, perhaps a more natural, way to break up the program execution into chunks. They allow the programmer to write code that resembles synchronous code but will execute asynchronously. You may think of a coroutine as&nbsp;a function that can be stopped and resumed. A&nbsp;basic example of coroutines is&nbsp;generators.</p>
<p>Generators can be defined in Python using the <kbd>yield</kbd> statement inside a function. In the following example, we implement the <kbd>range_generator</kbd>&nbsp;function, which&nbsp;produces and returns values from <kbd>0</kbd> to <kbd>n</kbd>. We also add a print statement to log the internal state of the generator:</p>
<pre><code class="lang-python">    def range_generator(n):
        i = 0
        while i &lt; n:
            print("Generating value {}".format(i))
            yield i
            i += 1
</code></pre>
<p>When we call the <kbd>range_generator</kbd>&nbsp;function, the code is not executed immediately. Note that nothing is printed to output when the following snippet is executed. Instead, a <em>generator object</em> is returned:</p>
<pre><code class="lang-python">    generator = range_generator(3)
    generator
    # Result:
    # &lt;generator object range_generator at 0x7f03e418ba40&gt;
</code></pre>
<p>In order to start pulling values from a generator, it is necessary to use the <kbd>next</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    next(generator)
    # Output:
    # Generating value 0

    next(generator)
    # Output:
    # Generating value 1
</code></pre>
<p>Note that every time we invoke <kbd>next</kbd>, the code runs until it encounters the next <kbd>yield</kbd> statement and it is necessary to issue another <kbd>next</kbd> statement to resume the generator execution. You can think of a <kbd>yield</kbd> statement as a breakpoint where we can stop and resume execution (while also maintaining the internal state of the generator). This ability of stopping and resuming execution can be leveraged by the event loop to allow for concurrency.&nbsp;</p>
<p>It is also possible to <em>inject</em> (rather than <em>extract)</em> values in the generator through the <kbd>yield</kbd> statement. In the following example, we declare a function parrot that will repeat each message that we send. To allow a generator to receive a value, you can assign yield to a variable (in our case, it is <kbd>message = yield</kbd>). To insert values in the generator, we can use the <kbd>send</kbd> method. In the Python world, a generator that can also receive values is called a&nbsp;<em>generator-based coroutine</em>:</p>
<pre><code class="lang-python">    def parrot():
        while True:
            message = yield
            print("Parrot says: {}".format(message))

    generator = parrot()
    generator.send(None)
    generator.send("Hello")
    generator.send("World")
</code></pre>
<p>Note that we also need to issue a <kbd>generator.send(None)</kbd> before we can start sending messages; this is done to bootstrap the function execution and bring us to the first <kbd>yield</kbd> statement. Also, note that there is an infinite loop inside <kbd>parrot</kbd>; if we implement this without using generators, we will&nbsp;get stuck running the loop forever!</p>
<p>With this in mind, you can imagine how an event loop can partially progress several of these generators without blocking the execution of the whole program. You can also imagine how a generator can&nbsp;be advanced only when some resource is ready, therefore eliminating the need for a callback.</p>
<p>It is possible to implement coroutines in <kbd>asyncio</kbd>&nbsp;using the <kbd>yield</kbd> statement. However, Python supports the definition of powerful coroutines using a more intuitive syntax since version 3.5.</p>
<p>To define a coroutine with <kbd>asyncio</kbd>, you can use the <kbd>async def</kbd> statement:</p>
<pre><code class="lang-python">    async def hello():
        print("Hello, async!")

    coro = hello()
    coro
    # Output:
    # &lt;coroutine object hello at 0x7f314846bd58&gt;
</code></pre>
<p>As you can see, if we call the <kbd>hello</kbd> function, the function body is not executed immediately, but a <em>coroutine object</em> is returned. The&nbsp;<kbd>asyncio</kbd> coroutines do not support <kbd>next</kbd>, but they can be easily run in the <kbd>asyncio</kbd> event loop using the <kbd>run_until_complete</kbd>&nbsp;method:</p>
<pre><code class="lang-python">    loop = asyncio.get_event_loop()
    loop.run_until_complete(coro)
</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Coroutines defined with the <kbd>async def</kbd> statement are also called <em>native coroutines</em>.</p>
</div>
</div>
<p>The <kbd>asyncio</kbd>&nbsp; module&nbsp;provides resources (called <em>awaitables</em>) that can be requested inside coroutines through the <kbd>await</kbd> syntax. For example, if we want to wait for a certain time and then execute a statement, we can use the <kbd>asyncio.sleep</kbd>&nbsp;function:</p>
<pre><code class="lang-python">    async def wait_and_print(msg):
        await asyncio.sleep(1)
        print("Message: ", msg)
    
    loop.run_until_complete(wait_and_print("Hello"))
</code></pre>
<p>The result is beautiful, clean code. We are writing perfectly functional asynchronous code without all the ugliness of callbacks!</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>You may have noted how <kbd>await</kbd> provides a breakpoint for the event loop so that, as it wait for the resource, the event loop can move on and concurrently manage other coroutines.</p>
</div>
</div>
<p>Even better, coroutines are also <kbd>awaitable</kbd>, and we can use the <kbd>await</kbd> statement to chain coroutines asynchronously. In the following example, we rewrite the <kbd>network_request</kbd>&nbsp;function, which we defined earlier, by replacing the call to <kbd>time.sleep</kbd> with <kbd>asyncio.sleep</kbd>:</p>
<pre><code class="lang-python">    async def network_request(number):
         await asyncio.sleep(1.0)
         return {"success": True, "result": number ** 2}
</code></pre>
<p>We can follow up by reimplementing <kbd>fetch_square</kbd>. As you can see, we can await <kbd>network_request</kbd> directly without needing additional futures or callbacks.</p>
<pre><code class="lang-python">    async def fetch_square(number):
         response = await network_request(number)
         if response["success"]:
             print("Result is: {}".format(response["result"]))
</code></pre>
<p>The coroutines can be executed individually using <kbd>loop.run_until_complete</kbd>:</p>
<pre><code class="lang-python">    loop.run_until_complete(fetch_square(2))
    loop.run_until_complete(fetch_square(3))
    loop.run_until_complete(fetch_square(4))
</code></pre>
<p>Running tasks using <kbd>run_until_complete</kbd> is fine for testing and debugging. However, our program will be started with <kbd>loop.run_forever</kbd> most of the times, and we will need to submit our tasks while the loop is already running.</p>
<p><kbd>asyncio</kbd> provides the <kbd>ensure_future</kbd>&nbsp;function, which&nbsp;schedules coroutines (as well as futures) for execution. <kbd>ensure_future</kbd> can be used by simply passing the coroutine we want to schedule. The following code will schedule multiple calls to <kbd>fetch_square</kbd> that will be executed concurrently:</p>
<pre><code class="lang-python">    asyncio.ensure_future(fetch_square(2))
    asyncio.ensure_future(fetch_square(3))
    asyncio.ensure_future(fetch_square(4))

    loop.run_forever()
    # Hit Ctrl-C to stop the loop
</code></pre>
<p>As a bonus, when passing a coroutine, the <kbd>asyncio.ensure_future</kbd> function will return a <kbd>Task</kbd> instance (which is a subclass of <kbd>Future</kbd>) so that we can take advantage of the await syntax without having to give up the resource tracking capabilities of regular futures.</p>

<h3>Converting blocking code into non-blocking code</h3>
<p>While <kbd>asyncio</kbd> supports connecting to resources in an asynchronous way, it is required to use blocking calls in certain cases. This happens, for example, when third-party APIs exclusively expose blocking calls (for example, many database libraries), but also when executing long-running computations. In this subsection, we will learn how to deal with blocking APIs and make them compatible with <kbd>asyncio</kbd>.</p>
<p>An effective strategy for dealing with blocking code is to run it in a separate thread. Threads are implemented at the <strong>Operating System</strong> (<strong>OS</strong>) level and allow parallel execution of blocking code. For this purpose, Python provides the <kbd>Executor</kbd> interface designed to run tasks in a separate thread and to monitor their progress using futures.</p>
<p>You can initialize a <kbd>ThreadPoolExecutor</kbd> by importing it from the <kbd>concurrent.futures</kbd> module. The executor will spawn a collection of threads (called <kbd>workers</kbd>) that will wait to execute whatever task we throw at them. Once a function is submitted, the executor will take care of dispatching its execution to an available worker thread and keep track of the result. The <kbd>max_workers</kbd> argument can be used to select the number of threads.</p>
<p>Note that the executor will not destroy a thread once a task is completed. By doing so, it reduces the cost associated with the creation and destruction of threads.&nbsp;</p>
<p>In the following example, we create a <kbd>ThreadPoolExecutor</kbd> with three&nbsp;workers, and we submit a <kbd>wait_and_return</kbd> function that will block the program execution for one&nbsp;second and return a message string. We then use the <kbd>submit</kbd> method to schedule its execution:</p>
<pre><code class="lang-python">    from concurrent.futures import ThreadPoolExecutor
    executor = ThreadPoolExecutor(max_workers=3)

    def wait_and_return(msg):
        time.sleep(1)
        return msg

    executor.submit(wait_and_return, "Hello. executor")
    # Result:
    # &lt;Future at 0x7ff616ff6748 state=running&gt;
</code></pre>
<p>The <kbd>executor.submit</kbd> method immediately schedules the function and returns a future. It is possible to manage the execution of tasks in <kbd>asyncio</kbd>&nbsp;using the <kbd>loop.run_in_executor</kbd> method, which works quite similarly to <kbd>executor.submit</kbd>:</p>
<pre><code class="lang-python">    fut = loop.run_in_executor(executor, wait_and_return, "Hello, asyncio 
    executor")
    # &lt;Future pending ...more info...&gt;
</code></pre>
<p>The <kbd>run_in_executor</kbd> method will also return an <kbd>asyncio.Future</kbd> instance that can be awaited from other code, the main difference being that the future will not be run until we start the loop. We can run and obtain the response using <kbd>loop.run_until_complete</kbd>:</p>
<pre><code class="lang-python">    loop.run_until_complete(fut)
    # Result:
    # 'Hello, executor'
</code></pre>
<p>As a practical example, we can use this technique to implement concurrent fetching of several web pages. To do this, we will import the popular (blocking) <kbd>requests</kbd> library and run the <kbd>requests.get</kbd> function in the executor:</p>
<pre><code class="lang-python">    import requests

    async def fetch_urls(urls):
        responses = []
        for url in urls:
            responses.append(await loop.run_in_executor
                                (executor, requests.get, url))
        return responses

    loop.run_until_complete(fetch_ruls(['http://www.google.com', 
                                        'http://www.example.com',
                                        'http://www.facebook.com']))
    # Result
    # []
</code></pre>
<p>This version of <kbd>fetch_url</kbd> will not block the execution and allow other coroutines in <kbd>asyncio</kbd> to run; however, it is not optimal as the function will not fetch a URL&nbsp;in parallel. To do this, we can use <kbd>asyncio.ensure_future</kbd> or employ the <kbd>asyncio.gather</kbd> convenience function that will submit all the coroutines at once and gather the results as they come. The usage of <kbd>asyncio.gather</kbd> is demonstrated here:</p>
<pre><code class="lang-python">    def fetch_urls(urls):
        return asyncio.gather(*[loop.run_in_executor
                                 (executor, requests.get, url) 
                                 for url in urls])
</code></pre>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The number of URLs&nbsp;you can fetch in parallel with this method will be dependent on the number of worker threads you have. To avoid this limitation, you should use a natively non-blocking library, such as <kbd>aiohttp</kbd>.</p>
</div>
</div>

<h2>Reactive programming</h2>
<p>Reactive programming is a paradigm that aims at building better concurrent systems. Reactive applications are designed to comply with the requirements exemplified by the reactive manifesto:</p>
<ul>
<li><strong>Responsive</strong>: &nbsp;The system responds immediately to the user.</li>
<li><strong>Elastic</strong>: The system is capable of handling different levels of load and is able to adapt to accommodate increasing demands.</li>
<li><strong>Resilient</strong>: The system deals with failure gracefully. This is achieved by modularity and avoiding having a single point of failure.</li>
<li><strong>Message driven</strong>: The system should not block and take advantage of events and messages. A message-driven application helps achieve all the previous requirements.</li>
</ul>
<p>As you can see, the intent of reactive systems is quite noble, but how exactly does reactive programming work? In this section, we will learn about the principles of reactive programming using the RxPy library.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The RxPy library is part of ReactiveX (<a href="http://reactivex.io/">http://reactivex.io/</a>), which is a project that implements reactive programming tools for a large variety of languages.</p>
</div>
</div>

<h3>Observables</h3>
<p>As the name implies, the main idea of reactive programming is to <em>react</em> to events. In the preceding section, we saw some examples of this idea with callbacks; you subscribe to them and the callback is executed as soon as the event takes place.</p>
<p>In reactive programming, this idea is expanded by thinking of events as streams of data. This can be exemplified by showing examples of such streams in RxPy. A data stream can be created from an iterator using the <kbd>Observable.from_iterable</kbd> factory method, as follows:</p>
<pre><code class="lang-python">    from rx import Observable
    obs = Observable.from_iterable(range(4))
</code></pre>
<p>In order to receive data from <kbd>obs</kbd>, we can use the <kbd>Observable.subscribe</kbd>&nbsp;method, which&nbsp;will execute the function we pass for each value that the data source emits:</p>
<pre><code class="lang-python">    obs.subscribe(print)
    # Output:
    # 0
    # 1
    # 2
    # 3
</code></pre>
<p>You may have noted that observables are ordered collections of items just like lists or, more generally, iterators. This is not a coincidence.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The term observable comes from the combination of observer and iterable. An <em>observer</em> is an object that reacts to changes of the variable it observes, while an <em>iterable</em> is an object that is capable of producing and keeping track of an iterator.</p>
</div>
</div>
<p>In Python, iterators are objects that define the <kbd>__next__</kbd> method, and whose elements can be extracted by calling <kbd>next</kbd>. An iterator can generally be obtained by a collection using <kbd>iter</kbd>; then we can extract elements using <kbd>next</kbd> or a <kbd>for</kbd> loop. Once an element is consumed from the iterator, we can't go back. We can demonstrate its usage by creating an iterator from a list:</p>
<pre><code class="lang-python">    collection = list([1, 2, 3, 4, 5])
    iterator = iter(collection)

    print("Next")
    print(next(iterator))
    print(next(iterator))

    print("For loop")
    for i in iterator:
         print(i)

    # Output:
    # Next
    # 1
    # 2
    # For loop
    # 3
    # 4
    # 5
</code></pre>
<p>You can see how, every time we call <kbd>next</kbd> or we iterate, the iterator produces a value and advances. In a sense, we are <em>pulling</em> results from the iterator.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Iterators sound a lot like generators; however, they are more general. In Python, generators are returned by functions that use yield expressions. As we saw, generators support <kbd>next</kbd>, therefore, they are a special class of iterators.</p>
</div>
</div>
<p>Now you can appreciate the contrast between an iterator and an observable.&nbsp; An observable<em>pushes</em> a stream of data to us whenever it's ready, but that's not everything. An observable is also able to tell us when there is an error and where there is no more data. In fact, it is possible to register further callbacks to the <kbd>Observable.subscribe</kbd> method. In the following example, we create an observable and register callbacks to be called using <kbd>on_next</kbd>&nbsp;whenever the next item is available and using the <kbd>on_completed</kbd> argument when there is no more data:</p>
<pre><code class="lang-python">    obs = Observable.from_iter(range(4))
    obs.subscribe(on_next=lambda x: print(on_next="Next item: {}"),
                  on_completed=lambda: print("No more data"))
    # Output:
    # Next element: 0
    # Next element: 1
    # Next element: 2
    # Next element: 3
    # No more data
</code></pre>
<p>This analogy with the iterator is important because we can use the same techniques that can be used with iterators to handle streams of events.</p>
<p>RxPy provides operators that can be used to create, transform, filter, and group observables. The power of reactive programming lies in the fact that those operations return other observables that can be conveniently chained and composed together. For a quick taste, we will demonstrate the usage of the <kbd>take</kbd>&nbsp;operator.</p>
<p>Given an observable, <kbd>take</kbd> will return a new observable that will stop after <kbd>n</kbd> items. Its usage is straightforward:</p>
<pre><code class="lang-python">    obs = Observable.from_iterable(range(100000))
    obs2 = obs.take(4)

    obs2.subscribe(print)
    # Output:
    # 0
    # 1
    # 2
    # 3
</code></pre>
<p>The collection of operations implemented in RxPy is varied and rich, and can be used to build complex applications using these operators as building blocks.</p>

<h3>Useful operators</h3>
<p>In this subsection, we will explore operators that transform the elements of a source observable in some way. The most prominent member of this family of operators is the familiar <kbd>map</kbd>, which emits the elements of the source observable after applying a function to them. For example, we may use <kbd>map</kbd> to calculate the square of a sequence of numbers:</p>
<pre><code class="lang-python">    (Observable.from_iterable(range(4))
               .map(lambda x: x**2)
               .subscribe(print))
    # Output:
    # 0
    # 1
    # 4
    # 9
</code></pre>
<p>Operators can be represented with marble diagrams that help us better understand how the operator works, especially when taking into account the fact that elements can be emitted over a region of time. In a marble diagram, a data stream (in our case, an observable) is represented by a solid line. A circle (or other shape) identifies a value emitted by the observable, an <strong>X</strong> symbol represents an error, and a vertical line represents the end of the stream.</p>
<p>In the following figure, we can see the marble diagram of <strong>map</strong>:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000017.png" class="lazyload" /></p>
</div>
<p>The source observable is placed at the top of the diagram, the transformation is placed in the middle, and the resulting observable is placed at the bottom.</p>
<p>Another example of a transformation is <kbd>group_by</kbd>, which&nbsp;sorts the items into groups based on a key. The <kbd>group_by</kbd> operator takes a function that extracts a key when given an element&nbsp;and produces an observable for each key with the elements associated to it.</p>
<p>The <kbd>group_by</kbd> operation can be expressed more clearly using a marble diagram. In the following figure, you can see how <kbd>group_by</kbd> emits two observables. Additionally, the items are dynamically sorted into groups <em>as soon as they are emitted</em>:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000018.png" class="lazyload" /></p>
</div>
<p>We can further understand how <kbd>group_by</kbd> works with a simple example. Let's say that we want to group the number according to the fact that&nbsp;they're even or odd. We can implement this using <kbd>group_by</kbd> by passing the <kbd>lambda x: x % 2</kbd>&nbsp;expression as a key function, which will return <kbd>0</kbd> if the number is even and <kbd>1</kbd> if the number is odd:</p>
<pre><code class="lang-python">    obs = (Observable.from_range(range(4))
                     .group_by(lambda x: x % 2))
</code></pre>
<p>At this point, if we subscribe and print the content of <kbd>obs</kbd>, actually two observables are printed:</p>
<pre><code class="lang-python">    obs.subscribe(print)
    # &lt;rx.linq.groupedobservable.GroupedObservable object at 0x7f0fba51f9e8&gt;
    # &lt;rx.linq.groupedobservable.GroupedObservable object at 0x7f0fba51fa58&gt;
</code></pre>
<p>You can determine the group key using the <kbd>key</kbd> attribute. To extract all the even numbers, we can take the first observable (corresponding to a key equal to 0) and subscribe to it. In the following code, we show how this works:</p>
<pre><code class="lang-python">    obs.subscribe(lambda x: print("group key: ", x.key))
    # Output:
    # group key:  0
    # group key:  1
    obs.take(1).subscribe(lambda x: x.subscribe(print))
    # Output:
    # 0
    # 2
</code></pre>
<p>With <kbd>group_by</kbd>, we introduced an observable that emits other observables. This turns out to be quite a common pattern in reactive programming, and there are functions that allow you to combine different observables.</p>
<p>Two useful tools for combining observables are <kbd>merge_all</kbd> and <kbd>concat_all</kbd>. Merge takes multiple observables and produces a single observable that contains the element of the two observables in the order they are emitted. This is better illustrated using a marble diagram:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000019.png" class="lazyload" /></p>
</div>
<p><kbd>merge_all</kbd> can be compared to a similar operator, <kbd>concat_all</kbd>, which&nbsp;returns a new observable that emits the elements of all the elements of the first observable, followed by the elements of the second observable and so on. The marble diagram for <kbd>concat_all</kbd> is presented here:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000020.png" class="lazyload" /></p>
</div>
<p>To demonstrate the usage of these two operators, we can apply those operations to the observable of observables returned by <kbd>group_by</kbd>. In the case of <kbd>merge_all</kbd>, the items are returned in the same order as they were initially (remember that <kbd>group_by</kbd> emits elements in the two groups as they come):</p>
<pre><code class="lang-python">    obs.merge_all().subscribe(print)
    # Output
    # 0
    # 1
    # 2
    # 3
</code></pre>
<p>On the other hand, <kbd>concat_all</kbd> first returns the even elements and then the odd elements as it waits for the first observable to complete, and then starts emitting the elements of the second observable. This is demonstrated in the following snippet.&nbsp;In this specific example, we also applied a function,&nbsp;<kbd>make_replay</kbd>; this is needed because, by the time the "even" stream is consumed, the elements of the second stream have already been produced and will not be available to <kbd>concat_all</kbd>. This concept will become much clearer after reading the <em>Hot and cold observables</em>&nbsp;section:</p>
<pre><code class="lang-python">    def make_replay(a):
        result = a.replay(None)
        result.connect()
        return result

    obs.map(make_replay).concat_all().subscribe(print)
    # Output
    # 0
    # 2
    # 1
    # 3
</code></pre>
<p>This time around, the even numbers are printed first, followed by the odd numbers.&nbsp;</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>RxPy also provides the&nbsp; <kbd>merge</kbd> and <kbd>concat</kbd> operations that can be used to combine individual observables</p>
</div>
</div>

<h3>Hot and cold observables</h3>
<p>In the preceding&nbsp;section, we learned&nbsp;how to create an observable using the&nbsp;<kbd>Observable.from_iterable</kbd> method. RxPy provides many other tools to create more interesting event sources.</p>
<p><kbd>Observable.interval</kbd> takes a time interval in milliseconds, <kbd>period</kbd>, and will create an observable that emits a value every time the period has passed. The following line of code can be used to define an observable, <kbd>obs</kbd>, that will emit a number, starting from zero, every second. We use the <kbd>take</kbd> operator to limit the timer to four&nbsp;events:</p>
<pre><code class="lang-python">    obs = Observable.interval(1000)
    obs.take(4).subscribe(print)
    # Output:
    # 0
    # 1
    # 2
    # 3
</code></pre>
<p>A very important fact about <kbd>Observable.interval</kbd> is that the timer doesn't start until we subscribe. We can observe this by printing both the index and the delay from when the timer starts definition using <kbd>time.time()</kbd>, as follows:</p>
<pre><code class="lang-python">    import time

    start = time.time()
    obs = Observable.interval(1000).map(lambda a: 
                                           (a, time.time() - start))

    # Let's wait 2 seconds before starting the subscription
    time.sleep(2)
    obs.take(4).subscribe(print)
    # Output:
    # (0, 3.003735303878784)
    # (1, 4.004871129989624)
    # (2, 5.005947589874268)
    # (3, 6.00749135017395)
</code></pre>
<p>As you can see, the first element (corresponding to a <kbd>0</kbd> index) is produced after three&nbsp;seconds, which means that the timer started when we issue the&nbsp;<kbd>subscribe(print)</kbd> method.</p>
<p>Observables, such as <kbd>Observable.interval</kbd>, are called&nbsp;<em>lazy</em>&nbsp;because they start producing values only when requested (think of them as vending machines, which won't dispense food unless we press the button). In Rx jargon, these&nbsp;kind of observables are called <strong>cold</strong>. A property of cold observables is that, if we attach two subscribers, the interval timer will be started multiple times. This is quite evident from the following example. Here, we add a new subscription 0.5 seconds after the first, and you can see how the output of the two subscriptions come at different times:</p>
<pre><code class="lang-python">    start = time.time()
    obs = Observable.interval(1000).map(lambda a: 
                                           (a, time.time() - start))

    # Let's wait 2 seconds before starting the subscription
    time.sleep(2)
    obs.take(4).subscribe(lambda x: print("First subscriber: 
                                             {}".format(x)))
    time.sleep(0.5)
    obs.take(4).subscribe(lambda x: print("Second subscriber: 
                                             {}".format(x)))
    # Output:
    # First subscriber: (0, 3.0036110877990723)
    # Second subscriber: (0, 3.5052847862243652)
    # First subscriber: (1, 4.004414081573486)
    # Second subscriber: (1, 4.506155252456665)
    # First subscriber: (2, 5.005316972732544)
    # Second subscriber: (2, 5.506817102432251)
    # First subscriber: (3, 6.0062034130096436)
    # Second subscriber: (3, 6.508296489715576)
</code></pre>
<p>Sometimes we may not want this behavior as we may want multiple subscribers to subscribe to the same data source. To make the observable produce the same data, we can delay the data production and ensure that all the subscribers will get the same data using the <kbd>publish</kbd>&nbsp;method.</p>
<p>Publish will transform our observable into a <kbd>ConnectableObservable</kbd>, which won't start pushing data immediately, but only when we call the <kbd>connect</kbd> method. The usage of <kbd>publish</kbd> and <kbd>connect</kbd> is demonstrated in the following snippet:</p>
<pre><code class="lang-python">    start = time.time()
    obs = Observable.interval(1000).map(lambda a: (a, time.time() - 
    start)).publish()
    obs.take(4).subscribe(lambda x: print("First subscriber: 
                                             {}".format(x)))
    obs.connect() # Data production starts here

    time.sleep(2)
    obs.take(4).subscribe(lambda x: print("Second subscriber: 
                                             {}".format(x)))
    # Output:
    # First subscriber: (0, 1.0016899108886719)
    # First subscriber: (1, 2.0027990341186523)
    # First subscriber: (2, 3.003532648086548)
    # Second subscriber: (2, 3.003532648086548)
    # First subscriber: (3, 4.004265308380127)
    # Second subscriber: (3, 4.004265308380127)
    # Second subscriber: (4, 5.005320310592651)
    # Second subscriber: (5, 6.005795240402222)
</code></pre>
<p>In the preceding&nbsp;example, you can see how we first issue <kbd>publish</kbd>, then we subscribe the first subscriber and, finally, we issue <kbd>connect</kbd>. When <kbd>connect</kbd> is issued, the timer will start producing data. The second subscriber joins the party late and, in fact, won't receive the first two messages but will start receiving data from the third and so on. Note how, this time around, the subscribers share the exact same data. This kind of data source, where data is produced independently of the subscribers, is called <strong>hot</strong>.</p>
<p>Similar to <kbd>publish</kbd>, you can use the <kbd>replay</kbd> method that will produce the data&nbsp;<em>from the beginning</em>&nbsp;for each new subscriber. This is illustrated in the following example that, which is identical to the preceding&nbsp;one except that we replaced <kbd>publish</kbd> with <kbd>replay</kbd>:</p>

<pre><code class="lang-python">    import time

    start = time.time()
    obs = Observable.interval(1000).map(lambda a: (a, time.time() - 
    start)).replay(None)
    obs.take(4).subscribe(lambda x: print("First subscriber: 
                                             {}".format(x)))
    obs.connect()

    time.sleep(2)
    obs.take(4).subscribe(lambda x: print("Second subscriber: 
                                             {}".format(x)))

    First subscriber: (0, 1.0008857250213623)
    First subscriber: (1, 2.0019824504852295)
    Second subscriber: (0, 1.0008857250213623)
    Second subscriber: (1, 2.0019824504852295)
    First subscriber: (2, 3.0030810832977295)
    Second subscriber: (2, 3.0030810832977295)
    First subscriber: (3, 4.004604816436768)
    Second subscriber: (3, 4.004604816436768)
</code></pre>

<p>You can see how, this time around, even though the second subscriber arrives late to the party, it is still given all the items that have been given out so far.</p>
<p>Another way of creating hot observables is through the <kbd>Subject</kbd> class. <kbd>Subject</kbd> is interesting because it's capable of both receiving and pushing data, and thus it can be used to manually <em>push</em> items to an observable. Using <kbd>Subject</kbd> is very intuitive; in the following code, we create a <kbd>Subject</kbd> and subscribe to it. Later, we push values to it using the <kbd>on_next</kbd>&nbsp;method; as soon as we do that, the subscriber is called:</p>
<pre><code class="lang-python">    s = Subject()
    s.subscribe(lambda a: print("Subject emitted value: {}".format(x))
    s.on_next(1)
    # Subject emitted value: 1
    s.on_next(2)
    # Subject emitted value: 2
</code></pre>
<p>Note that <kbd>Subject</kbd> is another example of hot observables.</p>

<h3>Building a CPU monitor</h3>
<p>Now that we have a grasp on the main reactive programming concepts, we can implement a sample application. In this subsection, we will implement a monitor that will give us real-time information about our CPU usage and is capable of detecting spikes.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The complete code for the CPU&nbsp;monitor can be found in the <kbd>cpu_monitor.py</kbd>&nbsp;file.</p>
</div>
</div>
<p>As a first step, let's implement a data source. We will use the <kbd>psutil</kbd> module that&nbsp;provides a function, <kbd>psutil.cpu_percent</kbd>, that returns the latest available CPU usage as a percent (and doesn't block):</p>
<pre><code class="lang-python">    import psutil
    psutil.cpu_percent()
    # Result: 9.7
</code></pre>
<p>Since we are developing a monitor, we would like to sample this information over a few time intervals. To accomplish this we can use the familiar <kbd>Observable.interval</kbd> , followed by <kbd>map</kbd> just like we did in the previous section. Also, we would like to make this observable <em>hot</em> as, for this application, all subscribers should receive a single source of data; to make <kbd>Observable.interval</kbd> hot,&nbsp; we can use the <kbd>publish</kbd> and <kbd>connect</kbd> methods. The full code for the creation of the <kbd>cpu_data</kbd> observable is as follows</p>
<pre><code class="lang-python">    cpu_data = (Observable
                .interval(100) # Each 100 milliseconds
                .map(lambda x: psutil.cpu_percent())
                .publish())
    cpu_data.connect() # Start producing data
</code></pre>
<p>We can test our monitor by printing a sample of 4 items</p>
<pre><code class="lang-python">    cpu_data.take(4).subscribe(print)
    # Output:
    # 12.5
    # 5.6
    # 4.5
    # 9.6
</code></pre>
<p>Now that our main data source is in place, we can implement a monitor visualization using <kbd>matplotlib</kbd>. The idea is to create a plot that contains a fixed amount of measurements and, as new data arrives, we include the newest measurement and remove the oldest one. This is commonly referred to as a&nbsp;<em>moving window</em> and is better understood with an illustration. In the following figure, our <kbd>cpu_data</kbd> stream is represented as a list of numbers. The first plot is produced as soon as we have the first four numbers and, each time a new number arrives, we shift the window by one position and update the plot:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000022.png" class="lazyload" /></p>
</div>
<p>To implement this algorithm, we can write a function, called <kbd>monitor_cpu</kbd>, that will create and update our plotting window. The function will do the following things:</p>
<ul>
<li>Initialize an empty plot and set up the correct plot limits.</li>
<li>Transform our <kbd>cpu_data</kbd> observable to return a moving window over the data. This can be accomplished using the <kbd>buffer_with_count</kbd> operator, which will take the number of points in our window, <kbd>npoints</kbd>, as parameters and the shift as&nbsp;<kbd>1</kbd>.</li>
<li>Subscribe to this new data stream and update the plot with the incoming data.</li>
</ul>
<p>The complete code for the function is shown here&nbsp;and, as you can see, is extremely compact. Take some time to run the function and play with the parameters:</p>
<pre><code class="lang-python">    import numpy as np
    from matplotlib import pyplot as plt

    def monitor_cpu(npoints):
        lines, = plt.plot([], [])
        plt.xlim(0, npoints) 
        plt.ylim(0, 100) # 0 to 100 percent

        cpu_data_window = cpu_data.buffer_with_count(npoints, 1)

        def update_plot(cpu_readings):
            lines.set_xdata(np.arange(npoints))
            lines.set_ydata(np.array(cpu_readings))
            plt.draw()

        cpu_data_window.subscribe(update_plot)

        plt.show()
</code></pre>
<p>Another feature we may want to develop is, for example, an alert that triggers when the CPU has been high for a certain amount of time as this may indicate that some of the&nbsp; processes in our machine are working very hard. This can be accomplished by combining <kbd>buffer_with_count</kbd> and <kbd>map</kbd>. We can take the CPU stream and a window, and then we will test whether&nbsp;all items have a value higher than twenty percent&nbsp;usage (in a quad-core CPU that corresponds to about one processor working at hundred percent) in the map function. If all the points in the window have a higher than twenty percent usage, we display a warning in our plot window.</p>
<p>The implementation of the new observable can be written as follows and will produce an observable that emits <kbd>True</kbd> if the CPU&nbsp;has high usage, and&nbsp;<kbd>False</kbd> otherwise:</p>
<pre><code class="lang-python">    alertpoints = 4    
    high_cpu = (cpu_data
                .buffer_with_count(alertpoints, 1)
                .map(lambda readings: all(r &gt; 20 for r in readings)))
</code></pre>
<p>Now that the <kbd>high_cpu</kbd> observable is ready, we can create a <kbd>matplotlib</kbd> label and subscribe to it for updates:</p>
<pre><code class="lang-python">    label = plt.text(1, 1, "normal")
    def update_warning(is_high):
        if is_high:
            label.set_text("high")
        else:
            label.set_text("normal")
    high_cpu.subscribe(update_warning)
</code></pre>

<h2>Summary</h2>
<p>Asynchronous programming is useful when our code deals with slow and unpredictable resources, such as I/O devices and networks. In this chapter, we explored the fundamental concepts of concurrency and&nbsp;asynchronous programming and how to write concurrent code with the <kbd>asyncio</kbd> and RxPy libraries.</p>
<p><kbd>asyncio</kbd> coroutines are an excellent choice when dealing with multiple, interconnected resources as they&nbsp;greatly simplify the code logic by cleverly avoiding callbacks. Reactive programming is also very good in these situations, but it truly shines when dealing with streams of data that are common in real-time applications and user interfaces.</p>
<p>In the next two chapters, we will learn about parallel programming and how to achieve impressive performance gain by taking advantage of multiple cores and multiple machines.</p>

</div>





<!--Chapter 7-->


<div class="chapter" data-chapter-number="7">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 7 </span></div>
<h1 class="chaptertitle">Parallel Processing</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>With parallel processing by using multiple cores, you can increase the amount of calculations your program can do in a given time frame without needing a faster processor. The main idea is to divide a problem&nbsp;into independent subunits and use multiple cores to solve those subunits in parallel.</p>
<p>Parallel processing is necessary to tackle large-scale problems. Companies produce massive quantities of data every day that need to be stored in multiple computers and analyzed.&nbsp;Scientists and engineers run parallel code on supercomputers to simulate massive systems.</p>
<p>Parallel processing allows you to take advantage of multicore CPUs as well as GPUs that work extremely well with&nbsp;highly parallel problems. In this chapter, we will cover the following topics:</p>
<ul>
<li>A brief introduction to the fundamentals of parallel processing</li>
<li>Illustrating how to parallelize simple problems with the <kbd>multiprocessing</kbd> Python library</li>
<li>Using the simple <kbd>ProcessPoolExecutor</kbd> interface</li>
<li>Parallelizing our programs using multithreading with the help of Cython and OpenMP</li>
<li>Achieving parallelism automatically with Theano and Tensorflow</li>
<li>Executing code on a GPU with Theano, Tensorflow, and Numba</li>
</ul>

<h2>Introduction to parallel programming</h2>
<p>In order to parallelize a program,&nbsp;it is necessary to divide the problem into subunits that can run independently (or almost independently) from each other.</p>
<p>A problem where the subunits are totally independent from each other is called&nbsp;<strong>embarrassingly&nbsp;parallel</strong>. An element-wise operation on an array is a typical example--the operation needs to only know the element it is handling at the moment. Another example is our particle simulator. Since there are no interactions, each particle can evolve independently from the others. Embarrassingly parallel problems are very easy to implement and perform very well on parallel architectures.</p>
<p>Other problems may be divided into subunits but have to share some data to perform their calculations. In those cases, the implementation is less straightforward and can lead to performance issues because of the communication costs.</p>
<p>We will illustrate the concept with an example. Imagine that you have a particle simulator, but this time the particles attract other particles within a certain distance (as shown in the following figure). To parallelize this problem, we divide the simulation box into regions and assign each region to a different processor. If we evolve the system one step at a time, some particles will interact with particles in a neighboring region. To perform the next iteration, communication with the new particle positions of the neighboring region is required.</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000006.png" class="lazyload" /></p>
</div>
<p>Communication between processes is costly and can seriously hinder the performance of parallel programs. There exist two main ways to handle data communication in parallel programs:</p>
<ul>
<li><strong>Shared memory</strong></li>
<li><strong>Distributed memory</strong></li>
</ul>
<p>In shared memory, the subunits have access to the same memory space. The advantage of this approach is that you don't have to explicitly handle the communication as it is sufficient to write or read from the shared memory. However, problems arise when multiple processes try to access and change the same memory location at the same time. Care should be taken to avoid such conflict using synchronization techniques.</p>
<p>In the distributed memory model, each process is completely separated from the others and possesses its own memory space. In this case, communication is handled explicitly between the processes. The communication overhead is typically costlier compared to shared memory as data can potentially travel through a network interface.</p>
<p>One common way to achieve parallelism with the shared memory model is <strong>threads</strong>. Threads are independent subtasks that originate from a process and share resources, such as memory. This concept is further illustrated in the following figure. Threads produce multiple execution context and share the same memory space, while processes provide multiple execution context that possess their own memory space and communication has to be handled explicitly.</p>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000009.png" class="lazyload" /></p>
<p>Python can spawn and handle threads, but they can't be used to increase performance; due to the Python interpreter design, only one Python instruction is allowed to run at a time--this mechanism is called <strong>Global Interpreter Lock</strong> (<strong>GIL</strong>). What happens is that each time a thread executes a Python statement, the thread acquires a lock and, when the execution is completed, the same lock is released. Since the lock can be acquired only by one thread at a time, other threads are prevented from executing Python statements while some other thread holds the lock.</p>
<p>Even though the GIL prevents parallel execution of Python instructions, threads can still be used to provide concurrency in situations where the lock can be released, such as in time-consuming I/O operations or in C extensions.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Why not remove the GIL? In past years, many attempts have been made, including the most recent gilectomy&nbsp;experiment. First, removing the GIL is not an easy task and&nbsp;requires modification of most of the Python data structures. Additionally, such fine-grained locking can be costly and may introduce substantial performance loss in single-threaded programs. Despite this, some Python implementations (notable examples are Jython and IronPython) do not use the GIL.</p>
</div>
</div>
<p>The GIL can be completely sidestepped using processes instead of threads. Processes don't share the same memory area and are independent from each other--each process has its own interpreter. Processes have a few disadvantages: starting up a new process is generally slower than starting a new thread, they consume more memory, and&nbsp;inter-process communication can be slow. On the other hand, processes are still very flexible, and they scale better as they can be distributed on multiple machines.</p>

<h3>Graphic processing units</h3>
<p>Graphic processing units are special processors designed for computer graphics applications. Those applications usually require processing the geometry of a 3D scene and output an array of pixel to the screen. The operations performed by GPUs involve array and matrix operations on floating point numbers.</p>
<p>GPUs are designed to run this graphics-related&nbsp;operation very efficiently, and they achieve this by adopting a highly parallel architecture. Compared to a CPU, a GPU has many more (thousands) of small processing units. GPUs are intended to produce data at about 60 frames per second, which is much slower than the typical response time of a CPU, which possesses higher clock speeds.</p>
<p>GPUs possess a very different architecture from&nbsp;a standard CPU and are specialized for computing floating point operations. Therefore, to compile programs for GPUs, it is necessary to utilize special&nbsp;programming platforms, such as CUDA and OpenCL.</p>
<p><strong>Compute Unified Device Architecture</strong> (<strong>CUDA</strong>) is a proprietary NVIDIA technology. It provides an API that can be accessed from other languages. CUDA provides the NVCC&nbsp;tool that can be used to compile GPU programs written in a language similar to C (CUDA C) as well as numerous libraries that implement highly optimized mathematical routines.</p>
<p><strong>OpenCL</strong>&nbsp;is an open technology with the ability of writing parallel programs that can be compiled&nbsp;for a variety of target devices (CPUs and GPUs of several vendors) and is a good option for non-NVIDIA devices.</p>
<p>GPU programming sounds&nbsp;wonderful on paper. However, don't throw away your CPU yet. GPU programming is tricky and only specific use cases benefit from the GPU architecture. Programmers need to be aware&nbsp;of the costs incurred in memory transfers to and from the main memory and how to implement algorithms to take advantage of the GPU architecture.</p>
<p>Generally, GPUs are great at increasing the amount of operations you can perform per unit of time (also called <strong>throughput</strong>); however, they require more time to prepare the data for processing. In contrast, CPUs are much faster at producing an individual result from scratch (also called <strong>latency</strong>).</p>
<p>For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this reason, they often constitute a very inexpensive (the same speedup will require hundreds of CPUs) solution to improve the performance of numerically intensive&nbsp;applications. We will illustrate how to execute some algorithms on a GPU in the <em>Automatic Parallelism</em> section.</p>

<h2>Using multiple processes</h2>
<p>The standard <kbd>multiprocessing</kbd> module can be used to quickly parallelize simple tasks by spawning several processes, while&nbsp;avoiding the GIL problem. Its interface is easy to use and includes several utilities to handle task submission and synchronization.</p>

<h3>The Process and Pool classes</h3>
<p>You can create a process that runs independently by subclassing <kbd>multiprocessing.Process</kbd>. You can extend the <kbd>__init__</kbd> method to initialize resources, and you can write the portion of the code&nbsp;that will be executed in a subprocess by implementing the&nbsp;<kbd>Process.run</kbd> method. In the following code, we define a <kbd>Process</kbd> class that will wait for one second and print its assigned <kbd>id</kbd>:</p>
<pre><code class="lang-python">    import multiprocessing 
    import time 

    class Process(multiprocessing.Process): 
        def __init__(self, id): 
            super(Process, self).__init__() 
            self.id = id 

        def run(self): 
            time.sleep(1) 
            print("I'm the process with id: {}".format(self.id))
</code></pre>
<p>To spawn the process, we have to instantiate the&nbsp;<kbd>Process</kbd>&nbsp;class and call the <kbd>Process.start</kbd> method. Note that you don't directly call <kbd>Process.run</kbd>; the call to <kbd>Process.start</kbd> will create a new process that, in turn, will call the <kbd>Process.run</kbd> method. We can add the following lines at the end of the preceding&nbsp;snippet to create and start the new process:</p>
<pre><code class="lang-python">    if __name__ == '__main__': 
        p = Process(0) 
        p.start()
</code></pre>
<p>The instructions after <kbd>Process.start</kbd> will be executed immediately without waiting for the <kbd>p</kbd> process to finish. To wait for the task completion, you can use the&nbsp;<kbd>Process.join</kbd>&nbsp;method, as follows:</p>
<pre><code class="lang-python">    if __name__ == '__main__': 
       p = Process(0) 
       p.start() 
       p.join()
</code></pre>
<p>We can launch four different processes that will run parallely in the same way. In a serial program, the total required time will be four seconds. Since the execution is concurrent, the&nbsp;resulting wallclock time will be of one second. In the following code, we create four processes that will execute concurrently:</p>
<pre><code class="lang-python">    if __name__ == '__main__': 
        processes = Process(1), Process(2), Process(3), Process(4) 
        [p.start() for p in processes]
</code></pre>
<p>Note that the order of the execution for parallel processes is unpredictable and ultimately depends on how the OS&nbsp;schedules their execution. You can verify this behavior by executing the program multiple times; the order will likely be different between runs.</p>
<p>The <kbd>multiprocessing</kbd> module exposes a convenient interface that makes it easy to assign and distribute tasks to a set of processes that&nbsp;reside in the <kbd>multiprocessing.Pool</kbd> class.</p>
<p>The <kbd>multiprocessing.Pool</kbd> class spawns a set of processes--called <strong>workers</strong>--and lets us submit tasks through the <kbd>apply</kbd>/<kbd>apply_async</kbd> and <kbd>map</kbd>/<kbd>map_async</kbd>&nbsp;methods.</p>
<p>The <kbd>Pool.map</kbd> method applies a function to each element of a list and returns the list of results. Its usage is equivalent to the built-in (serial) <kbd>map</kbd>.</p>
<p>To use a parallel map, you should first initialize a <kbd>multiprocessing.Pool</kbd> object. It takes the number of workers as its first argument; if not provided, that number will be equal to the number of cores in the system. You can initialize a <kbd>multiprocessing.Pool</kbd> object in the following way:</p>
<pre><code class="lang-python">    pool = multiprocessing.Pool() 
    pool = multiprocessing.Pool(processes=4)
</code></pre>
<p>Let's see <kbd>pool.map</kbd> in action. If you have a function that computes the square of a number, you can map the function to the list by calling <kbd>Pool.map</kbd> and passing the function and the list of inputs as arguments, as follows:</p>
<pre><code class="lang-python">    def square(x): 
        return x * x 

    inputs = [0, 1, 2, 3, 4] 
    outputs = pool.map(square, inputs)
</code></pre>
<p>The <kbd>Pool.map_async</kbd> function is just like <kbd>Pool.map</kbd> but returns an <kbd>AsyncResult</kbd> object instead of the actual result. When we call &nbsp;<kbd>Pool.map</kbd>, the execution of the main program is stopped until all the workers are finished processing the result. With <kbd>map_async</kbd>, the <kbd>AsyncResult</kbd> object is returned immediately without blocking the main program and the calculations are done in the background. We can then retrieve the result using the <kbd>AsyncResult.get</kbd> method at any time, as shown in the following lines:</p>
<pre><code class="lang-python">    outputs_async = pool.map_async(square, inputs) 
    outputs = outputs_async.get()
</code></pre>
<p><kbd>Pool.apply_async</kbd> assigns a task consisting of a single function to one of the workers. It takes the function and its arguments and returns an <kbd>AsyncResult</kbd> object. We can obtain an effect similar to <kbd>map</kbd>&nbsp;using <kbd>apply_async</kbd>, as shown:</p>
<pre><code class="lang-python">    results_async = [pool.apply_async(square, i) for i in range(100))] 
    results = [r.get() for r in results_async]
</code></pre>

<h3>The Executor interface</h3>
<p>From version 3.2 onward, it is possible to execute Python code in parallel using&nbsp;the <kbd>Executor</kbd> interface provided in the <kbd>concurrent.futures</kbd> module. We already saw the <kbd>Executor</kbd> interface in action in the previous chapter, when we used&nbsp;<kbd>ThreadPoolExecutor</kbd> to perform&nbsp;multiple tasks concurrently. In this subsection, we'll demonstrate the usage of the <kbd>ProcessPoolExecutor</kbd> class.</p>
<p><kbd>ProcessPoolExecutor</kbd>&nbsp;exposes a&nbsp;very lean interface, at least when compared to the more featureful <kbd>multiprocessing.Pool</kbd>. A <kbd>ProcessPoolExecutor</kbd> can be instantiated, similar to <kbd>ThreadPoolExecutor</kbd>, by passing a number of worker threads using the <kbd>max_workers</kbd> argument (by default, <kbd>max_workers</kbd> will be the number of CPU cores available). The main methods available to the <kbd>ProcessPoolExecutor</kbd>&nbsp;are <kbd>submit</kbd> and <kbd>map</kbd>.</p>
<p>The <kbd>submit</kbd> method will take a function and return a <kbd>Future</kbd>&nbsp;(see the last&nbsp;chapter) that will keep track of the execution of the submitted function. The method map works similarly to the <kbd>Pool.map</kbd> function, except that it returns an iterator rather than a list:</p>
<pre><code class="lang-python">    from concurrent.futures import ProcessPoolExecutor

    executor = ProcessPoolExecutor(max_workers=4)
    fut = executor.submit(square, 2)
    # Result:
    # &lt;Future at 0x7f5b5c030940 state=running&gt;

    result = executor.map(square, [0, 1, 2, 3, 4])
    list(result)
    # Result:
    # [0, 1, 4, 9, 16]
</code></pre>
<p>To extract&nbsp;the result from one or more&nbsp;<kbd>Future</kbd>&nbsp;instances, you can use the <kbd>concurrent.futures.wait</kbd> and <kbd>concurrent.futures.as_completed</kbd>&nbsp;functions. The <kbd>wait</kbd> function accepts a list of <kbd>future</kbd> and will block the execution of the programs until all the futures have completed their execution. The result can then be extracted using the <kbd>Future.result</kbd> method. The <kbd>as_completed</kbd> function also accepts a function but will, instead, return an iterator over the results:</p>
<pre><code class="lang-python">    from concurrent.futures import wait, as_completed

    fut1 = executor.submit(square, 2)
    fut2 = executor.submit(square, 3)
    wait([fut1, fut2])
    # Then you can extract the results using fut1.result() and fut2.result()

    results = as_completed([fut1, fut2])
    list(results)
    # Result:
    # [4, 9]
</code></pre>
<p>Alternatively, you can generate futures using the <kbd>asyncio.run_in_executor</kbd> function and manipulate the results using all the tools and syntax provided by the <kbd>asyncio</kbd> libraries so that you can achieve concurrency and parallelism at the same time.&nbsp;&nbsp;</p>

<h3>Monte Carlo approximation of pi</h3>
<p>As an example, we will implement a canonical, embarrassingly&nbsp;parallel program--the <strong>Monte Carlo approximation of pi</strong>. Imagine that we have a square of size 2 units; its area will be 4 units. Now, we inscribe a circle of 1 unit radius in this square; the area of the circle will be <em>pi * r^2</em>. By substituting the value of <em>r</em> in the previous equation, we get that the numerical value for the area of the circle is <em>pi * (1)^2 = pi</em>. You can refer to the following figure for a graphical representation.</p>
<p>If we shoot a lot of random points on this figure, some points will fall into the circle, which we'll call&nbsp;<strong>hits,&nbsp;</strong>while the remaining points,&nbsp;<strong>misses,&nbsp;</strong>will be outside the circle. The area of the circle will be proportional to the number of hits, while the area of the square will be proportional to the total number of shots. To get the value of <em>pi</em>, it is sufficient to divide the area of the circle (equal to <em>pi</em>) by the area of the square (equal to 4):</p>
<pre><code class="lang-python">    hits/total = area_circle/area_square = pi/4 
    pi = 4 * hits/total
</code></pre>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000039.png" class="lazyload" /></p>
</div>
<p>The strategy we will employ in our program will be as follows:</p>
<ul>
<li>Generate a lot of uniformly random&nbsp;(<em>x</em>, <em>y</em>) numbers in the range (<strong>-1</strong>, <strong>1</strong>)</li>
<li>Test whether&nbsp;those numbers lie inside the circle by checking whether&nbsp;<em>x**2 + y**2</em> &lt;= <em>1</em></li>
</ul>
<p>The first step when writing a parallel program is to write a serial version and verify&nbsp;that it works. In a real-world scenario, you also want to leave the parallelization as the last step of your optimization process. First, we need to identify the slow parts, and second, parallelization is time-consuming and gives you <em>at most</em> a speedup equal to the number of processors. The implementation of the serial program is as follows:</p>
<pre><code class="lang-python">    import random 

    samples = 1000000 
    hits = 0 

    for i in range(samples): 
        x = random.uniform(-1.0, 1.0) 
        y = random.uniform(-1.0, 1.0) 

        if x**2 + y**2 &lt;= 1: 
            hits += 1 
     
    pi = 4.0 * hits/samples
</code></pre>
<p>The accuracy of our approximation will improve as we increase the number of samples. You can note that each loop iteration is independent from the other--this problem is embarrassingly parallel.</p>
<p>To parallelize this code, we can write a function, called <kbd>sample</kbd>, that corresponds to a single hit-miss check. If the sample hits the circle, the function will return <kbd>1</kbd>; otherwise, it will return <kbd>0</kbd>. By running <kbd>sample</kbd> multiple times and summing the results, we'll get the total number of hits. We can run <kbd>sample</kbd> over multiple processors with <kbd>apply_async</kbd> and get the results in the following way:</p>
<pre><code class="lang-python">    def sample(): 
        x = random.uniform(-1.0, 1.0) 
        y = random.uniform(-1.0, 1.0) 

        if x**2 + y**2 &lt;= 1: 
            return 1 
        else: 
            return 0 

    pool = multiprocessing.Pool() 
    results_async = [pool.apply_async(sample) for i in range(samples)] 
    hits = sum(r.get() for r in results_async)
</code></pre>
<p>We can wrap the two versions in the <kbd>pi_serial</kbd> and <kbd>pi_apply_async</kbd>&nbsp;functions (you can find their implementation in the <kbd>pi.py</kbd> file) and benchmark the execution speed, as follows:</p>
<pre><code class="lang-python">$ time python -c 'import pi; pi.pi_serial()'
real    0m0.734s
user    0m0.731s
sys     0m0.004s
$ time python -c 'import pi; pi.pi_apply_async()'
real    1m36.989s
user    1m55.984s
sys     0m50.386
</code></pre>
<p>As shown in the earlier&nbsp;benchmark, our first parallel version literally cripples our code. The reason is that the time spent doing the actual calculation is small compared to the overhead required to send and distribute the tasks to the workers.</p>
<p>To solve the issue, we have to make the overhead negligible compared to the calculation time. For example, we can ask each worker to handle more than one sample at a time, thus reducing the task communication overhead. We can write a&nbsp;<kbd>sample_multiple</kbd>&nbsp;function that processes more than one hit and modifies our parallel version by dividing our problem by 10; more intensive tasks are shown in the following code:</p>
<pre><code class="lang-python">    def sample_multiple(samples_partial): 
        return sum(sample() for i in range(samples_partial)) 

    n_tasks = 10 
    chunk_size = samples/n_tasks 
    pool = multiprocessing.Pool() 
    results_async = [pool.apply_async(sample_multiple, chunk_size) 
                     for i in range(n_tasks)] 
    hits = sum(r.get() for r in results_async)
</code></pre>
<p>We can wrap this in a function called <kbd>pi_apply_async_chunked</kbd> and run it as follows:</p>
<pre><code class="lang-python">$ time python -c 'import pi; pi.pi_apply_async_chunked()'
real    0m0.325s
user    0m0.816s
sys     0m0.008s
</code></pre>
<p>The results are much better; we more than doubled the speed of our program. You can also notice that the <kbd>user</kbd> metric is larger than <kbd>real</kbd>; the total CPU time is larger than the total time because more than one CPU worked at the same time. If you increase the number of samples, you will note that the ratio of communication to calculation decreases, giving even better speedups.</p>
<p>Everything is nice and simple when dealing with embarrassingly parallel problems. However,&nbsp;sometimes you have to share data between processes.</p>

<h3>Synchronization and locks</h3>
<p>Even if <kbd>multiprocessing</kbd> uses processes (with their own independent memory), it lets you define certain variables and arrays as shared memory. You can define a shared variable using <kbd>multiprocessing.Value</kbd>, passing its data type as a string (<kbd>i</kbd> integer, <kbd>d</kbd> double, <kbd>f</kbd> float, and so on). You can update the content of the variable through the <kbd>value</kbd> attribute, as shown in the following code snippet:</p>
<pre><code class="lang-python">    shared_variable = multiprocessing.Value('f') 
    shared_variable.value = 0
</code></pre>
<p>When using shared memory, you should be aware of concurrent accesses. Imagine that you have a shared integer variable and each process increments its value multiple times. You will define a process class as follows:</p>
<pre><code class="lang-python">    class Process(multiprocessing.Process): 

        def __init__(self, counter): 
            super(Process, self).__init__() 
            self.counter = counter 

        def run(self): 
            for i in range(1000): 
                self.counter.value += 1
</code></pre>
<p>You can initialize the shared variable in the main program and pass it to <kbd>4</kbd> processes, as shown in the following code:</p>
<pre><code class="lang-python">    def main(): 
        counter = multiprocessing.Value('i', lock=True) 
        counter.value = 0 

        processes = [Process(counter) for i in range(4)] 
        [p.start() for p in processes] 
        [p.join() for p in processes] # processes are done 
        print(counter.value)
</code></pre>
<p>If you run this program (<kbd>shared.py</kbd> in the code directory), you will note that the final value of <kbd>counter</kbd> is not 4000, but it has random values (on my machine, they are between 2000 and 2500). If we assume that the arithmetic is correct, we can conclude that there's a problem with the parallelization.</p>
<p>What happens is that multiple processes are trying to access the same shared variable at the same time. The situation is best explained by looking at the following figure. In a serial execution, the first process reads (the number <kbd>0</kbd>), increments it, and writes the new value (<kbd>1</kbd>); the second process reads the new value (<kbd>1</kbd>), increments it, and writes it again (<kbd>2</kbd>).</p>
<p>In the parallel execution, the two processes read (<kbd>0</kbd>), increment it, and write the value (<kbd>1</kbd>) at the same time, leading to a wrong answer:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000010.png" class="lazyload" /></p>
</div>
<p><br />To solve this problem, we need to synchronize the access to this variable so that only one process at a time can access, increment, and write the value on the shared variable. This feature is provided by the <kbd>multiprocessing.Lock</kbd> class. A lock can be acquired and released through the <kbd>acquire</kbd> method and <kbd>release</kbd>, or using the lock as a context manager. Since the lock can be acquired by only one process at a time, this method prevents multiple processes from&nbsp;executing the protected section of code at the same time.</p>
<p>We can define a global&nbsp;lock and use it as a context manager to restrict the access to the counter, as shown in the following code snippet:</p>
<pre><code class="lang-python">    lock = multiprocessing.Lock() 

    class Process(multiprocessing.Process): 

        def __init__(self, counter): 
            super(Process, self).__init__() 
            self.counter = counter 

        def run(self): 
            for i in range(1000): 
                with lock: # acquire the lock 
                    self.counter.value += 1 
                # release the lock
</code></pre>
<p>Synchronization primitives, such as locks, are essential to solve many problems, but they should be kept to a minimum to improve the performance of your program.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The <kbd>multiprocessing</kbd>&nbsp;module&nbsp;includes other communication and synchronization tools; you can refer to the official documentation at&nbsp;<a href="http://docs.python.org/3/library/multiprocessing.html">http://docs.python.org/3/library/multiprocessing.html</a>&nbsp;for a complete reference.</p>
</div>
</div>

<h2>Parallel Cython with OpenMP</h2>
<p>Cython provides a convenient interface to perform shared-memory parallel processing through <strong>OpenMP</strong>. This lets you write extremely efficient parallel code directly in Cython without having to create a C wrapper.</p>
<p>OpenMP is a specification and an API designed to write multithreaded, parallel programs. The OpenMP specification&nbsp;includes a series of C preprocessor directives to manage threads and provides communication patterns, load balancing, and other synchronization features. Several C/C++ and Fortran compilers (including GCC) implement the OpenMP API.</p>
<p>We can introduce the Cython parallel features with a small example. Cython provides a simple API based on OpenMP in the <kbd>cython.parallel</kbd> module. The simplest way to achieve parallelism is through&nbsp;<kbd>prange</kbd>, which is a construct that automatically distributes loop operations in multiple threads.</p>
<p>First of all, we can write the serial version of a program that computes the square of each element of a NumPy array in the <kbd>hello_parallel.pyx</kbd> file. We define a function, <kbd>square_serial</kbd>, that takes a buffer as input and populates an output array with the squares of the input array elements;&nbsp;<kbd>square_serial</kbd>&nbsp;is shown in the following code snippet:</p>
<pre><code class="lang-python">    import numpy as np 

    def square_serial(double[:] inp): 
        cdef int i, size 
        cdef double[:] out 
        size = inp.shape[0] 
        out_np = np.empty(size, 'double') 
        out = out_np 

        for i in range(size): 
            out[i] = inp[i]*inp[i] 

        return out_np
</code></pre>
<p>Implementing a parallel version of the loop over the array elements involves substituting the <kbd>range</kbd> call with <kbd>prange</kbd>. There's a caveat--to use <kbd>prange</kbd>, it is necessary that the body of the loop is interpreter-free. As already explained, we need to release the GIL and, since interpreter calls generally acquire the GIL, they need to be avoided to make use of threads.</p>
<p>In Cython, you can release the GIL using the&nbsp;<kbd>nogil</kbd>&nbsp;context, as follows:</p>
<pre><code class="lang-python">    with nogil: 
        for i in prange(size): 
            out[i] = inp[i]*inp[i]
</code></pre>
<p>Alternatively, you can use the option <kbd>nogil=True</kbd> of <kbd>prange</kbd> that will automatically wrap the loop body in a <kbd>nogil</kbd> block:</p>
<pre><code class="lang-python">    for i in prange(size, nogil=True): 
        out[i] = inp[i]*inp[i]
</code></pre>
<p>Attempts to call Python code in a <kbd>prange</kbd> block will produce an error. Prohibited operations include function calls, objects initialization, and so on. To enable&nbsp;such operations in a <kbd>prange</kbd> block (you may want to do so for debugging purposes), you have to re-enable the GIL using the <kbd>with gil</kbd> statement:</p>
<pre><code class="lang-python">    for i in prange(size, nogil=True): 
        out[i] = inp[i]*inp[i] 
        with gil:   
            x = 0 # Python assignment
</code></pre>
<p>We can now test our code by compiling it as a Python extension module. To enable OpenMP support, it is necessary to change the&nbsp;<kbd>setup.py</kbd>&nbsp;file so that it includes the compilation option <kbd>-fopenmp</kbd>&nbsp;. This can be achieved by using the <kbd>distutils.extension.Extension</kbd> class in <kbd>distutils</kbd> and passing it to <kbd>cythonize</kbd>. The complete <kbd>setup.py</kbd> file is as follows:</p>
<pre><code class="lang-python">    from distutils.core import setup 
    from distutils.extension import Extension 
    from Cython.Build import cythonize 

    hello_parallel = Extension('hello_parallel', 
                               ['hello_parallel.pyx'], 
                                extra_compile_args=['-fopenmp'], 
                                extra_link_args=['-fopenmp']) 

    setup( 
       name='Hello', 
       ext_modules = cythonize(['cevolve.pyx', hello_parallel]), 
    )
</code></pre>
<p>Using&nbsp;<kbd>prange</kbd>, we can easily parallelize the Cython version of our <kbd>ParticleSimulator</kbd>. The following code contains the&nbsp;<kbd>c_evolve</kbd> function of the <kbd>cevolve.pyx</kbd> Cython module that was written in Chapter 4,&nbsp;<em>C Performance with Cython</em>:</p>
<pre><code class="lang-python">    def c_evolve(double[:, :] r_i,double[:] ang_speed_i, 
                 double timestep,int nsteps): 

        # cdef declarations 

        for i in range(nsteps): 
            for j in range(nparticles): 
                # loop body
</code></pre>
<p>First,&nbsp;we will invert the order of the loops so that the outermost loop&nbsp;will be executed in parallel (each iteration is independent from the other). Since the particles don't interact with each other, we can change the order of iteration safely, as shown in the following snippet:</p>
<pre><code class="lang-python">        for j in range(nparticles): 
            for i in range(nsteps): 

                # loop body
</code></pre>
<p>Next, we will replace the <kbd>range</kbd> call of the outer loop with &nbsp;<kbd>prange</kbd> and remove calls that acquire the GIL. Since our code was already enhanced with static types, the <kbd>nogil</kbd>&nbsp;option can be applied safely as follows:</p>
<pre><code class="lang-python">    for j in prange(nparticles, nogil=True)
</code></pre>
<p>We can now compare the functions by wrapping them in the benchmark function to assess any performance improvement:</p>
<pre><code class="lang-python">    In [3]: %timeit benchmark(10000, 'openmp') # Running on 4 processors
    1 loops, best of 3: 599 ms per loop 
    In [4]: %timeit benchmark(10000, 'cython') 
    1 loops, best of 3: 1.35 s per loop
</code></pre>
<p>Interestingly, we achieved a 2x speedup by writing a parallel version using <kbd>prange</kbd>. &nbsp;</p>

<h2>Automatic parallelism</h2>
<p>As we mentioned earlier, normal Python programs have trouble achieving thread parallelism because of the GIL. So far, we worked around this problem using separate processes; starting a process, however, takes significantly more time and memory than starting a thread.</p>
<p>We also saw that sidestepping the Python environment allowed us to achieve a 2x speedup on an already fast Cython code. This strategy allowed us to achieve lightweight parallelism but required a separate compilation step. In this section, we will further explore this strategy using special libraries that are capable of automatically translating our code into a parallel version for efficient execution.</p>
<p>Examples of packages that implement automatic parallelism are the (by now) familiar JIT compilers &nbsp;<kbd>numexpr</kbd> and Numba. Other packages have been developed to automatically optimize and parallelize array and matrix-intensive expressions, which are crucial in specific numerical and machine learning applications.</p>
<p><strong>Theano</strong> is a project that allows you to define a mathematical expression on arrays (more generally, <em>tensors</em>), and compile them to a fast language, such as C or C++. Many of the operations that Theano implements are parallelizable and can run on both CPU and GPU.</p>
<p><strong>Tensorflow</strong> is another library that, similar to Theano, is targeted towards&nbsp;expression of array-intensive mathematical expression but, rather than translating the expressions to specialized C code, executes the operations on an&nbsp;efficient C++ engine.</p>
<p>Both Theano and Tensorflow are ideal when the problem at hand can be expressed in a chain&nbsp;of matrix and element-wise operations (such as <em>neural networks</em>).</p>

<h3>Getting started with Theano</h3>
<p>Theano is somewhat similar to a compiler but with the added bonuses of being able to express, manipulate, and optimize mathematical expressions as well as run code on CPU and GPU. Since 2010, Theano has improved release after release and has been adopted by several other Python projects as a way to automatically generate efficient computational models on the fly.</p>
<p>In Theano, you first <em>define</em> the function you want to run by specifying variables and transformation using a pure Python API. This specification will then be compiled to machine code for execution.</p>
<p>As a first example, let's examine how to implement a function that computes the square of a number. The input will be represented by a scalar variable,&nbsp;<kbd>a</kbd>, and then we will transform it to obtain its square, indicated by <kbd>a_sq</kbd>. In the following code, we will use the <kbd>T.scalar</kbd> function to define the variable and use the normal <kbd>**</kbd> operator to obtain a new variable:</p>
<pre><code class="lang-python">    import theano.tensor as T
    import theano as th
    a = T.scalar('a')
    a_sq = a ** 2
    print(a_sq)
    # Output:
    # Elemwise{pow,no_inplace}.0
</code></pre>
<p>As you can see, no specific value is computed and the transformation we apply is purely symbolic. In order to use this transformation, we need to generate a function. To compile a function, you can use the <kbd>th.function</kbd> utility that&nbsp;takes a list of the input variables as its&nbsp;first argument, and the output transformation (in our case <kbd>a_sq</kbd>) as its second argument:</p>
<pre><code class="lang-python">    compute_square = th.function([a], a_sq)
</code></pre>
<p>Theano will take some time and translate the expression to efficient C code and compile it, all in the background! The return value of <kbd>th.function</kbd> will be a ready-to-use Python function and its usage is demonstrated in the next line of code:</p>
<pre><code class="lang-python">    compute_square(2)
    4.0
</code></pre>
<p>Unsurprisingly, <kbd>compute_square</kbd>&nbsp;correctly returns the input value squared. Note, however, that the return type is not an integer (like the input type) but a floating point number. This is because the Theano default variable type is <kbd>float64</kbd>. you can verify that by inspecting the <kbd>dtype</kbd> attribute of the <kbd>a</kbd>&nbsp;variable:</p>
<pre><code class="lang-python">    a.dtype
    # Result: 
    # float64
</code></pre>
<p>The Theano behavior is very&nbsp;different compared&nbsp;to what we saw with&nbsp;Numba. Theano doesn't compile generic Python code and, also, doesn't do any type inference; defining Theano functions requires a more precise specification of the types involved.</p>
<p>The real power of Theano comes from its support for array expressions.&nbsp;Defining a one-dimensional vector can be done with the <kbd>T.vector</kbd> function; the returned variable supports broadcasting operations with the same semantics of&nbsp;NumPy arrays. For instance, we can take two vectors and compute the element-wise sum of their squares, as follows:</p>
<pre><code class="lang-python">    a = T.vector('a')
    b = T.vector('b')
    ab_sq = a**2 + b**2
    compute_square = th.function([a, b], ab_sq)

    compute_square([0, 1, 2], [3, 4, 5])
    # Result:
    # array([  9.,  17.,  29.])
</code></pre>
<p>The idea is, again, to use the Theano API as a mini-language to combine various Numpy array expressions will be compiled to efficient machine code.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>One of the selling points of Theano is its ability to perform arithmetic&nbsp;simplifications and automatic gradient calculations. For more information, refer to the official documentation (<a href="http://deeplearning.net/software/theano/introduction.html">http://deeplearning.net/software/theano/introduction.html</a>).</p>
</div>
</div>
<p>To demonstrate Theano functionality on a familiar use case, we can implement our parallel calculation of pi again. Our function will take a collection of two random coordinates as input and return the <kbd>pi</kbd> estimate. The input&nbsp;random numbers will be defined as vectors named <kbd>x</kbd> and <kbd>y</kbd>, and we can test their position inside the circle using standard element-wise operation that we will store in the <kbd>hit_test</kbd> variable:</p>
<pre><code class="lang-python">    x = T.vector('x')
    y = T.vector('y')

    hit_test = x ** 2 + y ** 2 &lt; 1
</code></pre>
<p>At this point, we need to count the number of <kbd>True</kbd> elements in <kbd>hit_test</kbd>, which can be done taking its sum (it will be implicitly cast to integer). &nbsp;To obtain the pi estimate, we finally need to calculate the ratio of hits versus the total number of trials. The calculation is illustrated in the following code block:</p>
<pre><code class="lang-python">    hits = hit_test.sum()
    total = x.shape[0]
    pi_est = 4 * hits/total
</code></pre>
<p>We can benchmark the execution of the Theano implementation using <kbd>th.function</kbd> and the <kbd>timeit</kbd> module. In our test, we will pass two arrays of size 30,000 and use the <kbd>timeit.timeit</kbd>&nbsp;utility to execute the <kbd>calculate_pi</kbd> function multiple times:</p>
<pre><code class="lang-python">    calculate_pi = th.function([x, y], pi_est)

    x_val = np.random.uniform(-1, 1, 30000)
    y_val = np.random.uniform(-1, 1, 30000)

    import timeit
    res = timeit.timeit("calculate_pi(x_val, y_val)", 
    "from __main__ import x_val, y_val, calculate_pi", number=100000)
    print(res)
    # Output:
    # 10.905971487998613
</code></pre>
<p>The serial execution of this function takes about 10 seconds. Theano is capable of automatically parallelizing the code by implementing element-wise and matrix operations using specialized packages, such as OpenMP and the <strong>Basic Linear Algebra Subprograms</strong>&nbsp;(<strong>BLAS</strong>)&nbsp;linear algebra routines. Parallel execution can be enabled using configuration options.</p>
<p>In Theano, you can set up configuration options by modifying variables in the <kbd>theano.config</kbd> object&nbsp;at import time. For example, you can issue the following commands&nbsp;to enable OpenMP support:</p>
<pre><code class="lang-python">import theano
theano.config.openmp = True
theano.config.openmp_elemwise_minsize = 10
</code></pre>
<p>The parameters relevant to OpenMP&nbsp;are as follows:</p>
<ul>
<li><kbd>openmp_elemwise_minsize</kbd>: This&nbsp;is an integer number that represents the minimum size of the arrays where element-wise parallelization should be enabled (the overhead of the parallelization can harm performance for small arrays)</li>
<li><kbd>openmp</kbd>: This is a Boolean flag that controls the activation of OpenMP compilation (it should be activated by default)</li>
</ul>
<p>Controlling the number of threads assigned for OpenMP execution can be done by setting the&nbsp;<kbd>OMP_NUM_THREADS</kbd>&nbsp;environmental variable before executing the code.</p>
<p>We can now write&nbsp;a simple&nbsp;benchmark to demonstrate the OpenMP usage in practice. In a file <kbd>test_theano.py</kbd>, we will put the complete&nbsp;code&nbsp;for the pi estimation&nbsp;example:</p>
<pre><code class="lang-python">    # File: test_theano.py
    import numpy as np
    import theano.tensor as T
    import theano as th
    th.config.openmp_elemwise_minsize = 1000
    th.config.openmp = True

    x = T.vector('x')
    y = T.vector('y')

    hit_test = x ** 2 + y ** 2 &lt;= 1
    hits = hit_test.sum()
    misses = x.shape[0]
    pi_est = 4 * hits/misses

    calculate_pi = th.function([x, y], pi_est)

    x_val = np.random.uniform(-1, 1, 30000)
    y_val = np.random.uniform(-1, 1, 30000)

    import timeit
    res = timeit.timeit("calculate_pi(x_val, y_val)", 
                        "from __main__ import x_val, y_val, 
                        calculate_pi", number=100000)
    print(res)
</code></pre>
<p>At this point, we can run the code from the command line and assess the scaling with an increasing number of&nbsp;threads by setting the <kbd>OMP_NUM_THREADS</kbd>&nbsp;environment variable:</p>
<pre><code class="lang-python">    $ OMP_NUM_THREADS=1 python test_theano.py
    10.905971487998613
    $ OMP_NUM_THREADS=2 python test_theano.py
    7.538279129999864
    $ OMP_NUM_THREADS=3 python test_theano.py
    9.405846934998408
    $ OMP_NUM_THREADS=4 python test_theano.py
    14.634153957000308
</code></pre>
<p>Interestingly, there is a small speedup when using two threads, but the performance degrades quickly as we increase their number. This means that for this input size, it is not advantageous to use more than two threads as the price you pay to start new threads and synchronize their shared data is higher than the speedup that you can obtain from the parallel execution.&nbsp;</p>
<p>Achieving good parallel performance can be tricky as it will depend on the specific operations and how they access the underlying data. As a general rule, measuring the performance of a parallel program is crucial and&nbsp;obtaining&nbsp;substantial speedups&nbsp;is a work of trial and error.</p>
<p>As an example, we can see that the parallel performance quickly degrades using a slightly different code. In our hit test, we used the <kbd>sum</kbd> method directly and relied on the explicit casting of the&nbsp;<kbd>hit_tests</kbd> Boolean array. If we make the cast explicit, Theano will generate a slightly different code that benefits less from multiple threads. We can modify the <kbd>test_theano.py</kbd> file to verify this effect:</p>
<pre><code class="lang-python">    # Older version
    # hits = hit_test.sum()
    hits = hit_test.astype('int32').sum()
</code></pre>
<p>If we rerun our benchmark, we see that the number of threads does not affect the running time significantly. Despite that, the timings improved considerably as compared to the original version:</p>
<pre><code class="lang-python">    $ OMP_NUM_THREADS=1 python test_theano.py
    5.822126664999814
    $ OMP_NUM_THREADS=2 python test_theano.py
    5.697357518001809
    $ OMP_NUM_THREADS=3 python test_theano.py 
    5.636914656002773
    $ OMP_NUM_THREADS=4 python test_theano.py
    5.764030176000233
</code></pre>

<h4>Profiling Theano</h4>
<p>Given the importance of measuring and analyzing performance, Theano provides powerful and informative profiling tools. To generate profiling data, the only modification needed is the addition of the <kbd>profile=True</kbd> option to&nbsp;<kbd>th.function</kbd>:</p>
<pre><code class="lang-python">    calculate_pi = th.function([x, y], pi_est, profile=True)
</code></pre>
<p>The profiler will collect data as the function is being run (for example,&nbsp;through <kbd>timeit</kbd> or direct invocation). The profiling summary can be printed to output by&nbsp;issuing the <kbd>summary</kbd> command, as follows:</p>
<pre><code class="lang-python">    calculate_pi.profile.summary()
</code></pre>
<p>To generate profiling data, we can rerun our script after adding the <kbd>profile=True</kbd> option (for this experiment, we will set the <kbd>OMP_NUM_THREADS</kbd> environmental variable to 1). Also, we will revert our script to the version that performed the casting of <kbd>hit_tests</kbd> implicitly.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>You can also set up profiling globally using the <kbd>config.profile</kbd> option.</p>
</div>
</div>
<p>The output printed by <kbd>calculate_pi.profile.summary()</kbd> is quite long and informative. A part of it is reported in the next block of text. The output is comprised of three sections that refer to timings sorted by <kbd>Class</kbd>, <kbd>Ops</kbd>, and <kbd>Apply</kbd>. In our example, we are concerned with&nbsp;<kbd>Ops</kbd>, which&nbsp;roughly maps to the functions used in the Theano compiled code. As you can see, roughly 80% of the time is spent in taking the element-wise square and sum of the two numbers, while the rest of the time is spent calculating the sum:</p>
<pre><code class="lang-python">Function profiling
==================
  Message: test_theano.py:15

... other output
Time in 100000 calls to Function.__call__: 1.015549e+01s
... other output

Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
.... timing info by class

Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;
  80.0%    80.0%       6.722s       6.72e-05s     C     100000        1   Elemwise{Composite{LT((sqr(i0) + sqr(i1)), i2)}}
  19.4%    99.4%       1.634s       1.63e-05s     C     100000        1   Sum{acc_dtype=int64}
   0.3%    99.8%       0.027s       2.66e-07s     C     100000        1   Elemwise{Composite{((i0 * i1) / i2)}}
   0.2%   100.0%       0.020s       2.03e-07s     C     100000        1   Shape_i{0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;
... timing info by apply
</code></pre>
<p>This information is consistent with what was found in our first benchmark. The code went from&nbsp;about 11 seconds to roughly 8 seconds when two threads were used. From these numbers, we can analyze how the time was spent.</p>
<p>Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing element-wise operations. This means that, in perfectly parallel conditions, the increase in speed by adding two threads will be 4.4 seconds. In this scenario, the theoretical execution time will be 6.6 seconds. Considering that we obtained a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds) for the thread usage.</p>

<h3>Tensorflow</h3>
<p>Tensorflow is another library designed for fast numerical calculations and automatic&nbsp;parallelism. It was released as an open source project&nbsp;by Google in 2015. Tensorflow works by building mathematical expressions similar to Theano, except that the computation is not compiled to machine code but is executed on an external engine written in C++. Tensorflow supports execution and deployment of parallel codes on one or more CPUs and GPUs.</p>
<p>The usage of Tensorflow is quite similar to that of Theano. To create a variable in Tensorflow, you can use the <kbd>tf.placeholder</kbd>&nbsp;function that takes a data type as input:</p>
<pre><code class="lang-python">    import tensorflow as tf

    a = tf.placeholder('float64')
</code></pre>
<p>Tensorflow&nbsp;mathematical expressions can be expressed quite similarly to Theano, except for a few different naming conventions as well as a more restricted support for the NumPy semantics.</p>
<p>Tensorflow doesn't compile functions to C and then machine code like Theano, but serializes the defined mathematical functions (the data structure containing variables and transformations is called <strong>computation graph</strong>)&nbsp;and executes them on specific devices. The configuration of devices and context can be done using the&nbsp;<kbd>tf.Session</kbd> object.</p>
<p>Once the desired expression is defined, a <kbd>tf.Session</kbd> needs to be initialized and can be used to execute computation graphs using the <kbd>Session.run</kbd>&nbsp;method. In the following example, we demonstrate the usage of the Tensorflow API to implement&nbsp;a simple element-wise sum of squares:</p>
<pre><code class="lang-python">    a = tf.placeholder('float64')
    b = tf.placeholder('float64')
    ab_sq = a**2 + b**2

    with tf.Session() as session:
        result = session.run(ab_sq, feed_dict={a: [0, 1, 2], 
                                               b: [3, 4, 5]})
        print(result)
    # Output:
    # array([  9.,  17.,  29.])
</code></pre>
<p>Parallelism in Tensorflow&nbsp;is achieved automatically by its smart execution engine, and it generally works well without much fiddling. However, note that it is&nbsp;mostly suited for deep learning workloads that involve the definition of complex functions that use a lot of matrix multiplications and calculate their gradient.</p>
<p>We can now replicate the estimation of the&nbsp;pi example using&nbsp;Tensorflow capabilities and benchmark its execution speed and parallelism against the Theano implementation. What we will do is this:</p>
<ul>
<li>Define our <kbd>x</kbd>&nbsp;and&nbsp;<kbd>y</kbd> variables and perform a hit test using broadcasted operations.</li>
<li>Calculate the sum of <kbd>hit_tests</kbd> using the&nbsp;<kbd>tf.reduce_sum</kbd>&nbsp;function.</li>
<li>Initialize a <kbd>Session</kbd> object with the <kbd>inter_op_parallelism_threads</kbd> and <kbd>intra_op_parallelism_threads</kbd>&nbsp;configuration options. These options control the number of threads used for different classes&nbsp;of parallel operations. Note that the first <kbd>Session</kbd> created with such options&nbsp;sets the&nbsp;number of threads for the whole script (even future <kbd>Session</kbd> instances).</li>
</ul>
<p>We can now write a script name,&nbsp;<kbd>test_tensorflow.py</kbd>, containing the&nbsp;following code. Note that the number of threads is passed as the first argument of the script (<kbd>sys.argv[1]</kbd>):</p>
<pre><code class="lang-python">    import tensorflow as tf
    import numpy as np
    import time
    import sys

    NUM_THREADS = int(sys.argv[1])
    samples = 30000

    print('Num threads', NUM_THREADS)
    x_data = np.random.uniform(-1, 1, samples)
    y_data = np.random.uniform(-1, 1, samples)

    x = tf.placeholder('float64', name='x')
    y = tf.placeholder('float64', name='y')

    hit_tests = x ** 2 + y ** 2 &lt;= 1.0
    hits = tf.reduce_sum(tf.cast(hit_tests, 'int32'))

    with tf.Session
        (config=tf.ConfigProto
            (inter_op_parallelism_threads=NUM_THREADS,
             intra_op_parallelism_threads=NUM_THREADS)) as sess:
        start = time.time()
        for i in range(10000):
            sess.run(hits, {x: x_data, y: y_data})
        print(time.time() - start)
</code></pre>
<p>If we run the script multiple times with different values of <kbd>NUM_THREADS</kbd>, we see that the performance is quite similar to Theano and that&nbsp;the speedup increased by parallelization is quite modest:</p>
<pre><code class="lang-python">    $ python test_tensorflow.py 1
    13.059704780578613
    $ python test_tensorflow.py 2
    11.938535928726196
    $ python test_tensorflow.py 3
    12.783955574035645
    $ python test_tensorflow.py 4
    12.158143043518066
</code></pre>
<p>The main advantage of using software packages such as Tensorflow and Theano is the support for parallel matrix operations that are commonly used&nbsp;in machine learning algorithms. This is very effective because those operations can achieve impressive performance gains on GPU hardware that is designed to perform these operations&nbsp;with high throughput.&nbsp;</p>

<h3>Running code on a GPU</h3>
<p>In this subsection, we will demonstrate the usage of a GPU with Theano and Tensorflow. As an example, we will benchmark the execution of a very simple matrix multiplication on the GPU and compare it to its running time on a CPU.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>The code in this subsection requires the possession of a GPU. For learning purposes, it is possible to use the Amazon EC2 service (<a href="https://aws.amazon.com/ec2">https://aws.amazon.com/ec2</a>) to request a GPU-enabled instance.</p>
</div>
</div>
<p>The following code performs a simple matrix multiplication using Theano. We use the <kbd>T.matrix</kbd> function to initialize a two-dimensional array, and then we use&nbsp;the <kbd>T.dot</kbd>&nbsp;method to perform the matrix multiplication:</p>
<pre><code class="lang-python">    from theano import function, config
    import theano.tensor as T
    import numpy as np
    import time

    N = 5000

    A_data = np.random.rand(N, N).astype('float32')
    B_data = np.random.rand(N, N).astype('float32')

    A = T.matrix('A')
    B = T.matrix('B')

    f = function([A, B], T.dot(A, B))

    start = time.time()
    f(A_data, B_data)

    print("Matrix multiply ({}) took {} seconds".format(N, time.time() - start))
    print('Device used:', config.device)
</code></pre>
<p>It is possible to ask Theano&nbsp;to execute this code on a GPU by setting the <kbd>config.device=gpu</kbd>&nbsp;option. For added convenience, we can set up the configuration value from the command line using the <kbd>THEANO_FLAGS</kbd> environmental variable, shown as follows. After copying the previous code in the <kbd>test_theano_matmul.py</kbd>&nbsp;file, we can benchmark the execution time by issuing the following command:</p>
<pre><code class="lang-python">    $ THEANO_FLAGS=device=gpu python test_theano_gpu.py 
    Matrix multiply (5000) took 0.4182612895965576 seconds
    Device used: gpu
</code></pre>
<p>We can analogously run the same code on the CPU using the <kbd>device=cpu</kbd> configuration option:</p>
<pre><code class="lang-python">    $ THEANO_FLAGS=device=cpu python test_theano.py 
    Matrix multiply (5000) took 2.9623231887817383 seconds
    Device used: cpu
</code></pre>
<p>As you can see, the GPU is 7.2 times faster than the CPU version for this example!</p>
<p>For comparison, we may benchmark equivalent code using Tensorflow. The implementation of a Tensorflow version is reported in the next code snippet. The main differences with the Theano version are as follows:</p>
<ul>
<li>The usage of the <kbd>tf.device</kbd> config manager that serves to specify the target device (<kbd>/cpu:0</kbd> or <kbd>/gpu:0</kbd>)</li>
<li>The matrix multiplication is performed using the <kbd>tf.matmul</kbd> operator:</li>
</ul>
<pre><code class="lang-python">    import tensorflow as tf
    import time
    import numpy as np
    N = 5000

    A_data = np.random.rand(N, N)
    B_data = np.random.rand(N, N)

    # Creates a graph.

    with tf.device('/gpu:0'):
        A = tf.placeholder('float32')
        B = tf.placeholder('float32')

        C = tf.matmul(A, B)

    with tf.Session() as sess:
        start = time.time()
        sess.run(C, {A: A_data, B: B_data})
        print('Matrix multiply ({}) took: {}'.format(N, time.time() - start))
</code></pre>
<p>If we run the <kbd>test_tensorflow_matmul.py</kbd> script with the appropriate <kbd>tf.device</kbd> option, we obtain the following timings:</p>
<pre><code class="lang-python">    # Ran with tf.device('/gpu:0')
    Matrix multiply (5000) took: 1.417285680770874

    # Ran with tf.device('/cpu:0')
    Matrix multiply (5000) took: 2.9646761417388916 
</code></pre>
<p>As you can see, the performance gain is substantial (but not as good as the Theano version) in this simple case.</p>
<p>Another way to achieve automatic GPU computation is the now familiar Numba. With Numba, it is possible to compile Python code to programs that can be run on a GPU. This flexibility allows for advanced GPU programming as well as more simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready, generalized universal functions.</p>
<p>In the next example, we will demonstrate how to write a universal function that applies an exponential function on two numbers and sums the results. As we already saw&nbsp;in Chapter 5, <em>Exploring Compilers</em>&nbsp;this can be accomplished using the <kbd>nb.vectorize</kbd> function (we'll also specify the <kbd>cpu</kbd> target explicitly):</p>
<pre><code class="lang-python">    import numba as nb
    import math
    @nb.vectorize(target='cpu')
    def expon_cpu(x, y):
        return math.exp(x) + math.exp(y)
</code></pre>
<p>The <kbd>expon_cpu</kbd> universal function can be compiled for the GPU device using the&nbsp;<kbd>target='cuda'</kbd>&nbsp;option. Also, note that it is necessary to specify the input types for CUDA universal functions. The implementation of <kbd>expon_gpu</kbd> is as follows:</p>
<pre><code class="lang-python">    @nb.vectorize(['float32(float32, float32)'], target='cuda')
    def expon_gpu(x, y):
        return math.exp(x) + math.exp(y)
</code></pre>
<p>We can now benchmark the execution of the two functions by applying the functions on two arrays of size 1,000,000. Also, note that we execute the function before measuring the timings to trigger the Numba just-in-time compilation:</p>
<pre><code class="lang-python">    import numpy as np
    import time

    N = 1000000
    niter = 100

    a = np.random.rand(N).astype('float32')
    b = np.random.rand(N).astype('float32')

    # Trigger compilation
    expon_cpu(a, b)
    expon_gpu(a, b)

    # Timing
    start = time.time()
    for i in range(niter):
       expon_cpu(a, b)
    print("CPU:", time.time() - start)

    start = time.time()
    for i in range(niter): 
        expon_gpu(a, b) 
    print("GPU:", time.time() - start) 
    # Output:
    # CPU: 2.4762887954711914
    # GPU: 0.8668839931488037
</code></pre>
<p>Thanks to the GPU execution, we were able to achieve a 3x speedup over the CPU version. Note that transferring data on the GPU is quite expensive; therefore, GPU execution becomes advantageous only for very&nbsp;large arrays.</p>

<h2>Summary</h2>
<p>Parallel processing is an effective way to improve performance on large datasets. Embarrassingly parallel problems are excellent candidates for parallel execution that can be&nbsp;easily implemented to achieve good performance&nbsp;scaling.</p>
<p>In this chapter, we illustrated the basics of parallel programming in Python. We learned how to circumvent Python threading limitation by spawning processes using the tools available in the Python standard library. We also explored how to implement a multithreaded program using&nbsp;Cython and OpenMP.</p>
<p>For more complex problems, we learned how to use the Theano, Tensorflow, and Numba packages to automatically compile array-intensive expressions for parallel execution on CPU and GPU devices.</p>
<p>In the next chapter, we will learn how to write and execute parallel programs on multiple processors and machines using libraries such as dask and PySpark.</p>

</div>


<!--Chapter 8-->


<div class="chapter" data-chapter-number="8">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 8 </span></div>
<h1 class="chaptertitle">Distributed Processing</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>In the last&nbsp;chapter, we introduced the concept of parallel processing and learned how to leverage multicore processors and GPUs. Now, we can step up our game a bit and turn our attention on distributed processing, which&nbsp;involves executing tasks across multiple machines to solve a certain problem.</p>
<p>In this chapter, we will illustrate the challenges, use cases, and examples of how to run&nbsp;code&nbsp;on a cluster of computers.&nbsp;Python offers easy-to-use and reliable packages for distribute processing, which&nbsp;will allow us to implement scalable and fault-tolerant code with relative ease.</p>
<p>The list of topics for this chapter is as follows:</p>
<ul>
<li>Distributed computing and the MapReduce model</li>
<li>Directed Acyclic Graphs with Dask</li>
<li>Writing parallel code with Dask's&nbsp;<kbd>array</kbd>, <kbd>Bag</kbd>, and <kbd>DataFrame</kbd> data structures</li>
<li>Distributing parallel algorithms with Dask Distributed</li>
<li>An introduction to PySpark</li>
<li>Spark's Resilient Distributed Datasets and DataFrame</li>
<li>Scientific computing with <kbd>mpi4py</kbd></li>
</ul>

<h2>Introduction to distributed computing</h2>
<p>In today's world, computers, smartphones, and other devices&nbsp;have become an integral part of our lives. Every day, massive quantities of data is produced. Billions of people access services on the Internet, and companies are constantly collecting data to learn about their users to better target products and improve user experience.&nbsp;</p>
<p>Handling&nbsp;this ever increasing amount of data presents substantial challenges. Large companies and organizations often build clusters of machines designed to store, process, and analyze large and complex datasets. Similar datasets are also produced in data-intensive fields such as environmental sciences and health care. These&nbsp;large-scale datasets have been recently called <strong>big data</strong>. The analysis techniques applied to big data usually involve a combination of machine learning, information retrieval, and visualization.</p>
<p>Computing clusters have been used for decades in scientific computing, where the study of complex problems requires the use of parallel algorithms executed on high-performance distributed systems. For such applications, universities and other organizations provide and manage supercomputers for research and engineering purposes. Applications that run on supercomputers are generally focused on highly numerical workloads, such as protein and molecular simulations, quantum mechanical calculations, climate models, and much more.</p>
<p>The challenges of programming for distributed systems are apparent if we think back on how the cost of communication increases as we distribute data and computational tasks&nbsp;across a local network. Network transfers are extremely slow compared to the processor speed, and when using distributed processing, it is even more important to keep network communications as limited as possible. This can be achieved using a few different strategies that favor&nbsp;local data processing and resort to data transfers only when strictly necessary.</p>
<p>Other challenges of distributed processing involve the general unreliability of computer networks. When you think that in a computing cluster there may be thousands of machines, it becomes clear that (probabilistically speaking) faulty nodes become very common. For this reason, distributed systems need to be able to handle node failures gracefully and without disrupting the ongoing work.&nbsp;Luckily, companies have invested a great deal of resources in developing fault-tolerant distributed engines&nbsp;that take care of these aspects automatically.</p>

<h3>An introduction to MapReduce</h3>
<p><strong>MapReduce</strong> is a programming model that allows you to express algorithms for efficient execution on a distributed system. The MapReduce model was first introduced by Google in 2004 (<a href="https://research.google.com/archive/mapreduce.html">https://research.google.com/archive/mapreduce.html</a>), as a way&nbsp;to automatically partition datasets&nbsp;over different machines and for automatic local processing and the communication between <em>cluster nodes</em>.</p>
<p>The MapReduce framework was used in cooperation with a distributed filesystem, the <strong>Google File System</strong> (GFS or GoogleFS), which was designed to partition and replicate data across the computing cluster. Partitioning was useful for storing and processing datasets that wouldn't fit on a single node while replication ensured that the system was able to handle failures gracefully. MapReduce was used by Google, in conjunction with GFS, for indexing of their web pages. Later on,&nbsp;the MapReduce and GFS concepts were&nbsp;implemented by Doug Cutting (at the time, an employee at Yahoo!), resulting in the first versions of the <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) and Hadoop MapReduce.</p>
<p>The programming model exposed by MapReduce is actually quite simple. The idea is to express the computation as a combination of two, fairly generic, steps: <em>Map</em> and <em>Reduce</em>. Some readers will probably be familiar with Python's <kbd>map</kbd> and <kbd>reduce</kbd> functions; however, in the context of MapReduce, the Map and Reduce steps are capable of representing a broader class of operations.</p>
<p>Map takes a collection of data as input and produces a <em>transformation</em> on this data. What is generally emitted by Map is a series of key value pairs that can be passed to a Reduce step. The Reduce step will aggregate items with the same key and apply a function to the collection to form a usually smaller collection of values.</p>
<p>The estimation of <em>pi</em>, which was shown in the last&nbsp;chapter, can be trivially converted using a series of Map and Reduce steps. In that case, the input was a collection of pairs of random numbers. The transformation (Map step) was the hit test, and the Reduce step was counting the number of times the hit test was True.</p>
<p>The prototypical example of the MapReduce model is the implementation of a word count; the program takes a series of documents as input, and returns, for each word, the total number of occurrences in the document collection. The following figure illustrates&nbsp;the Map and Reduce steps of the word count program. On the left, we have the input documents. The Map operation will produce a (key, value) entry where the first element is the word and the second element is <strong>1</strong> (that's because every word contributes <strong>1</strong> to the final count).</p>
<p>We then perform the reduce operation to aggregate all the elements of the same key and produce the global count for each of the words. In the figure, we can see how all values of the items with key <strong>the</strong>&nbsp;are summed to produce the final entry (<strong>the, 4</strong>): &nbsp;</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000030.png" class="lazyload" /></p>
</div>
<p>If we implement our algorithm using the Map and Reduce operation, the framework implementation will ensure that data production and aggregation is done efficiently, by limiting the communication between nodes through clever algorithms.</p>
<p>However, how does MapReduce manage to keep communication to a minimum? Let's go through the journey of a MapReduce task. Imagine that you have a cluster with two&nbsp;nodes, and a partition of the data (this is usually found locally in each node) is loaded in each node from disk and is ready for processing. A mapper process is created in each node and processes the data to produce the intermediate results.&nbsp;</p>
<p>Next, it is necessary to send the data to the reducer for further processing. In order to do this, however, it is necessary that all the items that possess the same key are shipped to the same reducer. This operation is called <strong>shuffling</strong> and is the principal communication task in the MapReduce model:</p>
<div>&nbsp;
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000021.png" class="lazyload" /></p>
</div>
<p>Note that, before the data exchange happens, it is necessary to assign a subset of keys to&nbsp;each reducer; this step is called <strong>partitioning</strong>. &nbsp;Once a reducer receives its own partition of keys, it is free to process&nbsp;data and write the resulting output on disk.</p>
<p>The MapReduce framework (through the Apache Hadoop project) has been extensively used in its original form by many companies and organizations. More recently, new frameworks that extend the ideas introduced by MapReduce have been developed to create systems able to express more complex workflows, to use memory more efficiently and to support a lean and efficient execution of distributed tasks.</p>
<p>In the following sections, we will describe two of the most used libraries in the Python distributed landscape:&nbsp;Dask and PySpark.</p>

<h2>Dask</h2>
<p><strong>Dask</strong> is a project of Continuum Analytics (the same company that's responsible for Numba and the <kbd>conda</kbd> package manager) and a pure Python library for parallel and distributed computation. It excels at performing data analysis tasks and is very well integrated in the Python ecosystem.</p>
<p>Dask was initially conceived as a package for bigger-than-memory calculations on a single machine. Recently, with the Dask Distributed&nbsp;project, its code has been adapted to execute tasks on a cluster with excellent performance and fault-tolerance capabilities. It supports MapReduce-style tasks as well as complex numerical algorithms.</p>

<h3>Directed Acyclic Graphs</h3>
<p>The idea behind&nbsp;Dask is quite similar to what we already saw in the last&nbsp;chapter with Theano and Tensorflow. We can use a familiar Pythonic API to build an execution plan, and the framework will automatically split the workflow into tasks that will be shipped and executed on multiple processes or computers.</p>
<p>Dask expresses its variables and operations as a <strong>Directed Acyclic Graph</strong> (<strong>DAG</strong>) that&nbsp;can be represented through&nbsp;a simple Python dictionary. To briefly illustrate how this works, we will implement the sum of two numbers with Dask. We will define our computational graph by storing the values of our input variables in the dictionary. The <kbd>a</kbd> and <kbd>b</kbd> input variables&nbsp;will be given a value of <kbd>2</kbd>:</p>
<pre><code class="lang-python">    dsk = {
      "a" : 2,
      "b" : 2,
    }
</code></pre>
<p>Each variable represents a node in the DAG. The next step necessary to build our DAG is the execution of operations on the nodes we just defined. In Dask, a task can be defined by placing a tuple containing a Python function and its positional arguments&nbsp;in the <kbd>dsk</kbd> dictionary. To implement a <kbd>sum</kbd>, we can add a new node, named <kbd>result</kbd>, (the actual name is completely arbitrary) with a tuple containing the function we intend to execute, followed by its arguments. This is illustrated in the following code:</p>
<pre><code class="lang-python">    dsk = {
      "a" : 2,
      "b" : 2,
      "result": (lambda x, y: x + y, "a", "b")
    }
</code></pre>
<p>For better style and clarity, we can calculate the sum by replacing the <kbd>lambda</kbd> statement with the standard <kbd>operator.add</kbd>&nbsp;library function:</p>
<pre><code class="lang-python">    from operator import add
    dsk = {
      "a" : 2,
      "b" : 2,
      "result": (add, "a", "b")
    }
</code></pre>
<p>It's important to note that the arguments we intend to pass to the function are the&nbsp;<kbd>"a"</kbd> and <kbd>"b"</kbd>&nbsp;strings, which refer to the <kbd>a</kbd> and <kbd>b</kbd> nodes in the graph. Note that we didn't use any Dask-specific functions to define the DAG; this is the&nbsp;first indication of how the framework is flexible and lean since all&nbsp;manipulations are performed on simple and familiar Python dictionaries.</p>
<p>The execution of tasks is performed by a scheduler, which is a function that takes a DAG and the task or tasks we'd like to perform and returns the computed value. The default Dask scheduler is the <kbd>dask.get</kbd>&nbsp;function, which can be used as follows:</p>
<pre><code class="lang-python">    import dask

    res = dask.get(dsk, "result")
    print(res)
    # Output:
    # 4 
</code></pre>
<p>All the complexity is hidden behind the scheduler, which will take care of distributing the tasks across&nbsp;threads, processes, or even different machines. The <kbd>dask.get</kbd> scheduler is a synchronous and serial implementation that is useful for testing and debugging purposes.</p>
<p>Defining graphs using a simple dictionary is useful to understand how Dask does its magic and for debugging purposes. Raw dictionaries can also be used to implement more complex algorithms not covered by the Dask API. Now, we will learn how Dask is capable of generating tasks automatically through a familiar NumPy- and Pandas-like interface.</p>

<h3>Dask arrays</h3>
<p>One of the main use-cases of Dask is the automatic generation of parallel array operations, which greatly simplifies the handling of arrays that don't fit into memory. The strategy employed by Dask is to split the array into a number of subunits that, in Dask array terminology, are called <strong>chunks</strong>.</p>
<p>Dask implement a NumPy-like interface for arrays in the <kbd>dask.array</kbd> module (which we will abbreviate as&nbsp;<kbd>da</kbd>). An array can be created from a NumPy-like array using the <kbd>da.from_array</kbd>&nbsp;function, which requires the specification of a chunk size. The <kbd>da.from_array</kbd> function will return a <kbd>da.array</kbd> object&nbsp;that will handle the splitting of&nbsp;the original array into subunits of the specified chunk size. In the following example, we create an array of <kbd>30</kbd> elements, and we split it into chunks with <kbd>10</kbd> elements each:</p>
<pre><code class="lang-python">    import numpy as np
    import dask.array as da

    a = np.random.rand(30)

    a_da = da.from_array(a, chunks=10)
    # Result:
    # dask.array&lt;array-4..., shape=(30,), dtype=float64, chunksize=(10,)&gt;
</code></pre>
<p>The <kbd>a_da</kbd> variable maintains a Dask graph that&nbsp;can be accessed using the <kbd>dask</kbd> attribute. To understand what Dask does under the hood, we can inspect its content. In the following example, we can see that the Dask graph contains four nodes. One of them is the source array, denoted by the <kbd>'array-original-4c76'</kbd>&nbsp;key, the other three keys in the <kbd>a_da.dask</kbd> dictionary are tasks that are&nbsp;used to access a chunk of the original array using the <kbd>dask.array.core.getarray</kbd> function and, as you can see, each task extracts a slice of 10 elements:</p>
<pre><code class="lang-python">    dict(a_da.dask)
    # Result
    {('array-4c76', 0): (&lt;function dask.array.core.getarray&gt;, 
                         'array-original-4c76',
                         (slice(0, 10, None),)),
     ('array-4c76', 2): (&lt;function dask.array.core.getarray&gt;,
                         'array-original-4c76',
                         (slice(20, 30, None),)),
     ('array-4c76', 1): (&lt;function dask.array.core.getarray&gt;, 
                         'array-original-4c76',
                         (slice(10, 20, None),)),
     'array-original-4c76': array([ ... ])
    }


</code></pre>
<p>If we perform an operation on the <kbd>a_da</kbd> array, Dask will generate more subtasks that operate on the smaller chunks, opening the possibility of achieving parallelism. The interface exposed by <kbd>da.array</kbd> is compatible with common NumPy semantics and broadcasting rules. The complete code, shown as follows, demonstrates the good compatibility of Dask with NumPy broadcasting rules, element-wise operations, and other methods:</p>
<pre><code class="lang-python">    N = 10000
    chunksize = 1000 

    x_data = np.random.uniform(-1, 1, N)
    y_data = np.random.uniform(-1, 1, N)

    x = da.from_array(x_data, chunks=chunksize)
    y = da.from_array(y_data, chunks=chunksize)

    hit_test = x ** 2 + y ** 2 &lt; 1

    hits = hit_test.sum()
    pi = 4 * hits / N
</code></pre>
<p>The value of pi can be calculated using the <kbd>compute</kbd> method, which can also be called with the&nbsp;<kbd>get</kbd>&nbsp;optional argument&nbsp;to specify a different scheduler (by default,&nbsp;<kbd>da.array</kbd> uses a multithreaded scheduler):</p>
<pre><code class="lang-python">    pi.compute() # Alternative: pi.compute(get=dask.get)
    # Result:
    # 3.1804000000000001
</code></pre>
<p>Even deceptively simple algorithms, such as the estimation of pi, may require a lot of tasks to be executed. Dask provides utilities to visualize the computational&nbsp;graph. The following figure shows part of the Dask graph for the estimation of pi, which can be obtained by executing the method <kbd>pi.visualize()</kbd>. In the graph, circles refer to transformations that get applied on the nodes, which are represented as rectangles.&nbsp;This example helps us to get a feel of the complexity of the Dask graph and to appreciate the scheduler's job of creating&nbsp;an efficient execution plan that includes proper ordering of tasks and the selection of tasks that will be executed in parallel:&nbsp;</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000038.png" class="lazyload" /></p>
</div>

<h3>Dask Bag and DataFrame</h3>
<p>Dask provides other data structures for automatic generation of computation graphs. In this subsection, we'll take a look at&nbsp;<kbd>dask.bag.Bag</kbd>, a generic collection of elements that can be used to code MapReduce-style algorithms, and <kbd>dask.dataframe.DataFrame</kbd>, a distributed version of <kbd>pandas.DataFrame</kbd>.</p>
<p>A <kbd>Bag</kbd> can be easily created from a Python collection. For example, you can create a <kbd>Bag</kbd> from a list using the <kbd>from_sequence</kbd>&nbsp;factory function. The level of parallelism can be specified using the <kbd>npartitions</kbd> argument (this will distribute the <kbd>Bag</kbd> content into a number of partitions). In the following example, we create a <kbd>Bag</kbd> containing numbers from <kbd>0</kbd> to <kbd>99</kbd>, partitioned into&nbsp;four chunks:</p>
<pre><code class="lang-python">    import dask.bag as dab
    dab.from_sequence(range(100), npartitions=4)
    # Result:
    # dask.bag&lt;from_se..., npartitions=4&gt;
</code></pre>
<p>In the next example, we will demonstrate how to perform a word count of a set of strings using an algorithm that's&nbsp;similar to MapReduce. Given our collection of sequences, we apply <kbd>str.split</kbd>, followed by <kbd>concat</kbd> to obtain a linear list of words in the documents. Then, for each word, we produce a dictionary that contains a word and the value <kbd>1</kbd> (refer to the <em>An introduction to MapReduce</em> section for an illustration). We then write a <em>Reduce</em> step using the <kbd>foldby</kbd> operator to calculate the word count.</p>
<p>The <kbd>foldby</kbd> transformation is useful to implement&nbsp;a Reduce step that combines the word counts without having to shuffle all the elements over the network. Imagine that our word dataset is divided into two partitions. A good strategy to calculate the total count is to first sum the word occurrences in each partition and then combine those partial sums to get the final result. The following figure illustrates the concept. On the left, we have our input partitions. The partial sum is calculated for each individual partition (this is done using a binary operation, <strong>binop</strong>), and then the final sums are computed by combining the partial sums using a <strong>combine</strong> function.</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000031.png" class="lazyload" /></p>
</div>
<p>The following code illustrates how to use <kbd>Bag</kbd> and the&nbsp;<kbd>foldby</kbd>&nbsp;operator to compute the word count. For the <kbd>foldby</kbd> operator, we need to define two functions that take&nbsp;five arguments:</p>
<ul>
<li><kbd>key</kbd>: This is a function that returns the key for the reduce operation.</li>
<li><kbd>binop</kbd>: This is a function that takes two arguments: <kbd>total</kbd> and <kbd>x</kbd>. Given a <kbd>total</kbd> value (the values accumulated so far), <kbd>binop</kbd>&nbsp;incorporates the next item into the total.</li>
<li><kbd>initial</kbd>: This is the initial value for the <kbd>binop</kbd> accumulation.</li>
<li><kbd>combine</kbd>: This is a function that combines the totals for each partition (in this case it is a simple sum).</li>
<li><kbd>initial_combine</kbd>: This is the initial value for the <kbd>combine</kbd> accumulation.</li>
</ul>
<p>Now, let's look at the code:</p>
<pre><code class="lang-python">    collection = dab.from_sequence(["the cat sat on the mat",
                                    "the dog sat on the mat"], npartitions=2)

    binop = lambda total, x: total + x["count"]
    combine = lambda a, b: a + b
    (collection
     .map(str.split)
     .concat()
     .map(lambda x: {"word": x, "count": 1})
     .foldby(lambda x: x["word"], binop, 0, combine, 0)
     .compute())
    # Output:
    # [('dog', 1), ('cat', 1), ('sat', 2), ('on', 2), ('mat', 2), ('the', 4)]
</code></pre>
<p>As we just saw, expressing complex operations in an efficient way using <kbd>Bag</kbd> can become cumbersome. For this reason, Dask provides&nbsp;another data structure designed for analytical workloads--<kbd>dask.dataframe.DataFrame</kbd>. A <kbd>DataFrame</kbd> can be initialized in Dask using a variety of methods, such as from&nbsp;<kbd>CSV</kbd> files on distributed filesystems, or directly from a <kbd>Bag</kbd>. Just like <kbd>da.array</kbd> provides an API that closely mirrors NumPy features, Dask <kbd>DataFrame</kbd> can be used as a distributed version of <kbd>pandas.DataFrame</kbd>.</p>
<p>As a demonstration, we will re-implement the word count using a <kbd>DataFrame</kbd>. We first load the data to obtain a <kbd>Bag</kbd>&nbsp;of words, and then we convert the <kbd>Bag</kbd> to a <kbd>DataFrame</kbd> using the <kbd>to_dataframe</kbd> method. By passing a column name to the <kbd>to_dataframe</kbd> method, we can initialize a <kbd>DataFrame</kbd>, which contains a single column, named <kbd>words</kbd>:</p>
<pre><code class="lang-python">    collection = dab.from_sequence(["the cat sat on the mat",
                                    "the dog sat on the mat"], npartitions=2)
    words = collection.map(str.split).concat()
    df = words.to_dataframe(['words'])
    df.head()
    # Result:
    #   words
    # 0   the
    # 1   cat
    # 2   sat
    # 3    on
    # 4   the
</code></pre>
<p>Dask <kbd>DataFrame</kbd> closely replicates the <kbd>pandas.DataFrame</kbd> API. To compute the word count, we only need to call the <kbd>value_counts</kbd> method on the words column, and Dask will automatically devise a parallel computation strategy. To trigger the calculation, it is sufficient to call the <kbd>compute</kbd> method:</p>
<pre><code class="lang-python">    df.words.value_counts().compute()
    # Result:
    # the    4
    # sat    2
    # on     2
    # mat    2
    # dog    1
    # cat    1
    # Name: words, dtype: int64
</code></pre>
<p>An interesting question one may ask is "<em>what kind of algorithm does DataFrame use under the hood?</em>". The answer can be found by looking at the upper part of the generated Dask graph,&nbsp;which is displayed in the following figure. The first two rectangles at the bottom represent two partitions of the dataset, which are stored as two <kbd>pd.Series</kbd> instances. To calculate the overall count, Dask will first execute <kbd>value_counts</kbd> on each of the <kbd>pd.Series</kbd> and then combine the counts&nbsp;along with the <kbd>value_counts_aggregate</kbd> step:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000007.png" class="lazyload" /></p>
</div>
<p>As you can see, both Dask <kbd>array</kbd> and <kbd>DataFrame</kbd> take advantage of the fast vectorized implementations of NumPy and Pandas to achieve excellent performance and stability.</p>

<h3>Dask distributed</h3>
<p>The first iterations of the Dask project were designed to run on a single computer using a thread-based or a process-based scheduler. Recently, the implementation of a new distributed backend can be used to set up and run Dask graphs on a network of computers.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Dask distributed is not installed automatically with Dask. The library is available through the <kbd>conda</kbd> package manager (use the <kbd>$&nbsp;conda install distributed</kbd> command&nbsp;) as well as <kbd>pip</kbd> (with the <kbd>$&nbsp;pip install distributed</kbd> command).</p>
</div>
</div>
<p>Getting started with Dask distributed is really easy. The most basic setup is obtained by instantiating a <kbd>Client</kbd> object:</p>
<pre><code class="lang-python">    from dask.distributed import Client
     
    client = Client()
    # Result:
    # &lt;Client: scheduler='tcp://127.0.0.1:46472' processes=4 cores=4&gt;
</code></pre>
<p>By default, Dask will start a few key processes (on the local machine) necessary for scheduling and executing distributed tasks through the <kbd>Client</kbd> instance. The main components of a Dask cluster are a single&nbsp;<em>scheduler</em> and a collection of <em>workers</em>.</p>
<p>The <strong>scheduler</strong> is the process responsible for distributing the work across the workers and to monitor and manage the results. Generally, when a task is submitted to the user, the scheduler will find a free worker and submit a task for execution. Once the worker is done, the scheduler is informed that the result is available.</p>
<p>A worker is a process that accepts incoming tasks and produces results. Workers can reside on different machines over the network. Workers execute tasks using <kbd>ThreadPoolExecutor</kbd>. This can be used to achieve parallelism when using functions that do not acquire the GIL (such as Numpy, Pandas, and Cython functions in <kbd>nogil</kbd> blocks). When executing pure Python code, it is advantageous to start many single-threaded worker processes as this will enable parallelism for code that acquires the GIL.</p>
<p>The <kbd>Client</kbd> class can be used to submit tasks manually to the scheduler using familiar asynchronous methods. For example, to submit a function for execution on the cluster, one can use the <kbd>Client.map</kbd> and&nbsp;<kbd>Client.submit</kbd> methods.&nbsp;In the following code, we demonstrate the use of <kbd>Client.map</kbd> and <kbd>Client.submit</kbd> to calculate the square of a few numbers. The <kbd>Client</kbd> will submit a series of tasks to the scheduler and we will receive a <kbd>Future</kbd> instance for each task:</p>
<pre><code class="lang-python">    def square(x):
       return x ** 2

    fut = client.submit(square, 2)
    # Result:
    # &lt;Future: status: pending, key: square-05236e00d545104559e0cd20f94cd8ab&gt;

    client.map(square)
    futs = client.map(square, [0, 1, 2, 3, 4])
    # Result:
    # [&lt;Future: status: pending, key: square-d043f00c1427622a694f518348870a2f&gt;,
    #  &lt;Future: status: pending, key: square-9352eac1fb1f6659e8442ca4838b6f8d&gt;,
    #  &lt;Future: status: finished, type: int, key: 
    #  square-05236e00d545104559e0cd20f94cd8ab&gt;,
    #  &lt;Future: status: pending, key: 
    #  square-c89f4c21ae6004ce0fe5206f1a8d619d&gt;,
    #  &lt;Future: status: pending, key: 
    #  square-a66f1c13e2a46762b092a4f2922e9db9&gt;]
</code></pre>
<p>So far, this is quite similar to what we&nbsp;saw in the earlier chapters with&nbsp;<kbd>TheadPoolExecutor</kbd>&nbsp;and <kbd>ProcessPoolExecutor</kbd>. Note however, that Dask Distributed not only submits the tasks, but also caches the computation results on the worker memory. You can see caching in action by looking at the&nbsp;preceding code example. When we first invoke <kbd>client.submit</kbd>, the <kbd>square(2)</kbd> task is created and its status is set to <em>pending</em>. When we subsequently invoke <kbd>client.map</kbd>, &nbsp;the <kbd>square(2)</kbd> task is resubmitted to the scheduler, but this time, rather than recalculating its value, the scheduler directly retrieves the result for the worker. As a result, the third <kbd>Future</kbd> returned by map already has a finished status.</p>
<p>Results from a collection of <kbd>Future</kbd> instances can be retrieved using the <kbd>Client.gather</kbd> method:</p>
<pre><code class="lang-python">    client.gather(futs)
    # Result:
    # [0, 1, 4, 9, 16]
</code></pre>
<p><kbd>Client</kbd> can also be used to run arbitrary Dask graphs. For example, we can trivially run our approximation of pi by passing the <kbd>client.get</kbd>&nbsp;function as an optional argument to <kbd>pi.compute</kbd>:</p>
<pre><code class="lang-python">    pi.compute(get=client.get)
</code></pre>
<p>This feature makes Dask extremely scalable as it is possible to develop and run algorithms on the local machine using one of the simpler schedulers and, in case the performance is not satisfactory, to reuse the same algorithms on a cluster of hundreds of machines. &nbsp;</p>

<h4>Manual cluster setup</h4>
<p>To instantiate scheduler and workers manually, one can&nbsp;use the <kbd>dask-scheduler</kbd> and <kbd>dask-worker</kbd> command-line utilities. First, we can initialize&nbsp;a scheduler using the <kbd>dask-scheduler</kbd> command:</p>
<pre><code class="lang-python">$ dask-scheduler
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Scheduler at: tcp://192.168.0.102:8786
distributed.scheduler - INFO - bokeh at: 0.0.0.0:8788
distributed.scheduler - INFO - http at: 0.0.0.0:9786
distributed.bokeh.application - INFO - Web UI: http://127.0.0.1:8787/status/
distributed.scheduler - INFO - -----------------------------------------------
</code></pre>
<p>This will provide an address for the scheduler and a Web UI address that can be accessed to monitor the state of the cluster. Now, we can assign some workers to the scheduler; this can be accomplished using the <kbd>dask-worker</kbd> command and by passing the address of the scheduler to the worker. This will automatically start a worker with&nbsp;four threads:</p>
<pre><code class="lang-python">$ dask-worker 192.168.0.102:8786
distributed.nanny - INFO - Start Nanny at: 'tcp://192.168.0.102:45711'
distributed.worker - INFO - Start worker at: tcp://192.168.0.102:45928
distributed.worker - INFO - bokeh at: 192.168.0.102:8789
distributed.worker - INFO - http at: 192.168.0.102:46154
distributed.worker - INFO - nanny at: 192.168.0.102:45711
distributed.worker - INFO - Waiting to connect to: tcp://192.168.0.102:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Threads: 4
distributed.worker - INFO - Memory: 4.97 GB
distributed.worker - INFO - Local Directory: /tmp/nanny-jh1esoo7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Registered to: tcp://192.168.0.102:8786
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Nanny 'tcp://192.168.0.102:45711' starts worker process 'tcp://192.168.0.102:45928'
</code></pre>
<p>The Dask scheduler is fairly resilient in the sense that if we add and remove a worker, it is able to track which results are unavailable and recompute them on-demand. Finally, in order to use the initialized scheduler from a Python session, it is sufficient to initialize a <kbd>Client</kbd> instance and provide the address for the scheduler:</p>
<pre><code class="lang-python">client = Client(address='192.168.0.102:8786')
# Result: 
# &lt;Client: scheduler='tcp://192.168.0.102:8786' processes=1 cores=4&gt;
</code></pre>
<p>Dask also provides a convenient diagnostic Web UI&nbsp;that can be used to monitor the status and time spent for each of the tasks performed on the cluster. In the next figure, the <strong>Task Stream</strong> shows the time taken for executing the pi estimation. In the plot, each horizontal gray line corresponds to a thread used by the workers (in our case, we have one worker with four&nbsp;threads, also called <strong>Worker Core</strong>), and each rectangular box corresponds to a task, colored&nbsp;so&nbsp;that&nbsp;the same task types have the same color (for example,&nbsp;addition, power, or exponent). From this plot, you can observe how all the boxes are very small and far from each other. This means that the tasks are quite small compared to the overhead of communication.</p>
<p>In this case,&nbsp;an increase in chunk size, which implies to an increase&nbsp;in the time required to run each task compared to the time of communication, will be beneficial.</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000037.png" class="lazyload" /></p>
&nbsp;</div>

<h2>Using PySpark</h2>
<p>Nowadays, Apache Spark is one of the most popular projects for distributed computing. Developed in Scala, Spark was released in 2014, and integrates with HDFS and provides several advantages and improvements over the Hadoop MapReduce framework.</p>
<p>Contrary to Hadoop MapReduce, Spark is designed to process data interactively and supports APIs for the Java, Scala, and Python programming languages. Given its different architecture, especially by the fact that Spark keep results in memory, Spark is generally much faster than Hadoop MapReduce.&nbsp;</p>

<h3>Setting up Spark and PySpark</h3>
<p>Setting up PySpark from scratch requires the installation of the Java and Scala runtimes, the compilation of&nbsp;the project from source, and the configuration of&nbsp;Python and Jupyter notebook&nbsp;so that they can be used alongside the Spark installation. An easier and less error-prone way to set up PySpark is to use an already configured Spark cluster made available through a <strong>Docker</strong> container. &nbsp;</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>Docker can be downloaded at <a href="https://www.docker.com/">https://www.docker.com/</a>&nbsp;. If you're new to containers, you can read the next chapter for an introduction.</p>
</div>
</div>
<p>To set up a Spark cluster, it is sufficient to go in this chapter's code files (where&nbsp;a file named <kbd>Dockerfile</kbd> is located) and issue the following command:</p>
<pre><code class="lang-python">$ docker build -t pyspark

</code></pre>
<p>This command will automatically download, install, and configure Spark, Python, and Jupyter notebook in an isolated environment. To start Spark and a Jupyter notebook session, you can execute the following command:</p>
<pre><code class="lang-python">$ docker run -d -p 8888:8888 -p 4040:4040 pyspark
22b9dbc2767c260e525dcbc562b84a399a7f338fe1c06418cbe6b351c998e239
</code></pre>
<p>The command will print a unique&nbsp;ID (called <em>container id</em>) that you can use to reference the application container and will start Spark and Jupyter notebook in the background. The <kbd>-p</kbd> option ensures that we can access the SparkUI and Jupyter network ports from the local machine. After issuing the command, you can open a browser to <kbd>http://127.0.0.1:8888</kbd> to access the Jupyter notebook session. You can test the correct initialization of Spark by creating a new notebook and executing the following content inside a cell:</p>
<pre><code class="lang-python">    import pyspark
    sc = pyspark.SparkContext('local[*]')

    rdd = sc.parallelize(range(1000))
    rdd.first()
    # Result:
    # 0
</code></pre>
<p>This will initialize a <kbd>SparkContext</kbd>&nbsp;and take the first element in a collection (those new terms will be explained in detail later). Once the <kbd>SparkContext</kbd> is initialized, we can also head over to <a href="http://127.0.0.1:4040">http://127.0.0.1:4040</a> to open the Spark Web UI.</p>
<p>Now that the setup is complete, we will understand how Spark works and how to implement simple parallel algorithms using its powerful API.</p>

<h3>Spark architecture</h3>
<p>A Spark cluster is a set of processes distributed over different machines. The <strong>Driver Program</strong> is a process, such as a Scala or Python interpreter, used by the user to submit the tasks to be executed.</p>
<p>The user can build task graphs, similar to Dask, using a special API and submit those tasks to the <strong>Cluster Manager</strong> that is responsible for&nbsp;assigning these tasks to <strong>Executors</strong>, processes responsible for executing&nbsp;the tasks. In a multi-user system, the Cluster Manager is also responsible for allocating resources on a per-user basis.</p>
<p>The user interacts with the Cluster Manager through&nbsp;the Driver Program. The class responsible for communication between the user and the Spark cluster is&nbsp;called <kbd>SparkContext</kbd>. This class is able to connect and configure the Executors&nbsp;on the cluster based on the resources available to the user.</p>
<p>For its most common use-cases, Spark manages its data through a data structure called <strong>Resilient Distributed Datasets</strong> (<strong>RDD</strong>), which represents a collection of items. RDDs are capable of handling massive datasets by separating their elements into partitions and operating on the partitions in parallel (note that this mechanism is mainly hidden from the user). RDDs can also be stored in memory (optionally, and when appropriate) for fast access and to cache expensive intermediate results.</p>
<p>Using RDDs, it is possible to define tasks and transformations (similarly to how we were automatically generating computation graphs in Dask) and, when requested, the Cluster Manager will automatically dispatch and execute tasks on the available Executors.</p>
<p>The Executors will receive the tasks from the Cluster Manager, execute them, and keep the results around if needed. Note that an Executor can have multiple cores and each node in the cluster may have multiple Executors. Generally speaking, Spark is fault tolerant on Executor's failures.</p>
<p>In the following diagram, we show how the aforementioned components interact in a Spark cluster. The <strong>Driver Program</strong> interacts with the <strong>Cluster Manager</strong> that manages the <strong>Executor</strong> instances on different nodes (each Executor instance can also have multiple threads). Note that, even if the <strong>Driver Program</strong> doesn't directly control the Executors, the results, which are stored in the <strong>Executor</strong> instances, are transferred directly between the Executors and the Driver Program.&nbsp;For this reason, it's important that the <strong>Driver Program</strong> is network-reachable from the <strong>Executor</strong> processes:&nbsp;</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000036.png" class="lazyload" /></p>
</div>
<p>A natural question to ask is: How is Spark, a software written in Scala, able to execute Python code? The integration is done through the <kbd>Py4J</kbd> library, which maintains a Python process under-the-hood and communicates with it through sockets (a form of interprocess communication). In order to run the tasks, Executors maintain a series of Python processes so that they are able to process Python code in parallel.&nbsp;</p>
<p>RDDs and variables defined in a Python process in the Driver Program are serialized, and the communication between the Cluster Manager and the Executors (including shuffling) is dealt with by Spark's Scala code. The extra serialization steps necessary for the Python and Scala interchange, all contribute to the overhead of communication; therefore, when using PySpark, extra care must be taken to ensure that the data structures used are serialized efficiently and that the data partitions are big enough so that the cost of communication is negligible compared to the cost of execution.&nbsp;</p>
<p>The following&nbsp;diagram illustrates the additional Python processes needed for PySpark execution. These additional Python processes come with associated memory costs and an extra layer of indirection that complicate error reporting:</p>
<div>
<p class="f-center"><img data-src="https://learnable-static.s3.amazonaws.com/premium/reeedr/books/python-web-development-tools/images/000026.png" class="lazyload" /></p>
</div>
<p>Despite these drawbacks, PySpark is still a widely used tool because it&nbsp;bridges the vivid Python ecosystem with the industrial strength of the Hadoop infrastructure.&nbsp;</p>

<h3>Resilient Distributed Datasets</h3>
<p>The easiest way to create an RDD in Python is&nbsp;with the <kbd>SparkContext.parallelize</kbd> method. This method was also used earlier where we parallelized a collection of integers between <kbd>0</kbd> and <kbd>1000</kbd>:</p>
<pre><code class="lang-python">    rdd = sc.parallelize(range(1000))
    # Result:
    # PythonRDD[3] at RDD at PythonRDD.scala:48
</code></pre>
<p>The <kbd>rdd</kbd> collection will be divided into a number of partitions which, in this case, correspond to a default value of&nbsp;four (the default value can be changed using configuration options). To explicitly specify the number of partitions, one can pass an extra argument to <kbd>parallelize</kbd>:</p>
<pre><code class="lang-python">    rdd = sc.parallelize(range(1000), 2)
    rdd.getNumPartitions() # This function will return the number of partitions
    # Result:
    # 2
</code></pre>
<p>RDDs support a lot of functional programming operators,&nbsp;similar to what we used back in Chapter 6, <em>Implementing Concurrency</em>,&nbsp;with reactive programming and data streams (even though, in that case, the operators were designed to work on events over time rather than normal collections). For example, we may illustrate the basic&nbsp;<kbd>map</kbd> function which, by now, should be quite familiar. In the following code, we use <kbd>map</kbd>&nbsp;to calculate the square of a series of numbers:</p>
<pre><code class="lang-python">    square_rdd = rdd.map(lambda x: x**2)
    # Result:
    # PythonRDD[5] at RDD at PythonRDD.scala:48
</code></pre>
<p>The <kbd>map</kbd>&nbsp;function will return a new RDD but won't compute anything just yet. In order to trigger the&nbsp;execution, you can use the <kbd>collect</kbd> method, which will retrieve all the elements in the collection, or&nbsp;<kbd>take</kbd>, which will return only the first ten&nbsp;elements:</p>
<pre><code class="lang-python">    square_rdd.collect()
    # Result:
    # [0, 1, ... ]

    square_rdd.take(10)
    # Result:
    # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
</code></pre>
<p>For a comparison between&nbsp;PySpark, Dask, and the other parallel programming libraries we explored in the&nbsp;earlier chapters, we will reimplement the approximation of pi. In the PySpark implementation, we will first create two RDDs of random numbers using <kbd>parallelize</kbd>, then we combine the datasets using the&nbsp;<kbd>zip</kbd> function (this is equivalent to Python's <kbd>zip</kbd>), and we finally test&nbsp;whether the random points are inside the circle:</p>
<pre><code class="lang-python">    import numpy as np

    N = 10000
    x = np.random.uniform(-1, 1, N)
    y = np.random.uniform(-1, 1, N)

    rdd_x = sc.parallelize(x)
    rdd_y = sc.parallelize(y)

    hit_test = rdd_x.zip(rdd_y).map(lambda xy: xy[0] ** 2 + xy[1] ** 2 &lt; 1)
    pi = 4 * hit_test.sum()/N
</code></pre>
<p>It's important to note that both the <kbd>zip</kbd> and <kbd>map</kbd> operations produce new RDDs and do not actually execute the instruction on the underlying data. In the&nbsp;preceding example, code execution is triggered when we call the <kbd>hit_test.sum</kbd> function, which returns an integer. This behavior is different from the Dask API&nbsp;where the whole computation (including the final result, <kbd>pi</kbd>) did not trigger the execution.</p>
<p>We can now move on to a more interesting application to demonstrate more RDD methods. We will learn how to count the number of visits each user of a website performs in a day. In a real-world scenario, the data would have been collected in a database and/or stored in a distributed filesystem, such as HDFS. However, in our example, we will generate some data that we will then analyze.</p>
<p>In the following code, we generate a list of dictionaries, each containing a <kbd>user</kbd>&nbsp;(selected among twenty&nbsp;users) and a <kbd>timestamp</kbd>. The steps to produce the dataset are as follows:</p>
<ol>
<li>Create a pool of 20 users (the <kbd>users</kbd> variable).</li>
<li>Define a function that returns a random time between two dates.</li>
<li>For 10,000 times, we choose a random user from our <kbd>users</kbd> pool and a random timestamp between the dates January 1, 2017 and January 7, 2017.</li>
</ol>
<pre><code class="lang-python">      import datetime

      from uuid import uuid4
      from random import randrange, choice

      # We generate 20 users
      n_users = 20 
      users = [uuid4() for i in range(n_users)]

      def random_time(start, end):
          '''Return a random timestamp between start date and end 
          date'''
          # We select a number of seconds
          total_seconds = (end - start).total_seconds()
          return start + 
          datetime.timedelta(seconds=randrange(total_seconds))

      start = datetime.datetime(2017, 1, 1)
      end = datetime.datetime(2017, 1, 7)

      entries = []
      N = 10000
      for i in range(N):
          entries.append({
           'user': choice(users),
           'timestamp': random_time(start, end)
          })
</code></pre>
<p>With the dataset at hand, we can start asking questions and use PySpark to find&nbsp;the answers. One common question is "<em>How many times has a given user visited the website?.</em>" A naive way to compute this result can be achieved by grouping the entries&nbsp;RDD by user (using the&nbsp;<kbd>groupBy</kbd>&nbsp;operator) and counting how many items are present for each user.&nbsp;In PySpark, <kbd>groupBy</kbd>&nbsp;takes&nbsp;a function as argument to extract the grouping key for each element and returns a new RDD&nbsp;that contain tuples of the <kbd>(key, group)</kbd> form.&nbsp;In the following example, we use the user ID as the key for our <kbd>groupBy</kbd>, and we inspect the first element using <kbd>first</kbd>:</p>
<pre><code class="lang-python">    entries_rdd = sc.parallelize(entries)
    entries_rdd.groupBy(lambda x: x['user']).first()
    # Result:
    # (UUID('0604aab5-c7ba-4d5b-b1e0-16091052fb11'),
    #  &lt;pyspark.resultiterable.ResultIterable at 0x7faced4cd0b8&gt;)
</code></pre>
<p>The return value of <kbd>groupBy</kbd> contains a <kbd>ResultIterable</kbd>&nbsp;(which is basically a list) for each user ID. To count the number of visits per user, it's sufficient to calculate the length of each <kbd>ResultIterable</kbd>:</p>
<pre><code class="lang-python">    (entries_rdd
     .groupBy(lambda x: x['user'])
     .map(lambda kv: (kv[0], len(kv[1])))
     .take(5))
    # Result:
    # [(UUID('0604aab5-c7ba-4d5b-b1e0-16091052fb11'), 536),
    #  (UUID('d72c81c1-83f9-4b3c-a21a-788736c9b2ea'), 504),
    #  (UUID('e2e125fa-8984-4a9a-9ca1-b0620b113cdb'), 498),
    #  (UUID('b90acaf9-f279-430d-854f-5df74432dd52'), 561),
    #  (UUID('00d7be53-22c3-43cf-ace7-974689e9d54b'), 466)]
</code></pre>
<p>Even though this algorithm may work well in small datasets, <kbd>groupBy</kbd> requires us to collect and store the whole set of entries for each user in memory, and this can exceed the memory capacity of an individual node. Since we don't need the list but only the count, there's a better way to calculate this number without having to hold the list of visits for each user in memory.</p>
<div class="box tip">
<h4>Tip</h4>
<div class="body">
<p>When dealing with an RDD of <kbd>(key, value)</kbd> pairs, it is possible to use <kbd>mapValues</kbd> to apply a function only to the values. In the preceding code, we can replace the <kbd>map(lambda kv: (kv[0], len(kv[1])))</kbd> call with <kbd>mapValues(len)</kbd> for better readability.</p>
</div>
</div>
<p>For a more efficient calculation, we can leverage the <kbd>reduceByKey</kbd> function, which will perform a step similar to the Reduce step that&nbsp;we saw in the <em>An introduction to MapReduce</em> section. The&nbsp;<kbd>reduceByKey</kbd>&nbsp;function can be called from an RDD of tuples that contain a key as their first element and a value as their second element, and accepts a function as its first argument that will calculate the reduction. &nbsp;A simple example of the&nbsp;<kbd>reduceByKey</kbd>&nbsp;function is illustrated in the following snippet. We have a few string keys associated with integer numbers, and we want to get the sum of the values for each key; the reduction, expressed as a lambda, corresponds to the sum of the elements:</p>
<pre><code class="lang-python">    rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4), ("c", 5)])
    rdd.reduceByKey(lambda a, b: a + b).collect()
    # Result:
    # [('c', 5), ('b', 6), ('a', 4)]
</code></pre>
<p>The <kbd>reduceByKey</kbd>&nbsp;function is much more efficient than <kbd>groupBy</kbd> because the reduction is parallelizable and doesn't require the in-memory storage of the groups; also, it&nbsp;limits the data shuffled between Executors (it performs similar operations&nbsp;to Dask's&nbsp;<kbd>foldby</kbd>, which was explained earlier). At this point, we can rewrite our visit count calculation using <kbd>reduceByKey</kbd>:</p>
<pre><code class="lang-python">    (entries_rdd
     .map(lambda x: (x['user'], 1))
     .reduceByKey(lambda a, b: a + b)
     .take(3))
    # Result:
    # [(UUID('0604aab5-c7ba-4d5b-b1e0-16091052fb11'), 536),
    #  (UUID('d72c81c1-83f9-4b3c-a21a-788736c9b2ea'), 504),
    #  (UUID('e2e125fa-8984-4a9a-9ca1-b0620b113cdb'), 498)]
</code></pre>
<p>With Spark's RDD API, it is also easy to answer questions such as "<em>How many visits did the website receive each day?.</em>" This can be computed using <kbd>reduceByKey</kbd> with the appropriate key (which is the date extracted from the timestamp). In the following example, we demonstrate the calculation. Also, note the usage of the&nbsp;<kbd>sortByKey</kbd> operator to return the counts sorted by date:</p>
<pre><code class="lang-python">    (entries_rdd
     .map(lambda x: (x['timestamp'].date(), 1))
     .reduceByKey(lambda a, b: a + b)
     .sortByKey()
     .collect())
    # Result:
    # [(datetime.date(2017, 1, 1), 1685),
    #  (datetime.date(2017, 1, 2), 1625),
    #  (datetime.date(2017, 1, 3), 1663),
    #  (datetime.date(2017, 1, 4), 1643),
    #  (datetime.date(2017, 1, 5), 1731),
    #  (datetime.date(2017, 1, 6), 1653)]
</code></pre>

<h3>Spark DataFrame</h3>
<p>For numerical and analytical tasks, Spark provides a convenient interface available through the <kbd>pyspark.sql</kbd>&nbsp;module (also called SparkSQL). The module includes a <kbd>spark.sql.DataFrame</kbd> class that can be used for efficient SQL-style queries similar to those of Pandas. Access to the SQL interface is provided&nbsp;through the <kbd>SparkSession</kbd> class:</p>
<pre><code class="lang-python">    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
</code></pre>
<p><kbd>SparkSession</kbd> can then be used to create a <kbd>DataFrame</kbd> through the function <kbd>createDataFrame</kbd>. The function <kbd>createDataFrame</kbd> accepts either a RDD, a list, or a <kbd>pandas.DataFrame</kbd>.</p>
<p>In the following example, we will create a <kbd>spark.sql.DataFrame</kbd>&nbsp;by converting an RDD, <kbd>rows</kbd>, which contains a collection of <kbd>Row</kbd>&nbsp;instances. The <kbd>Row</kbd> instances represent an association between a set of column names and a set of values, just like a row in a <kbd>pd.DataFrame</kbd>. In this example, we have two columns--<kbd>x</kbd> and&nbsp;<kbd>y</kbd>--to which we will associate random numbers:</p>
<pre><code class="lang-python">    # We will use the x_rdd and y_rdd defined previously.
    rows = rdd_x.zip(rdd_y).map(lambda xy: Row(x=float(xy[0]), y=float(xy[1])))

    rows.first() # Inspect the first element
    # Result:
    # Row(x=0.18432163061239137, y=0.632310101419016)
</code></pre>
<p>After&nbsp;obtaining our collection of <kbd>Row</kbd> instances, we can combine them in a <kbd>DataFrame</kbd>, as follows. We can also inspect the <kbd>DataFrame</kbd> content using the&nbsp;<kbd>show</kbd> method:</p>
<pre><code class="lang-python">    df = spark.createDataFrame(rows)
    df.show(5)
    # Output:
    # +-------------------+--------------------+
    # |                  x|                   y|
    # +-------------------+--------------------+
    # |0.18432163061239137|   0.632310101419016|
    # | 0.8159145525577987| -0.9578448778029829|
    # |-0.6565050226033042|  0.4644773453129496|
    # |-0.1566191476553318|-0.11542211978216432|
    # | 0.7536730082381564| 0.26953055476074717|
    # +-------------------+--------------------+
    # only showing top 5 rows
</code></pre>
<p><kbd>spark.sql.DataFrame</kbd> supports&nbsp;performing transformations on the distributed dataset using a convenient SQL syntax. For example, you can use the <kbd>selectExpr</kbd> method to calculate a value using a SQL expression. In the following code, we compute the hit test using&nbsp;the <kbd>x</kbd> and <kbd>y</kbd>&nbsp;columns&nbsp;and the <kbd>pow</kbd> SQL function:</p>
<pre><code class="lang-python">    hits_df = df.selectExpr("pow(x, 2) + pow(y, 2) &lt; 1 as hits")
    hits_df.show(5)
    # Output:
    # +-----+
    # | hits|
    # +-----+
    # | true|
    # |false|
    # | true|
    # | true|
    # | true|
    # +-----+
    # only showing top 5 rows
</code></pre>
<p>To demonstrate the expressivity of SQL, we can also calculate the estimation of pi using&nbsp;a single expression. The expression involves using SQL functions such as <kbd>sum</kbd>, <kbd>pow</kbd>, <kbd>cast</kbd>, and <kbd>count</kbd>:</p>
<pre><code class="lang-python">    result = df.selectExpr('4 * sum(cast(pow(x, 2) + 
                           pow(y, 2) &lt; 1 as int))/count(x) as pi')
    result.first()
    # Result:
    # Row(pi=3.13976)
</code></pre>
<p>Spark SQL follows the same syntax as Hive, a SQL engine for distributed datasets built on Hadoop. Refer to <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual">https://cwiki.apache.org/confluence/display/Hive/LanguageManual</a> for a complete syntax reference.</p>
<p>DataFrames are a great way to leverage the power and optimization of Scala while using the Python interface. The main reason is that queries are interpreted symbolically by SparkSQL and the execution happens&nbsp;directly in Scala without having to pass intermediate results through Python. This greatly reduces the serialization overhead and takes advantage of the query optimizations performed by SparkSQL. Optimizations and query planning allows the use of SQL operators, such as <kbd>GROUP BY</kbd>, without incurring in performance penalties, such as the one we experienced using <kbd>groupBy</kbd> directly on an RDD.</p>

<h2>Scientific computing with mpi4py</h2>
<p>Even though Dask and Spark are great technologies widely used in the IT industry, they have not been widely adopted in academic research. High-performance supercomputers with thousands of processors have been used in academia for decades to run intense numerical applications. For this reason, supercomputers are generally configured using a very different software stack&nbsp;that focuses on a computationally-intensive algorithm implemented in a low-level language, such as C, Fortran, or even assembly.</p>
<p>The principal library used for parallel execution on these kinds of systems is <strong>Message Passing Interface</strong> (<strong>MPI</strong>), which, while less convenient or sophisticated than Dask or Spark, is perfectly capable of expressing parallel algorithms and achieving excellent performance. Note that, contrary to Dask and Spark, MPI does not follow the MapReduce model and is best used for running thousands of processes with very little data sent between them.</p>
<p>MPI works quite differently compared to what we've seen so far. Parallelism in MPI is achieved by running <em>the same script</em>&nbsp;in multiple processes (which possibly exist on different nodes); communication and synchronization between processes is handled by a designated process, which is commonly called <strong>root</strong> and&nbsp;is usually identified by a&nbsp;<kbd>0</kbd> ID.&nbsp;</p>
<p>In this section, we will briefly demonstrate the main concepts of MPI using its <kbd>mpi4py</kbd> Python interface. In the following example, we demonstrate the simplest possible parallel code with MPI. The code imports the MPI module and retrieves <kbd>COMM_WORLD</kbd>, which is an interface that can be used to interact with other MPI processes. The <kbd>Get_rank</kbd> function will return an integer identifier for the current process:</p>
<pre><code class="lang-python">    from mpi4py import MPI

    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    print("This is process", rank)
</code></pre>
<p>We can place the&nbsp;preceding code in a file, <kbd>mpi_example.py</kbd>, and execute it.&nbsp;Running this script normally won't do anything special as it involves the execution of a single process:</p>
<pre><code class="lang-python">    $ python mpi_example.py
    This is process 0
</code></pre>
<p>MPI jobs are meant to be executed using the&nbsp;<kbd>mpiexec</kbd>&nbsp;command, which takes a <kbd>-n</kbd> option to indicate the number of parallel processes. Running the script using the following command will generate&nbsp;four independent executions of the same script, each with a different ID:</p>
<pre><code class="lang-python">    $ mpiexec -n 4 python mpi_example.py
    This is process 0
    This is process 2
    This is process 1
    This is process 3
</code></pre>
<p>Distributing processes among the network is performed automatically through a resource manager (such as TORQUE). Generally, supercomputers are configured by the system administrator, which will also provide instructions on how to run MPI software.</p>
<p>To get a feel as to what an MPI program looks like, we will reimplement the approximation of <em>pi</em>. The complete code is shown here. The program&nbsp;will do the following:</p>
<ul>
<li>Create a random array of <kbd>N / n_procs</kbd>&nbsp;size for each process so that each process will test the same amount of samples (<kbd>n_procs</kbd> is obtained through the <kbd>Get_size</kbd> function)</li>
<li>In each separate process, calculate the sum of the hit tests and store it in <kbd>hits_counts</kbd>,&nbsp;which will represent the partial counts for each process</li>
<li>Use the <kbd>reduce</kbd> function to calculate the total sum of the partial counts. When using reduce, we need to specify the <kbd>root</kbd> argument to specify&nbsp;which process will receive the result</li>
<li>Print the final result only on the process corresponding to the root process:</li>
</ul>
<pre><code class="lang-python">      from mpi4py import MPI

      comm = MPI.COMM_WORLD
      rank = comm.Get_rank()

      import numpy as np

      N = 10000

      n_procs = comm.Get_size()

      print("This is process", rank)

      # Create an array
      x_part = np.random.uniform(-1, 1, int(N/n_procs))
      y_part = np.random.uniform(-1, 1, int(N/n_procs))

      hits_part = x_part**2 + y_part**2 &lt; 1
      hits_count = hits_part.sum()

      print("partial counts", hits_count)

      total_counts = comm.reduce(hits_count, root=0)

      if rank == 0:
         print("Total hits:", total_counts)
         print("Final result:", 4 * total_counts/N)
</code></pre>
<p>We can now place the preceding code in a file named <kbd>mpi_pi.py</kbd> and execute it using <kbd>mpiexec</kbd>. The output shows how the four process executions are intertwined until we get to the <kbd>reduce</kbd> call:</p>
<pre><code class="lang-python">$ mpiexec -n 4 python mpi_pi.py
This is process 3
partial counts 1966
This is process 1
partial counts 1944
This is process 2
partial counts 1998
This is process 0
partial counts 1950
Total hits: 7858
Final result: 3.1432
</code></pre>

<h2>Summary</h2>
<p>Distributed processing can be used to implement algorithms capable of handling massive datasets by distributing smaller tasks across a cluster of computers. Over the years, many software packages, such as Apache Hadoop, have been developed to implement performant and reliable execution of distributed software.</p>
<p>In this chapter, we learned about the architecture and usage of Python packages, such as&nbsp;Dask and PySpark, which provide powerful APIs to design and execute programs capable of scaling to hundreds of machines. We also briefly looked at MPI, a library that has been used for decades to distribute work on supercomputers designed for academic research.</p>
<p>Throughout this book, we explored several techniques to improve the performance&nbsp;of our program, and&nbsp;to increase the speed of our programs and the size of the datasets we are able to process. In the next chapter, we will describe the strategies and best practices to write and maintain high-performance code. &nbsp;</p>

</div>



<!--Chapter 9-->

<div class="chapter" data-chapter-number="9">
<div class="chapter-start">
<div class="ch-head">Chapter <span class="chapter-number"> 9 </span></div>
<h1 class="chaptertitle">Designing for High Performance</h1>
<h3 class="author">Gabriele Lanaro</h3>
</div>


<p>In the&nbsp;earlier chapters, we learned how to use the vast array of tools available in Python's standard library and third-party packages to assess and improve the performance of Python applications. In this chapter, we will provide some general guidelines on how to approach different kinds of applications as well as illustrate some good practices that are commonly adopted by several Python projects.</p>
<p>In this chapter, we will learn the following:</p>
<ul>
<li>Picking the right performance technique for generic, number crunching, and big data applications</li>
<li>Structuring a Python project</li>
<li>Isolating Python installations with virtual environments and containerization</li>
<li>Setting up continuous integration with Travis CI</li>
</ul>

<h2>Choosing a suitable strategy</h2>
<p>Many packages are available for improving the performance of programs, but how do we determine the best optimization strategy for our program? A variety of factors dictate the decision on which method to use. In this section, we will try to answer this question as comprehensively as possible, based on broad application categories.&nbsp;</p>
<p>The first aspect to take into consideration is the type of application. Python is a language that serves multiple and very diverse communities that span web services, system scripting, games, machine learning, and much more. Those different applications will require optimization efforts&nbsp;for different parts of the program.</p>
<p>For example, a web service can be optimized to have a very short response time. Also, it has to be able to answer as many requests as possible using as little resources as possible (that is, it&nbsp;will try to achieve lower latency), while numerical code may require weeks to run. It's important to improve the amount of data the system may process, even if there's a significant start up overhead (in this case, we are interested in throughput).&nbsp;</p>
<p>Another aspect is the platform and architecture we are developing for. While Python has support for a lot of platforms and architectures, many of the third-party libraries may have limited support for certain platforms, especially when dealing with packages that bind into C extensions. For this reason, it's necessary to check the availability of libraries for the target platforms and architectures.</p>
<p>Also, some architectures, such as embedded systems and small devices, may have severe CPU and memory restrictions. This is an important factor to take into consideration as, for instance, some techniques (such as multiprocessing) may consume&nbsp;too much memory or require the execution of additional software.</p>
<p>Finally, the business requirements are equally important. Many times, software products require fast iterations and the ability to change the code quickly. Generally speaking, you want to keep your software stack as minimal as possible so that modification, testing, deployment, and introducing additional platform support becomes easy and feasible in a short period&nbsp;of time. This also applies to the team--installing the software stack and starting the development should be as smooth as possible. For this reason, one should generally prefer pure Python libraries over extensions, with the possible exception of solid, battle-tested libraries, such as NumPy.&nbsp;Additionally, various business aspects will help determine which operations need to be optimized first (always remember that <em>premature optimization is the root of all evil</em>).</p>

<h3>Generic applications</h3>
<p>Generic applications, such as web apps or mobile app backends, usually involve calls to remote services and databases. For such cases, it can be useful to take advantage of asynchronous frameworks, such as the ones presented in Chapter 6, <em>Implementing Concurrency</em>; this will improve application logic, system design, responsiveness and, also, it will simplify the handling of network failures.</p>
<p>Use of asynchronous programming also makes it easier to implement and use microservices. A <strong>microservice</strong>, although there is no standard definition, can be thought of as a remote service&nbsp;that focuses on a specific aspect of the application (for example,&nbsp;authentication).</p>
<p>The idea behind microservices is that you can build an application by composing different microservices that communicate through a simple protocol (such as gRPC, REST calls, or through a dedicated message queue). This architecture is in contrast with a monolithic application where all the services are handled by the same Python process.</p>
<p>Advantages of microservices include strong decoupling of different parts of the application. Small, simple services can be implemented and maintained by different teams as well as be updated and deployed at different times. This also allows microservices to be easily replicated so that they can handle more users. Additionally, since the communication is done through a simple protocol, microservices can be implemented in a different language&nbsp;that can be more appropriate than Python for the specific application.</p>
<p>If the performance of a service is not satisfactory, the application can often be executed on a different Python interpreter, such as PyPy (provided that all the third-party extensions are compatible) to achieve sufficient&nbsp;speed gains. Otherwise, algorithmic strategies as well as porting&nbsp;bottlenecks to Cython is generally sufficient to achieve satisfactory performance.</p>

<h3>Numerical code</h3>
<p>If your goal is to write numerical code, an excellent strategy is to start directly with a NumPy implementation. Using NumPy is a safe bet because it is available and tested on many platforms and, as we have seen in the&nbsp;earlier chapters, many other packages treat NumPy arrays as first-class citizens.</p>
<p>When properly written (such as by taking advantage of broadcasting and other techniques we learned in Chapter 2, <em>Pure Python Optimizations</em>), NumPy performance is already quite close to the native performance achievable by C code, and won't require further optimization. That said, certain algorithms are hard to express efficiently using NumPy's data structures and methods. When this happens, two very good options can be, for example, Numba or Cython.</p>
<p>Cython is a very mature tool used intensely&nbsp;by many important projects, such as <kbd>scipy</kbd> and <kbd>scikit-learn</kbd>. Cython code, with its explicit, static type declarations, makes it very understandable, and most Python programmers will have no problem picking up its familiar syntax. Additionally, the absence of "magic" and good inspection tools make it easy for the programmer to predict its performance and have educated guesses&nbsp;as to&nbsp;what to change to achieve maximum performance.</p>
<p>Cython, however, has some drawbacks. Cython code needs to be compiled before it can be executed, thus breaking the convenience of the Python edit-run cycle. This also requires the availability of a compatible C compiler for the target platform.&nbsp; This also complicates distribution and deployment, as multiple platforms, architectures, configurations, and compilers need to be tested for every target.</p>
<p>On the other hand,&nbsp;Numba API requires only the definition of pure-Python functions, which get compiled on the fly, maintaining the fast Python edit-run cycle. In general, Numba requires a LLVM&nbsp;toolchain installation to be available on the target platform. Note that, as of version 0.30, there is some limited support for <strong>Ahead-Of-Time</strong> (<strong>AOT</strong>) compilation of Numba functions so that Numba-compiled functions can be packaged and deployed without requiring a Numba and LLVM installation.</p>
<p>Note that both Numba and Cython are usually available pre-packaged with all of their dependencies (including compilers) on the default channels of the conda package manager. Therefore, deployment of Cython can be greatly simplified on the platforms where the conda package manager is available.</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>What if Cython and Numba are still not enough? While this is generally not required, an additional strategy would be to implement a pure C module (which can be further optimized using compiler flags or hand-tuning) and use it from a Python module using either the <kbd>cffi</kbd> package (<a href="https://cffi.readthedocs.io/en/latest/">https://cffi.readthedocs.io/en/latest/</a>) or Cython.&nbsp;</p>
</div>
</div>
<p>Using NumPy, Numba, and Cython is a very effective strategy to obtain near-optimal performance on serial codes. For many applications, serial codes are certainly enough and, even if the ultimate plan is to have a parallel algorithm, it&rsquo;s still very worthy working on a serial&nbsp;reference implementation for debugging purposes and because a serial implementation is likely to be faster on small datasets.</p>
<p>Parallel implementations vary considerably in complexity depending on the particular application. In many cases, the program can be easily expressed as a series of independent calculations followed by some sort of <em>aggregation</em> and is parallelizable using simple process-based interfaces,&nbsp;such as <kbd>multiprocessing.Pool</kbd>&nbsp;or <kbd>ProcessPoolExecutor</kbd>, which have the advantage of being able to execute generic Python code in parallel without much trouble.</p>
<p>To avoid the time and memory overhead of starting multiple processes, one can use threads. NumPy functions typically release the GIL and are good candidates for thread-base parallelization. Additionally, Cython and Numba provide special&nbsp;<kbd>nogil</kbd>&nbsp;statements as well as automatic parallelization, which makes them suitable for simple, lightweight parallelization.</p>
<p>For more complex use cases, you may have to change the algorithm significantly. In those cases, Dask arrays are a decent option, which provide an almost-drop-in replacement for standard NumPy. Dask has the further advantage of operating very transparently and is easy to tweak.</p>
<p>Specialized applications&nbsp;that make intensive use of linear algebra routines (such as deep learning and computer graphics) may benefit from packages such as Theano and Tensorflow, which are capable of highly performant and automatic parallelization with built-in GPU support.</p>
<p>Finally, <kbd>mpi4py</kbd> usage can be used for deploying parallel python scripts on a MPI-based supercomputer (commonly available for researchers in universities).</p>

<h3>Big data</h3>
<p>Large datasets (typically larger than 1 TB) are becoming increasingly common and a lot of resources have been invested in developing technologies capable of collecting, storing, and analyzing them. Typically, the choice of which framework to use is bound to how the data is stored in the first place.</p>
<p>Many times, even if the complete dataset doesn't fit in a single machine, it is still possible to devise strategies to extract the answers without having to probe the whole dataset. For example, it is quite often possible to answer questions by extracting a small, interesting subset of data&nbsp;that can be easily loaded in memory and analyzed with highly convenient and performant libraries, such as Pandas. By filtering or randomly sampling data points, one can often find a good enough answer to a business question without having to resort to big data tools.</p>
<p>If the bulk of the company's software is written in Python, and you have the freedom to decide your software stack, it would make sense to use&nbsp;Dask distributed. The software package has a very simple setup and is tightly integrated with the Python ecosystem. Using something such as Dask <kbd>array</kbd> and <kbd>DataFrame</kbd>, it's very easy to scale your already-existing Python algorithms by adapting NumPy and Pandas code.</p>
<p>Quite often, some companies may have already set up a Spark cluster. In this case, PySpark is the optimal choice, and the use of SparkSQL is encouraged for higher performance. One of the Spark advantages is that it allows the use of other languages, such as Scala and Java.</p>

<h2>Organizing your source code</h2>
<p>The repository structure of a typical Python project consists, at a minimum, of a directory containing a <kbd>README.md</kbd> file, a Python module or package containing the source code for the application or library, and a <kbd>setup.py</kbd> file. Projects may also adopt different conventions to comply with company policies or specific frameworks in use. In this section, we will illustrate some common practices that are commonly found in community-driven Python projects which can include some of the tools we illustrated in the earlier chapters.</p>
<p>A typical directory structure&nbsp;for a Python project named&nbsp; <kbd>myapp</kbd> can look like this.&nbsp;Now, we will elucidate the role of each file and directory:</p>
<pre><code class="lang-python">    myapp/ 
      README.md
      LICENSE
      setup.py
      myapp/
        __init__.py
        module1.py
        cmodule1.pyx
        module2/
           __init__.py
      src/
        module.c
        module.h
      tests/
        __init__.py
        test_module1.py
        test_module2.py
      benchmarks/
        __init__.py
        test_module1.py
        test_module2.py
      docs/
      tools/
</code></pre>
<p><kbd>README.md</kbd>&nbsp;is a text file that contains general information about the software, such as project scope, installation, a quick start, and useful links. If the software is released to the public, a&nbsp;<kbd>LICENSE</kbd>&nbsp;file is used to specify terms and conditions for its usage.</p>
<p>Python software is commonly packaged using the&nbsp;<kbd>setuptools</kbd>&nbsp;library in a&nbsp;<kbd>setup.py</kbd> file. As we have seen in the&nbsp;earlier chapters, <kbd>setup.py</kbd> is also an effective way to compile and distribute Cython code.</p>
<p>The <kbd>myapp</kbd> package contains the source code for the application, including Cython modules. Sometimes, it's convenient to maintain pure-Python implementations besides their Cython-optimized counterparts. Commonly, the Cython version of a module is named with a c prefix (such as <kbd>cmodule1.pyx</kbd> in the preceding example).</p>
<p>If the external <kbd>.c</kbd> and <kbd>.h</kbd> files are needed, those are usually stored under an additional&nbsp;<kbd>src/</kbd> directory placed in the top-level (<kbd>myapp</kbd>) project directory.</p>
<p>The <kbd>tests/</kbd> directory contains testing code for application (usually in the form of unit tests), which can be run using a test runner, such as <kbd>unittest</kbd> or <kbd>pytest</kbd>. However, some projects prefer to place the <kbd>tests/</kbd> directory inside the <kbd>myapp</kbd> package.&nbsp;Since high-performance code is tweaked and rewritten continuously, having a solid test suite is crucial to spot bugs as early as possible and to improve the developer experience by shortening the test-edit-run cycle.&nbsp;</p>
<p>Benchmarks can be placed in the <kbd>benchmarks</kbd> directory; the advantage of having benchmarks separated from tests is that benchmarks can potentially take more time to execute. Benchmarks can also be run on a build server (see the <em>Continuous integration</em> section) as a simple mean to compare&nbsp;the performance&nbsp;of versions. While benchmarks usually take longer to run than unit tests, it's best to keep their execution as short as possible to avoid waste of resources.</p>
<p>Finally, the <kbd>docs/</kbd> directory contains user and&nbsp;developer documentation and API references. This usually also includes configuration files&nbsp;for documentation tools, such as <kbd>sphinx</kbd>. Other tools and scripts can be placed in the <kbd>tools/</kbd> directory.</p>

<h2>Isolation, virtual environments, and containers</h2>
<p>The importance of having isolated environments for code testing and execution becomes quite apparent by noticing what happens when you ask a friend to run one of your Python scripts. What happens is that&nbsp;you provide instructions to install Python version X and dependent packages <kbd>Y</kbd>, <kbd>X</kbd>, and ask&nbsp;them to copy and execute the script on&nbsp;their machine.</p>
<p>In many cases, your friend will proceed and download Python for its platform as well as the dependent libraries and try to execute the script. However, it can happen (more often than not) that&nbsp;the script will fail because either&nbsp;their computer has a different operating system than yours, or the installed libraries are not the same version as the one you installed&nbsp;on your machine. At other times, there&nbsp;can be previous installations&nbsp;that are improperly removed and will cause hard-to-debug conflicts and a lot of frustration.</p>
<p>A very easy way to avoid this scenario is to use virtual environments. Virtual environments are used to create and manage several Python installations by isolating Python, related executables, and third-party packages. Since Python's 3.3 version, the standard library includes the&nbsp;<kbd>venv</kbd>&nbsp;module (previously known as <strong>virtualenv</strong>), which is a tool designed&nbsp;to create and manage simple isolated environments. Python packages in <kbd>venv</kbd>-based virtual environments can be installed using <kbd>setup.py</kbd> files or through <kbd>pip</kbd>.</p>
<p>Providing exact and specific library versions is crucial&nbsp;when dealing with high-performance code. Libraries evolve all the time between releases and changes in the algorithms may dramatically affect the performance. For instance, popular libraries, such as <kbd>scipy</kbd> and <kbd>scikit-learn</kbd>, often port some of their codes and data structures to Cython, so it's really important that the user installs the correct version for optimal performance.</p>

<h3>Using conda environments</h3>
<p>Most of the time, using <kbd>venv</kbd> is a fine choice.&nbsp;However, when writing high-performance code, it often happens that some high-performance libraries also require non-Python software to be installed. This typically involves additional setting up of compilers and high-performance native libraries (in C, C++, or Fortran) to which Python packages link. As <kbd>venv</kbd> and <kbd>pip</kbd> are designed to deal with Python packages only, this scenario is poorly supported by these tools.</p>
<p>The <kbd>conda</kbd> package manager was created specifically to handle such cases. Creating a virtual environment with conda can be done using the <kbd>conda create</kbd> command. The command takes a <kbd>-n</kbd> argument (<kbd>-n</kbd> stands for <kbd>--name</kbd>)&nbsp;that specifies an identifier for the newly created environment and the packages we intend to install. If we wish to create an environment that uses python version <kbd>3.5</kbd> and the latest version of NumPy, we use the following command:</p>
<pre><code class="lang-python">$ conda create -n myenv Python=3.5 numpy
</code></pre>
<p>Conda will take care of fetching the relative packages from their repositories and placing them in an isolated Python installation. To enable the virtual environment, you can use the <kbd>source activate</kbd> command:</p>
<pre><code class="lang-python">$ source activate myenv
</code></pre>
<p>After executing this command, the default Python interpreter will be switched to the version we specified earlier. You can easily verify the location of your Python executable&nbsp;using the <kbd>which</kbd> command, which returns the full path of the executable:</p>
<pre><code class="lang-python">(myenv) $ which python
/home/gabriele/anaconda/envs/myenv/bin/python
</code></pre>
<p>At this point, you are free to add, remove, and modify packages in the virtual environment without affecting the global Python installation. Further packages can be installed using the <kbd>conda install &lt;package name&gt;</kbd> command or through <kbd>pip</kbd>.</p>
<p>The beauty of virtual environments is that you can install or compile any software you want in a well-isolated fashion. This means that if, for some reason, your environment gets corrupted, you can scratch it and start from zero.</p>
<p>To remove the <kbd>myenv</kbd> environment,&nbsp;you first need to deactivate it, and then use the <kbd>conda env remove</kbd>&nbsp;command, as follows:</p>
<pre><code class="lang-python">(myenv) $ source deactivate
$ conda env remove -n myenv
</code></pre>
<p>What if a package is not available on the standard&nbsp;<kbd>conda</kbd> repositories?&nbsp;One option is to look&nbsp;whether it is available in the <kbd>conda-forge</kbd> community channel. To search for a package in <kbd>conda-forge</kbd>, you can add&nbsp;the <kbd>-c</kbd> option (which stands for <kbd>--channel</kbd>) to the <kbd>conda search</kbd> command:</p>
<pre><code class="lang-python">$ conda search -c conda-forge scipy
</code></pre>
<p>The command will list a series of packages and versions available that match the <kbd>scipy</kbd> query string.&nbsp;Another option is to search for the package in the public channels hosted on <strong>Anaconda Cloud</strong>. The command-line client for Anaconda Cloud can be downloaded by installing the <kbd>anaconda-client</kbd> package:</p>
<pre><code class="lang-python">$ conda install anaconda-client
</code></pre>
<p>Once the client is installed, you can use the <kbd>anaconda</kbd> command-line client to search for packages. In the following example, we demonstrate&nbsp;how to look for the&nbsp;<kbd>chemview</kbd> package:</p>
<pre><code class="lang-python">$ anaconda search chemview 
Using Anaconda API: https://api.anaconda.org
Run 'anaconda show &lt;USER/PACKAGE&gt;' to get more details:
Packages:
 Name                      | Version | Package Types   | Platforms 
 ------------------------- | ------  | --------------- | ---------------
 cjs14/chemview            | 0.3     | conda           | linux-64, win-64, osx-64
                                     : WebGL Molecular Viewer for IPython notebook.
 gabrielelanaro/chemview   | 0.7     | conda           | linux-64, osx-64
                                     : WebGL Molecular Viewer for IPython notebook.

</code></pre>
<p>Installation can then be easily performed by specifying the appropriate channel with the <kbd>-c</kbd> option:</p>
<pre><code class="lang-python">$ conda install -c gabrielelanaro chemlab
</code></pre>

<h3>Virtualization and Containers</h3>
<p>Virtualization has been around for a long time as a way to run multiple operating systems on the same machine in order to better utilize physical resources.</p>
<p>One way to achieve virtualization is to employ a <em>virtual machine</em>. Virtual machines work by creating virtual hardware resources, such as CPU, memory, and devices, and use those to install and run multiple operating systems on the same machine. Virtualization can be accomplished by installing a hypervisor application on top of an operating system (called <em>host</em>). The hypervisor is capable of creating, managing, and monitoring virtual machines and their respective operating systems (called <em>guests</em>).</p>
<div class="box note">
<h4>Note</h4>
<div class="body">
<p>It's important to note that virtual environments, despite their name, have nothing to do with virtual machines. A virtual environment is Python-specific and works by setting up different Python interpreters through shell scripts.</p>
</div>
</div>
<p>Containers are a way to isolate an application by creating an environment separated from the host operating system and contain only the necessary dependencies. Containers are an operating system feature that allows you to share the hardware resources (provided by the operating system kernel) for multiple instances. A container is different from a virtual machine because it does not abstract hardware&nbsp;resources, but merely shares the&nbsp;operating system's kernel.</p>
<p>Containers are very efficient at utilizing hardware resources as those are accessed natively through the kernel. For this reason, they are an excellent solution for high-performance applications. They are also&nbsp;fast to create and destroy and can be used to quickly test an application in isolation. Containers are also used to simplify deployments (especially microservices) and to develop build servers, such as the ones we mentioned in the&nbsp;preceding section.</p>
<p>In Chapter 8, <em>Distributed Processing</em>, we used <strong>docker</strong> to easily set up a PySpark installation. Docker is one of the most popular containerization solutions available today. The best way to install docker is by following the instructions on the&nbsp;official website (<a href="https://www.docker.com/">https://www.docker.com/</a>). After installation, it is possible to easily create and manage containers using the docker command-line interface.</p>
<p>You can start a&nbsp;new container by using the <kbd>docker run</kbd> command. In the following example, we will demonstrate how to use <kbd>docker run</kbd> to execute a shell session in an Ubuntu 16.04 container. To do this, we will need to specify the following arguments:</p>
<ul>
<li><kbd>-i</kbd> specifies that we are trying to start an interactive session. It is also possible to execute individual docker commands without interactivity (for example, when starting a web server).</li>
<li><kbd>-t &lt;image name&gt;</kbd> specifies which system image to use. In the following example, we use the <kbd>ubuntu:16.04</kbd> image.</li>
<li><kbd>&nbsp;/bin/bash</kbd>, which is the command to run inside the container, demonstrated as follows:</li>
</ul>
<pre><code class="lang-python">      $ docker run -i -t ubuntu:16.04 /bin/bash
      root@585f53e77ce9:/#
</code></pre>
<p>This command will immediately take us into a separate, isolated shell&nbsp;where we can play around with the system and install software without touching the host operating system. Using a container is a very good way to test installations and deployments on different Linux flavors. After we are done with the interactive shell, we can type the&nbsp;<kbd>exit</kbd>&nbsp;command to return to the host system.</p>
<p>In the last chapter, we also made use of the port and detach options, <kbd>-p</kbd> and <kbd>-d</kbd>, to run the executable <kbd>pyspark</kbd>. The <kbd>-d</kbd> option simply asks Docker to run the command in the background. The <kbd>-p &lt;host_port&gt;:&lt;guest_port&gt;</kbd>&nbsp;option was, instead, necessary to map a network port of the host operating system to the guest system; without this option, the Jupyter Notebook would not have been reachable from a browser running in the host system.</p>
<p>We can monitor&nbsp;the status of the containers with <kbd>docker ps</kbd>, as shown in the following snippet.&nbsp;The <kbd>-a</kbd> option (which stands for <em>all</em>) serves to output information about&nbsp;all the containers, whether they are currently running or not:</p>
<pre><code class="lang-python">$ docker ps -a
CONTAINER ID IMAGE        COMMAND     CREATED       STATUS     PORTS NAMES
585f53e77ce9 ubuntu:16.04 "/bin/bash" 2 minutes ago Exited (0)       2 minutes ago pensive_hamilton
</code></pre>
<p>The information provided by <kbd>docker ps</kbd> includes a hexadecimal identifier, <kbd>585f53e77ce9</kbd>, as well as a human readable&nbsp;name, <kbd>pensive_hamilton</kbd>, both of which can be used to specify the container in other docker commands. It also includes additional information about the command executed, creation time, and the execution's current status.</p>
<p>You can resume the execution of an exited container using the <kbd>docker start</kbd> command. To gain shell access to the container, you can use <kbd>docker attach</kbd>. Both these commands can be followed by either the container&nbsp;ID or its human readable name:</p>
<pre><code class="lang-python">$ docker start pensive_hamilton 
pensive_hamilton
$ docker attach pensive_hamilton 
root@585f53e77ce9:/#
</code></pre>
<p>A container can be easily removed using the <kbd>docker run</kbd> command followed by a container identifier:</p>
<pre><code class="lang-python">$ docker rm pensive_hamilton
</code></pre>
<p>As you can see, you are free to execute commands, run, stop, and resume containers as needed, in less than a second. Using docker containers interactively is a great way to test things out and play with new packages without disturbing the host operating system. Since you can run many containers at the same time, docker can also be used to simulate a distributed system (for testing and learning purposes) without having to own an expensive computing cluster.</p>
<p>Docker also allows you to create your own system images, which is useful for distribution, testing, deployment, and documentation purposes. This will be the topic of the next subsection.</p>

<h4>Creating docker images</h4>
<p>Docker images are ready-to-use, pre-configured systems. The <kbd>docker run</kbd>&nbsp;command can be used to access and install the docker images available on the <strong>DockerHub</strong> (<a href="https://hub.docker.com/">https://hub.docker.com/</a>), a web service where package maintainers upload ready-to-use images to test and deploy various applications.</p>
<p>One way to create a docker image is by using the <kbd>docker commit</kbd> command on an existing container. The docker commit command takes a container reference and the output image names as arguments:</p>
<pre><code class="lang-python">$ docker commit &lt;container_id&gt; &lt;new_image_name&gt;
</code></pre>
<p>Using this method is useful to save snapshots of a certain container but, if the image is removed from the system, the steps to recreate the image are lost as well.</p>
<p>A better way to create an image is to build it using a&nbsp;<strong>Dockerfile</strong>. A Dockerfile is a text file that provides instructions on how to build an image starting from another image. As an example, we will illustrate the contents of the Dockerfile we used in the&nbsp;last chapter to set up PySpark with Jupyter notebook support. The complete file is reported here.</p>
<p>Each Dockerfile needs a starting image, which can be declared with the <kbd>FROM</kbd> command. In our case, the starting image is <kbd>jupyter/scipy-notebook</kbd>, which is available through DockerHub (<a href="https://hub.docker.com/r/jupyter/scipy-notebook/">https://hub.docker.com/r/jupyter/scipy-notebook/</a>).</p>
<p>Once we have defined our starting image, we can start issuing shell commands to install packages and perform other configurations using a series of <kbd>RUN</kbd>&nbsp;and <kbd>ENV</kbd> commands. In the following example, you can recognize installation of Java Runtime Environment (<kbd>openjdk-7-jre-headless</kbd>) as well as downloading Spark and setting up relevant environment variables. The <kbd>USER</kbd> instructions&nbsp;can be used&nbsp;to specify which user executes the subsequent commands:</p>
<pre><code class="lang-python">    FROM jupyter/scipy-notebook
    MAINTAINER Jupyter Project &lt;jupyter@googlegroups.com&gt;
    USER root

    # Spark dependencies
    ENV APACHE_SPARK_VERSION 2.0.2
    RUN apt-get -y update &amp;&amp; 
        apt-get install -y --no-install-recommends 
        openjdk-7-jre-headless &amp;&amp; 
        apt-get clean &amp;&amp; 
        rm -rf /var/lib/apt/lists/*
    RUN cd /tmp &amp;&amp; 
        wget -q http://d3kbcqa49mib13.cloudfront.net/spark-
        ${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz    &amp;&amp; 
        echo "ca39ac3edd216a4d568b316c3af00199
              b77a52d05ecf4f9698da2bae37be998a 
              *spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz" | 
        sha256sum -c - &amp;&amp; 
        tar xzf spark-${APACHE_SPARK_VERSION}
        -bin-hadoop2.6.tgz -C /usr/local &amp;&amp; 
        rm spark-${APACHE_SPARK_VERSION}-bin-hadoop2.6.tgz
    RUN cd /usr/local &amp;&amp; ln -s spark-${APACHE_SPARK_VERSION}
        -bin-hadoop2.6 spark

    # Spark and Mesos config
    ENV SPARK_HOME /usr/local/spark
    ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/
        py4j-0.10.3-src.zip
    ENV SPARK_OPTS --driver-java-options=-Xms1024M 
        --driver-java-options=-
        Xmx4096M --driver-java-options=-Dlog4j.logLevel=info

    USER $NB_USER
</code></pre>
<p>Dockerfiles can be used to create images using the following command from the directory where the Dockerfile is located. The <kbd>-t</kbd> option can be used to specify the tag that will be used to store the image. With the following line, we can create the image named&nbsp;<kbd>pyspark</kbd> from the preceding Dockerfile:</p>
<pre><code class="lang-python">$ docker build -t pyspark .
</code></pre>
<p>The command will automatically retrieve&nbsp;the starting image,&nbsp;<kbd>jupyter/scipy-notebook</kbd>, and produce a new image, named <kbd>pyspark</kbd>.&nbsp;</p>

<h2>Continuous integration</h2>
<p>Continuous integration is a great way to ensure that the application stays bug-free at every development iteration. The main idea behind continuous integration is to run the test suite for the project very frequently, usually on a separate build server that pulls the code directly from the main project repository.</p>
<p>Setting up a build server can be accomplished by manually setting up software&nbsp;such as Jenkins (<a href="https://jenkins.io/">https://jenkins.io/</a>), Buildbot (<a href="http://buildbot.net/">http://buildbot.net/</a>), and Drone (<a href="https://github.com/drone/drone">https://github.com/drone/drone</a>) on a machine. This a convenient and cheap solution, especially for small teams and private projects.</p>
<p>Most open source projects take advantage of Travis CI (<a href="https://travis-ci.org/">https://travis-ci.org/</a>), a service capable of building and testing your code automatically from your repository because it's tightly integrated with GitHub. As of today, Travis CI provides a free plan for open source projects. Many open source Python projects take advantage of Travis CI to ensure that the programs run correctly on multiple Python versions and platforms.</p>
<p>Travis CI can be set up easily from a GitHub repository by including a <kbd>.travis.yml</kbd> file containing the build instruction for the project and activating the builds on the Travis CI website (<a href="https://travis-ci.org/">https://travis-ci.org/</a>) after registering an account.</p>
<p>An example <kbd>.travis.yml</kbd>&nbsp;for a high performance application is illustrated&nbsp;here. The file contains&nbsp;instructions to build and run the software&nbsp;that are specified using a few sections written in YAML syntax.</p>
<p>The <kbd>python</kbd> section specifies which Python versions to use. The <kbd>install</kbd> section will download and set up conda for testing, installing dependencies, and setting up the project. While this step is not necessary (one can use <kbd>pip</kbd> instead), conda is a great package manager for high-performance applications as it contains useful native packages.</p>
<p>The <kbd>script</kbd> section contains the code needed to test the code. In this example, we limit ourselves to run our tests and benchmarks:</p>
<pre><code class="lang-python">    language: python
    python:
      - "2.7"
      - "3.5"
    install:
      # Setup miniconda
      - sudo apt-get update
      - if [[ "$TRAVIS_PYTHON_VERSION" == "2.7" ]]; then
          wget https://repo.continuum.io/miniconda/
          Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;
        else
          wget https://repo.continuum.io/miniconda/
          Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;
        fi
      - bash miniconda.sh -b -p $HOME/miniconda
      - export PATH="$HOME/miniconda/bin:$PATH"
      - hash -r
      - conda config --set always_yes yes --set changeps1 no
      - conda update -q conda
      # Installing conda dependencies
      - conda create -q -n test-environment python=
        $TRAVIS_PYTHON_VERSION numpy pandas cython pytest
      - source activate test-environment
      # Installing pip dependencies
      - pip install pytest-benchmark
      - python setup.py install

    script:
      pytest tests/
      pytest benchmarks/
</code></pre>
<p>Every time new code is pushed (as well as other configurable events) to the GitHub repository, Travis CI will spin up a container, install dependencies, and run&nbsp;the test suite. Using Travis CI in open source projects is a great practice as it is a form of constant feedback about the status of the project and also provides up-to-date installation instructions through a continuously tested <kbd>.travis.yml</kbd> file.</p>

<h2>Summary</h2>
<p>Deciding on a strategy to optimize your software is a complex and delicate task&nbsp;that depends on the application type, target platforms, and business requirements. In this chapter, we provided some guidelines to help you think and choose an appropriate software stack for your own applications.</p>
<p>High-performance numerical applications sometimes require managing installation and deployment of third-party packages&nbsp;that may require handling of external tools and native extensions. In this chapter, we saw how to structure your Python project, including tests, benchmarks, documentation, Cython modules, and C extensions. Also, we introduced the continuous integration service Travis CI, which can be used to enable continuous testing for your projects hosted on GitHub.</p>
<p>Finally, we also learned about&nbsp;virtual environments and docker containers&nbsp;that can be used to test applications in isolation and to greatly simplify deployments and ensure that multiple developers have access to the same platform.</p>

</div>
